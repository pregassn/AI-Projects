{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Assignment1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"g4AGKp3yd9a2","colab_type":"text"},"source":["\n","\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\n","#     Subject: NLP Assignment 1       # \n","##     Student Name: Nirmal Pregassame\n","##     Student ID:  R00181587\n","\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\n","\n","This notebook is done in Google Colab and represents the work which has been done to complete the Assignment 1 of NLP.\n","The aim of this assignemnt is to build an **\"Error Correction Module\"**.\n","In our model, he will use the output of the OCR system provided by the lecturer.\n","Then, we will attempt to match each word with a dictionary.\n","The **\"Error Correction Module\"** will follow the following procedure:\n","1. Decontraction of the string\n","1. Tokenization of the string\n","1. Check for each words its presence in the dictionary\n","1. If not, propose a list of proposed words for correction\n","1. Use a metric distance between the original word and the list of proposed words\n","1. Select the best words for correction\n","1. Reset the contraction to its original form\n","1. Untokenize the string\n","\n","In the following code, we are trying to unzip the code provided by the lecturer in order to reuse the OCR module"]},{"cell_type":"code","metadata":{"id":"yrr0ecaTKIHC","colab_type":"code","outputId":"0931e79c-a35f-43a3-edd9-beb2d72292d0","executionInfo":{"status":"ok","timestamp":1572180598342,"user_tz":0,"elapsed":3403,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!unzip -uq \"/content/drive/My Drive/Colab Notebooks/Project1NLP_Model.zip\" -d \"/content/drive/My Drive/Colab Notebooks/Project1NLP_Model\"\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ECx5Zi3KisU_","colab_type":"text"},"source":["# Decontraction of the string\n","\n","The concept of the decontraction is to expand the words which are originally contracted in the string we want to correct.\n","In the following code, a solution for **decontracting the sentence** is proposed.\n","\n","The idea is to use the module \"*Contractions*\" from library \"*pycontraction*\" in a function. This function has 3 parameters:\n","* *text*: it corresponds to the string which need to be expanded\n","* *model*: it corresponds to the type of model to use in the api-key of *Contractions*. By default, it is set to \"*glove-twitter-100*\"\n","* *precision*: it used to enhance the precision of the expansion. By default it is set to \"True\"\n","\n","The process in the function is quite sraight-forward: The model is loaded. Then, the expand_text is applied on the string which generates the end result."]},{"cell_type":"code","metadata":{"id":"pf3KMr4Egewr","colab_type":"code","outputId":"26651b19-d9d3-4189-c71a-9b970f62f2d5","executionInfo":{"status":"ok","timestamp":1572180605470,"user_tz":0,"elapsed":10493,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["# Installation of api to get pycontractions library\n","!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n","!pip install language-check\n","!pip install pycontractions\n","\n","# import the module Contractions from pycontractions\n","from pycontractions import Contractions\n","\n","\n","# decontractSentence is a function to expand the contracted word \n","def decontractSentence(text, model = 'glove-twitter-25', precision ='True'):\n","  \"Expand all the contracted words to make them proper dict words\"\n","  cont = Contractions(api_key= model)\n","  cont.load_models()\n","  print('Sentence to expand:', '\"' + text + '\"')\n","  txt_out = list(cont.expand_texts([text], precise= precision))\n","  \n","  print('Sentence after expansion:', '\"' + txt_out[0] + '\"')\n","  return txt_out\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: language-check in /usr/local/lib/python3.6/dist-packages (1.1)\n","Requirement already satisfied: pycontractions in /usr/local/lib/python3.6/dist-packages (2.0.1)\n","Requirement already satisfied: pyemd>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (0.5.1)\n","Requirement already satisfied: language-check>=1.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (1.1)\n","Requirement already satisfied: gensim>=2.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (3.6.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd>=0.4.4->pycontractions) (1.17.3)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.8.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.3.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.12.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.9.253)\n","Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.49.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.21.0)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.253 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.12.253)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.2.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2019.9.11)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.8)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.253->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.253->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"saYzKphUpzb1","colab_type":"text"},"source":["# Tokenization of the string\n","\n","For the tokenization process, the idea is to split the string into elements (tokens) which can be used as basis of work. Those tokens are generally words (and punctuations).\n","In the following code, a solution for **Tokenizing the sentence** is proposed.\n","\n","This time, we use the function \"*word_tokenize*\" from library \"*nltk*\". In our code, we build a function which load the string to be tokenized and then tokenize it by applying the nltk function.\n"]},{"cell_type":"code","metadata":{"id":"9_YMx54dlRpi","colab_type":"code","outputId":"38cb7589-3571-4bd3-e14e-c41863bce43d","executionInfo":{"status":"ok","timestamp":1572180606063,"user_tz":0,"elapsed":11063,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import nltk\n","nltk.download('punkt')\n","\n","# tokenizeTxt is a function to tokenize a sentence\n","def tokenizeTxt(text):\n","  \"The function tokenize the sentence stored in text vaiable\"\n","  print('Sentence before tokenization:', '\"' + text + '\"')\n","  tokens = nltk.word_tokenize(text)\n","  print('Sentence after tokenization:', '\"' , tokens , '\"')\n","  return tokens\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8iRtf953sMOD","colab_type":"text"},"source":["# Dictionary construction\n","\n","In order to check if a word in a string exists, we need to build a dictionary. This can be done by using the corpus database; more specifically the english version of it since, we should build an english dictionary. Some preprocessing steps were necessary to use the corpus. Indeed, after downloading the corpus, we have to merge all the english texts into one \"*all_en.txt*\" so that it could be easily read in our function to build the dictionary.\n","\n","However, Building the dictionary requires few function in order to do it:\n","1. **ngram generation**: This function is essential to build the ngram at word level so that the dictionary can associate each gram of the word with the word itself\n","1. **words storage in the dictionnary from the string**: This is the main step of the dictionary. The idea is to process the string/text in order to store each detected word in the dictionary.\n","1. **word addition**: This function is an open door to add some word if it is not existing in the initial text.\n","\n","We will go to each of these function to explain how they are used for our dictionary generation."]},{"cell_type":"markdown","metadata":{"id":"kVse3O3NzqRS","colab_type":"text"},"source":["## ngram generation\n","\n","The concept of this function is to capture the different grams of the words.\n","Below, the function accepts 2 parameters:\n","* *word*: it is the main word for which the gram will be generated\n","* *n* : it is the gram length. By default, it is set to 2 which means that it is a bigram\n","\n","This function will be used a multiple time in our code. Indeed, the idea is to associate the grams with the words and count for the number of common grams between 2 words. Typically, the more a non-existing word (a word which has been mispelled) has grams in common with an existing word (from the dictionary), the more likely, those 2 words are close. This can provide a good short-list of words candidate to replace the non-existing word."]},{"cell_type":"code","metadata":{"id":"SxKtBm10E8y0","colab_type":"code","colab":{}},"source":["\n","# ngrams_gen generate the gram a word ; the gram number is managed by the parameter n\n","def ngrams_gen(word, n=2):\n","    \"The function generates a list of gram of word. By default, n=2 which means that it is a bigram by default\"\n","    # the set operation is used below to avoid duplicate gram\n","    ngrams_list = set()\n","    \n","    # we go through each letter of word (without the last letters) to generate the gram and create the list\n","    for i in range(0, len(word) - n + 1):\n","        ngrams_list.add(word[i:i + n])\n","        \n","    # the grams list is sorted befor return\n","    return sorted(ngrams_list)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O29ArDXv3OsY","colab_type":"text"},"source":["## Word storage in the dictionary\n","​\n","In this part, the idea is to create a words dictionnary using a text file as input. For the assignment, the file used is the merge of all file in english from the corpus europarl, located at \"*/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/europarl/txt/en/all_en.txt*\".\n","\n","During the construction of the dictionary, 3 keys informations are stored. Those are relevant information to process word errors later on. The type of information stored, are:\n","* *eng_words_dict['txt']*: In this variable, the complete english text of corpus is stored in the form of words list.\n","* *eng_words_dict['count']*: In this variable, we use the \"*Counter*\" function from \"*collections*\" in order to store the number of occurence of a particular word. It is useful at to select the best word for correction when 2 candidate words for replacement of the mispelled word have a tie in the distanc. We can use the number of occurence of the word to propose the best solution.\n","* *eng_words_dict['gram']*: In this variable, we use the \"*defaultdict*\" function from \"*collections*\" in order to make the association of gram and its corresponding list of words in which the gram is present. This is useful, during the error module process, to propose a list of potential words candidate for replacement. Indeed, the idea is to count the number of common gram between the candidate and the original word. The more grams in common they have, the higher is the chance that the candidate is the best candidate for replacement.\n","* *eng_words_dict['shortw']*:\n","\n","Below, the function accepts 2 parameters:\n","* *file*: it represents the file to be read in order to build the dictionary. In the assignment, it is the corpus file \n","* *ngram* : it is the gram length. By default, it is set to 2 which means that it is a bigram. This variable is used to build the *eng_words_dict['gram']* associating grams with its list of word.\n","\n","​\n","This part represents an essential part for our Error Module Correction."]},{"cell_type":"code","metadata":{"id":"FRZAoS3mMGQZ","colab_type":"code","colab":{}},"source":["import collections\n","import re\n","\n","\n","# words_dict is a function to create a dictionnary from a text in a file\n","def words_dict(file, ngram=2):\n","  \n","  \"\"\"4 types classifications are performed for our dictionnary\n","        1) eng_words_dict['txt']: full text splitted into a list of words the symbot ' @ ' is added to make the sepearation between phrases \n","        2) eng_words_dict['count']: each word of the text is associated to its count\n","        3) eng_words_dict['gram'] : each gram is associated with the list of words containing it \n","        4) eng_words_dict['shortw']: list of tuple (<short word>, <short distance>). This tuple are reality for words which length are smaller or equal to the ngram since the ngram dict cannot properly capture them and for mistake detection the gram will be wrong\"\"\"\n","  \n","  eng_words_dict={}\n","  print('Read dictionnary')\n","  # we read the file, make all text to lower case and make a list of words \n","  rgxp = re.compile('(<.*>|\\.|\\?|\\!|\\;|\\(|\\))')\n","  # In this case, we are replacing punctuations and <.*> (which are unecessary introduction of sentence), with the character ' @ ', which is in our case a context delimitation (between sentence for example). \n","  # This symbole is useful later on to determine word context in our \"context\" mode\n","  eng_words_tmp = rgxp.sub(' @ ',open(file).read())\n","  # Below, we convert the text into list of words with the addition of context breaking symbole ' @ '.\n","  eng_words_dict['txt'] = re.findall(r'(\\b[A-Za-z_]+\\b|\\s@\\s)', eng_words_tmp.lower())\n","  \n","  print('Counting words dictionnary')\n","  # Thanks to Counter we can summ (and count) all identical words and create a dict with the word associated to its number \n","  eng_words_dict['count'] = collections.Counter(eng_words_dict['txt'])\n","  print('Number of words in dict      : ' , len(eng_words_dict['count'].keys()))\n","  print('Number of total words in text: ' , sum(eng_words_dict['count'].values()))\n","  \n","  # Now we create the correspondance between gram and its words list. (Again, we use the set to avoid the same word in the list)\n","  eng_words_dict['gram'] = collections.defaultdict(set)\n","  #\n","  eng_words_dict['shortw'] = set()\n","  print('Creating gram collection')\n","  # we go through all the words found in the text and add them to the list of the corresponding gram\n","  for w in eng_words_dict['count'].keys():\n","    for g in ngrams_gen(w,ngram):\n","      eng_words_dict['gram'][g].add(w)\n","    if (len(w) <= ngram):\n","      eng_words_dict['shortw'].add((w,1))\n","  print('Number of grams: ' , len(eng_words_dict['gram'].keys()))\n","      \n","  return eng_words_dict\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XSh4YfyZrteh","colab_type":"text"},"source":["## Word addition in the dictionary\n","​\n","In this part, the idea is to give the possibility to add an extra word in the dictionary. Indeed, we assume that not all words are present in the corpus file. Typically, for our assignment, the function below will be used to store the word \"Unitek\".\n","\n","During the addition of word in the dictionary, the 4 keys informations from the dictionary are updated to insert this new word.\n","\n","Below, the function accepts 3 parameters:\n","* *word*: It represents the word to be added in the dictionnary.\n","* *w_dict*: It represents the dictionnary which was built previously.\n","* *ngram* : It is the gram length. By default, it is set to 2 which means that it is a bigram.\n"]},{"cell_type":"code","metadata":{"id":"aebBc9rba-P7","colab_type":"code","colab":{}},"source":["#add_word_dict is a function to add a new word in the dictionnary. It is also important to specify the granularity of the gram (by default n=2) to complete our list of words for each gram\n","def add_word_dict(word,w_dict,ngram=2):\n","  \"Just like for the dictionnary construction above we have to count the number of words in total in the text and then update our words list for the gram\"\n","  print('Adding words in dictionnary:', '\"' + word + '\"')\n","  # Just like our word in dictionnary, it is set lower case.\n","  wl = word.lower()\n","  # Updating the list of word in the dictionnary with the context breaking symbole ' @ '\n","  w_dict['txt'].append(' @ ')\n","  w_dict['txt'].append(wl)\n","  w_dict['txt'].append(' @ ')\n","  # Updating the count of the word in the dictionnary\n","  w_dict['count'][wl] += 1\n","  # Updating the words list for the gram\n","  for g in ngrams_gen(wl,ngram):\n","    w_dict['gram'][g].add(wl)\n","  if (len(wl) <= ngram):\n","    w_dict['shortw'].add((wl,1))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6WtpUVPuO75","colab_type":"text"},"source":["# List of proposed words\n","\n","In this part, we will propose a list of acceptable words to replace the non-existing word. Typically, once we find that a word is not in the dictionary, we assume that the word is mispelled. Therefore, a list of word candidates should be proposed. For that, we are counting the number of common gram between the original word and the propsed word. The proposed word should be in an aceptable range of common grams to be part of the word proposed list.\n","\n","In order to propose this list, we require a few function:\n","1. **filter for acceptable words**: This function is eensuring the filtering the words within an acceptable range of common grams with the original word.\n","1. **words list proposition**: This is the main step of the list of words proposition. It ensures the calculation of common grams with the original words to build the list.\n","\n","We will go to each of these function to explain how they are used for our list of proposed word."]},{"cell_type":"markdown","metadata":{"id":"d1B2p9hCxTMy","colab_type":"text"},"source":["## Filter for accepted Words \n","​\n","The concept here, is to give a threshold above which the candidate words is considered as acceptable to be proposed as replacement for the original word. This function is used a multiple time in our code. Indeed, we used it for building the list of acceptable word after counting the number of common ngram with the original word; but also, we use this function after calculating the word distance between the candidate and the original word to refine the list of candidate.\n","\n","Below, the function accepts 2 parameters:\n","* *w*: It represents the tuple (word,value) where *word* is the candidate word for replacement and *value* its weight (It could be for example the number the common gram with the original word or the inverted Levenshtein distance from the original word).\n","* *threshold*: It represents the limit above which we accepts the candidate word as a potential solution for replacement of the original word.\n","\n"]},{"cell_type":"code","metadata":{"id":"OZepJmEMuZ7O","colab_type":"code","colab":{}},"source":["# filter_accepted_words is a function for filtering the words depending of its closeness to the ref word to correct. A threshold is therefore set to select the best matching words\n","def filter_accepted_words(w, threshold):\n","  \"The threshold set the limit above which a word is considerated close enough to the ref word for its correction\"\n","  if (w[1])>= threshold:\n","    return True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXtBzGpn6Zxp","colab_type":"text"},"source":["## Words List Proposition \n","​\n","This is the main part for proposing a list of candidate words to be a replacement of non-existing word. This function is basically called after the dictionnary is built to compare the non-existing word with the list of existing words. This comparison is done using the \"*dictionnary['gram']*\" collection. Indeed, the idea is to list all the gram of the non-existing word and for each of them look at the list of words corresponding present in \"*dictionnary['gram'][< gram_considered >]*\". For each word in the list, we should increment the number of common gram. At the end, we get, for each candidate word, a result of the number of common grams with the non-existing word. Of course, the higher is the number the more probabilty is that this candidate word is the best candidate for replacement.\n","\n","Below, the function accepts 5 parameters:\n","* *word*: It represents the original word to be replaced. We basically take this initial word to see what are the grams of this word and then we look for each gram, the list of posibbile words and increment the common gram value for each of those possible words. \n","* *w_dict*: It represents the dictionnary. It is used for having the correspondance between gram and words; but also to get the the list of short word w_dict['shortw'].\n","* *ngram*: It represents the gram length. We use it to calculate the list of the orignal word gram.\n","* *w_delta_len*: This parameter offer the possibility to select possible words replacement within a certain character range. Typically if this parameter is 1, then we allow the candidate word to have one more or one less character from the initial word: It includes then missing or addition of characters in the init words. By default, this parameter is set to 0. This means that we are considering only character swapping as mistake.\n","* *similarity*: This parameter should have a value from 0 to 1. It is used to give flexibility for the selection of candidates word. Typically, if set to 0, we accept all possible candidate words which has even only on gram in common with the original word. But if set to 1, we select only the top possible candidate words (The one with the most gram in common with the original word). By default, this parameter is set to 0.5.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2N34TJzL1psW","colab_type":"code","colab":{}},"source":["\n","      \n","# words_proposition is a function which provide a list of possible words which could replace the ref word to be corrected\n","def words_proposition(word, w_dict, ngram=2, w_delta_len=0, similarity=0.5):\n","  \"\"\"To propose the list of appropriate words, we count the number of common gram that the word has with the ref word (The word which needs to be corrected):\n","     the more grams, the word has in common with the ref word, the more likely it is as the best candidate for substitution of ref\"\"\"\n","  \n","  len_word = len(word)\n","  \n","  # if the word is inferior or equal to ngram, the gram process is not suitable for those: We have to take the short word list from dictionary\n","  # Later on this short word list will be treated the word calculation distance\n","  if (len_word <= ngram):\n","    accepted_w_lists = w_dict['shortw']\n","  else:  # Now the word has a sufficient length to be treated gram by gram\n","    # Create a dict of words containing common gram with the ref; the value of dict corresponds to the number of common gram\n","    word_common_gram = collections.defaultdict(int)\n","    # Below, we have a loop which goes to every gram of ref word (ngrams_gen(word,ngram))\n","    for g in ngrams_gen(word,ngram):\n","      # Now, we go to each word of the corresponding ref word common gram\n","      for w in w_dict['gram'][g]:\n","        # we pickup only substitute candidate words which has the same length as the ref word (provided a delta word length which is set by default to 0 \n","        # -- This means by default, we take care only the mispelling and not ommision or addition of letters)  \n","        if len(w) >= len_word - w_delta_len and len(w) <= len_word + w_delta_len:\n","          # we count the number of common gram with the ref word\n","          word_common_gram[w] += 1\n","    # We sort the candidates word for substitution by descending order (The best candidate is first)\n","    sort_word_oc = sorted(word_common_gram.items(), key=lambda d: d[1], reverse=True)\n","    # Below, we try to filter the most relevant words: Typically, we select the best candidate value and we allow a certain margin from the best candidate (by default this margin is set to 50% -- similarity=0.5)\n","    accepted_threshold = sort_word_oc[0][1] * similarity\n","    # We use the filter_accepted_words function define earlier to filter, the best candidates\n","    accepted_w_lists = list(filter(lambda l: filter_accepted_words(l,accepted_threshold), sort_word_oc))\n","  \n","  print('Words candidate for correction of word ', '\"' + word + '\"', ' using ngram technique: ', accepted_w_lists)\n","  return accepted_w_lists"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ViWzOFcc3jMO","colab_type":"text"},"source":["# Word Distance\n","\n","In this part, we implement a solution to calculate the distance between the candidate words and the original one. The idea is that once we get the list of words proposition, we have to evaluate \"how far\" each candidate is from the original word and then return the candidates which are the closest to the original word.\n","\n","We have 2 possible function to calculate this word distance:\n","1. **gram distance**: This function return the candidate with the highest ratio of the following coefficient: 2*nb_of_common_grams/(nb_of_gram_from_candidate + nb_of_gram_from_original).\n","1. **Levenshtein distance**: This function return the candidate with the highest ratio of the following coefficient: 1/lev_distance.\n","\n","We will go to each of these 2 ways of metric calculation."]},{"cell_type":"markdown","metadata":{"id":"aLo6e8LQNopc","colab_type":"text"},"source":["## Gram distance  \n","​\n","The Gram distance is obtained by the ratio between the common grams number of the 2 considered words and their total number of gram together: 2*nb_of_common_grams/(nb_of_gram_from_candidate + nb_of_gram_from_original). After calculating the distance value of each candidate words and store the tuple (candidate_word,gram_weight), we sort the list from the highest candidate to the lowest. We return then the word candidate with the highest score gram.\n","\n","Below, the function for gram distance accepts 3 parameters:\n","* *word*: It represents the original word. This parameter is used to caculate its length (in order to estimate the number of gram in the original word)\n","* *acc_w_list*: It represents the list of candidate words for replacement.\n","* *ngram*: It is the gram length. By default, it is set to 2 which means that it is a bigram\n","\n","**Nota Bene:** Actually, this gram distance is not used in our Assignment. Indeed, the Levenshtein distance was preferred compared to this solution since with the Levenshtein distance, we can add a granularity on the weight between 2 letters. We keep this function since it was the first function implemented to calculate the distance between the candidate and the original word\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"P0MZkjScm30n","colab_type":"code","colab":{}},"source":["# The function gram_distance is not used by our proram but is dedicated to calculate the similarity index using a formula with the number of common grams \n","def gram_distance(word,acc_w_list,ngram=2):\n","  \"This function using the number of common grams calculated in the function word_proposition, calculate an index of similarity with the ref word\"\n","  selected_words = []\n","  for w,ng in acc_w_list:\n","    selected_words.append((w,2*ng/(len(w)-ngram+1+len(word)-ngram+1)))\n","  \n","  sort_selected_words = sorted(selected_words, key=lambda d: d[1], reverse=True)\n","  best_result = sort_selected_words[0][1]\n","  best_words = list(filter(lambda l: filter_accepted_words(l,best_result), sort_selected_words))\n","  return best_words\n","                      \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLwfI0lKNfnm","colab_type":"text"},"source":["## Levenshtein distance  \n","​\n","The Levenshtein distance is here weighted since we add special weight between 2 letters. indeed, special weight are attributed in the case of letters which are considered as very close such as 'i' and 'l' or 'v' and 'u' (The list of letters considered as closed are present in the code). This allows us to add a certain granularity in the calculation of the word distance since a spelling mistake has not the same weight depending on the letter used.\n","\n","To calculate the Levenshtein distance, we use the \"*lev*\" function from the library \"*weighted_levenshtein*\". After calculating the distance value of each candidate words, we store the tuple (candidate_word,1/lev_dist) in our list of best candidate selection \"*best_words*\". You can note that we are storing the inverse of Levenshtein distance to have the highest score for the closest word candidate. Then, we sort the list from the highest candidate to the lowest and return then the word candidate with the highest score 1/lev.\n","\n","Below, the function for Levenstein distance accepts 2 parameters:\n","* *word*: It represents the original word. This parameter is used to caculate the Levenshtein distance with one word of the candidate list.\n","* *acc_w_list*: It represents the list of candidate words for replacement.\n"]},{"cell_type":"code","metadata":{"id":"RPwkB9vtv4hc","colab_type":"code","outputId":"e2f9e9cb-12da-4bbb-9de9-34c1d81a9487","executionInfo":{"status":"ok","timestamp":1572180608872,"user_tz":0,"elapsed":13683,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pip install weighted-levenshtein\n","import numpy as np\n","from weighted_levenshtein import lev, osa, dam_lev\n","\n","# wlev_distance is a function which implements the Levenshtein distance between words (ref word and its candidate for substitution) and build a sorted list of the possible candidates\n","def wlev_distance(word,acc_w_list):\n","  \"This function uses the weighted_levenshtein library to implement the Levenshtein distance of calculates it for each candidate in comparison of the ref word\"\n","  \n","  # Initialization of substitute cost for letters which are close\n","  # some letters like 'u' and 'v' are considered as very close.\n","  substitute_costs = np.ones((128, 128), dtype=np.float64)\n","  substitute_costs[ord('u'), ord('v')] = 0.25\n","  substitute_costs[ord('v'), ord('u')] = 0.25\n","  substitute_costs[ord('i'), ord('l')] = 0.25\n","  substitute_costs[ord('l'), ord('i')] = 0.25\n","  substitute_costs[ord('a'), ord('o')] = 0.25\n","  substitute_costs[ord('o'), ord('a')] = 0.25\n","  substitute_costs[ord('c'), ord('o')] = 0.25\n","  substitute_costs[ord('o'), ord('c')] = 0.25\n","  substitute_costs[ord('0'), ord('o')] = 0.25\n","  substitute_costs[ord('o'), ord('0')] = 0.25\n","  substitute_costs[ord('g'), ord('q')] = 0.25\n","  substitute_costs[ord('q'), ord('g')] = 0.25\n","  substitute_costs[ord('r'), ord('n')] = 0.25\n","  substitute_costs[ord('n'), ord('r')] = 0.25\n","  substitute_costs[ord('s'), ord('5')] = 0.25\n","  substitute_costs[ord('5'), ord('s')] = 0.25\n","  \n","  # selected_words will store a tuple of candiate word and it Levenshtein distance\n","  selected_words = []\n","  # the ref word is set to lower case just like all the words in the dictionnary  \n","  word_l = word.lower()\n","  # We go through the candidate words list generated previously (in word_proposition for example).\n","  for w,ng in list(acc_w_list):\n","    # Levenshtein distance is applied from ref word to the one in the list (we store in reality the inverse value of Lev distance)\n","    lev_dist = lev(word_l, w, substitute_costs=substitute_costs)\n","    selected_words.append((w,1/lev_dist))\n","  \n","  # We sort the candidates word for substitution by descending order -- The value corresponds to the inv of lev distance (The best candidate is first)\n","  sort_selected_words = sorted(selected_words, key=lambda d: d[1], reverse=True)\n","  # Below, we try to filter the most relevant words: Typically, we select only the top candidate (the ones with the highest score)\n","  best_result = sort_selected_words[0][1]\n","  # We use again the filter_accepted_words function define earlier to select only the top candidates\n","  best_words = list(filter(lambda l: filter_accepted_words(l,best_result), sort_selected_words))\n","  \n","  print('Words Top candidates for correction of word ', '\"' + word + '\"', '  after Levenshtein distance calculation: ', best_words)\n","  # We retrun only the candidates with the best score\n","  return best_words\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: weighted-levenshtein in /usr/local/lib/python3.6/dist-packages (0.2.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DMDNzx3Z8BKS","colab_type":"text"},"source":["# Best Word Correction Solution\n","\n","In this part, we implement a solution in order to solve tie between multiple candidate word for replacement. Typically, after calculating the word distance (with the Levenshtein distance for example), we might have more than one candidate which have the best distance. In his case, we should choose the appropriate word to return the best word. For this, we have 2 possibilities to choose the best word:\n","* **occurence**: The idea is to return the candidate word which has the most occurence in the text (corpus text). We use in this case the element dict['count'] of the corpus dictionary which has the correspondance between a word and its occurence in the text.\n","* **contextual**: The idea is to return the candidate word for which the context in the text is closer to the context in the sentence to be corrected. We use in this case the element dict['txt'] of the corpus dictionary and look the word before and after the candidate word and compare them with the word before and after the original word to be corrected in its sentence. If one of those words match then we increment the context_weight. It is in a way a simpified **bigram** process at sentence level to determine the context. We return the word which has the highest context weight.\n","\n","The function below support 5 parameters:\n","1. *t_position*: It represents the position of the word to be corrected in the tokenized list of the original sentence. This parameter is only used when we selct the best word solution in **contextual** mode as we have to look before and after the word to see its context. \n","1. *t_string*: It represents the tokenized list of the original sentence. This parameter is only used when we selct the best word solution in **contextual** mode just like the parameter above.\n","1. *wordsList*: It represents the list of candidate words to replace the original word. This list comes from the word distance calculation (Levenshtein distance) and the idea is to select the best word for replacement using the **occurence** mode or the **contextual** mode to solve the tie between them after calculating the distance. if there is only one word in the list, we just have to return that word.\n","1. *wordsDict*: It represents the dictionary we have built earlier. We use this dictionary for checking the occurence of the word in **occurence** mode or to see the context before and after a candidate word in **contextual** mode.\n","1. *selectType*: It represents the selection type of the best word candidate for replacement. It can be either \"*occurence*\" or \"*contextual*\". \n"]},{"cell_type":"code","metadata":{"id":"0jdHqnih2OvQ","colab_type":"code","colab":{}},"source":["import collections\n","import numpy as np\n","\n","# After calculating the Levenshtein distance, we may have multiple candidates word. We then have to take one of them.\n","# The idea of best_word_correction is to select the candidate with the highest occurance in the dictionnary.\n","def best_word_correction(t_position, t_string, wordsList, wordsDict, selectType='contextual'):\n","  \"If there are many candidates, The function return the best candidate accordingly to its occurance in the text used for the dict.  \"\n","  \n","  # if there is only one possibility then return this candidate\n","  if len(wordsList) == 1:\n","    best_word = wordsList[0][0]\n","  # otherwise, we have to look the number of occurence of the words in the text\n","  elif selectType=='occurence':\n","    selected_words = []\n","    # for each candidate we see its occurence ...\n","    for w,ng in wordsList:\n","      # ... and store them in the selected_words as tuple\n","      selected_words.append((w,wordsDict['count'][w]))\n","    # At the end we need just to sort in decreasing order (the ones with most occurence is 1st in the list)\n","    sort_selected_words = sorted(selected_words, key=lambda d: d[1], reverse=True)\n","    print('Words Top candidates for correction of word ', '\"' + t_string[t_position] + '\"', '  after using \"occurence\" mode: ', sort_selected_words)\n","    # We return the top word as a solution for the best word.\n","    best_word = sort_selected_words[0][0]\n","    \n","  elif selectType=='contextual':\n","    # Converting the dict into numpy array\n","    np_dict = np.array(dict_corpus['txt'])\n","    # we extract the word before and after the original word for comparison\n","    word_before = t_string[t_position-1].lower() if t_position > 0 else ' @ ' \n","    word_after = t_string[t_position+1].lower() if t_position < len(t_string)-1 else ' @ ' \n","    context_weight = collections.defaultdict(int)\n","    # for each candidate we see its context ...\n","    for w,ng in wordsList:\n","      # We extract all the position of the candidate word in the dict text\n","      np_ind_w = np.where(np_dict == w)[0]\n","      # then , we check the word before and after the candidate word\n","      np_context_before = np_dict[np_ind_w-1]\n","      np_context_after = np_dict[np_ind_w+1]\n","      # we compare them with the words before and after the original one and see how many are they\n","      context_weight_before = len(np_context_before[np_context_before==word_before])\n","      context_weight_after = len(np_context_after[np_context_after==word_after])\n","      # the context weight is calculated with the number of similar (before and after) words in comparison to the ones with the original words \n","      context_weight[w] = context_weight_before + context_weight_after\n","    # We sort the candidates word for substitution by descending order depending on the weight of the context (The best candidate is first)\n","    sort_context_weight = sorted(context_weight.items(), key=lambda d: d[1], reverse=True)\n","    print('Words Top candidates for correction of word ', '\"' + t_string[t_position] + '\"', '  after using \"context\" mode: ', sort_context_weight)\n","    best_word = sort_context_weight[0][0]\n","    \n","  else:\n","    best_word = wordsList[0][0]\n","    \n","  print('Best word for replacement: ', best_word)\n","  return best_word\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kzXf06FbK_fD","colab_type":"text"},"source":["# Sentence Correction\n","\n","This part is the main process of our **Error Correction Module**. Indeed, the concept is to go through each word of the original given string and propose a correction of it if it is not in the dictionnary. The full process of this Sentence Correction is described below:\n","1. Go to every token of the original sentence string '*t*'.\n","1. Set the token to lower case '*tl*'\n","1. If '*tl*' is a word and if it is not present in the dict, check the words correction proposition with the function '*words_proposition*'\n","1. Then, use the Levenshtein distance between the words candidate list and the original word thanks to the function '*wlev_distance*'\n","1. After, we select the best choice of word replacement using the function '*best_word_correction*' in contextual mode\n","1. All the corrected word (original or corrected) is appended into the list which forms the corrected sentence from the original string.\n","\n","The function below accepts 6 parameters:\n","* *token_sentence*: It represents the tokenized original string sentence which needs to be corrected. We basically go through each token of the string to check if it is present in the dictionary and eventually propose a correction if not. \n","* *w_dict*: It represents the dictionnary build initially from a reference text (like corpus). It is the base of the check of the existence of the word\n","* *ngram*: It represents the gram length. We use it to calculate the list of the orignal word gram.\n","* *w_len_mode*: This parameter offer the possibility to select possible words replacement within a certain character range. Typically if this parameter is 1, then we allow the candidate word to have one more or one less character from the initial word: It includes then missing or addition of characters in the init words. By default, this parameter is set to 0. This means that we are considering only character swapping as mistake.\n","* *similarity*: This parameter should have a value from 0 to 1. It is used to give flexibility for the selection of candidates word. Typically, if set to 0, we accept all possible candidate words which has even only on gram in common with the original word. But if set to 1, we select only the top possible candidate words (The one with the most gram in common with the original word). By default, this parameter is set to 0.5.\n","* *selectType*: It represents the selection type of the best word candidate for replacement. It can be either \"*occurence*\" or \"*contextual*\". \"*occurence*\" will propose the word for replacement based on its occurence in the ref text (corpus) whereas, \"*contextual*\" witll propose the word based on the context of the word.\n","\n"]},{"cell_type":"code","metadata":{"id":"TfRsWur97yK5","colab_type":"code","colab":{}},"source":["import re\n","\n","# sentence_correction is the function which goes through each word of the sentence to check it existence in the dict and eventually propose a correction\n","def sentence_correction(token_sentence, w_dict, ngram, w_len_mod=0, similarity=0.5, selectType='contextual'):\n","  \"This function is the body of the code, since it evaluates for each word of a given sentence its existence in the dict and apply a suggestion of correction \"\n","  \n","  # corrected_sentence is the final tokenized sentence after replacement of non-existing word\n","  corrected_sentence = []\n","  t_position = 0\n","  \n","  # we go through each element of the original reference sentence. (the element could be a non-word such as a punctuation)\n","  for t in token_sentence:\n","    # we convert the element to lower case\n","    tl = t.lower()\n","    # And we check if this element is a potential word and if it is in the dict\n","    if re.match(r\"\\w+\",tl) and w_dict['count'][tl] == 0:\n","      # if the word is not in the list, we apply the words_suggestion function to give potential words candidate for replacement\n","      words_suggestion = words_proposition(tl, w_dict, ngram, w_len_mod, similarity)\n","      # we refine the suggested candidates by applying the Lev distance\n","      words_list = wlev_distance(tl,words_suggestion)\n","      # we return the best choice according to the word occurence in the dict\n","      #corrected_word = best_word_correction(words_list,w_dict['count'])\n","      corrected_word = best_word_correction(t_position, token_sentence, words_list, w_dict, selectType)\n","      # we append this word in the corrected sentence\n","      corrected_sentence.append(corrected_word)\n","    else:\n","      # if the element is a non-word or if it is in the dict, reuse it\n","      corrected_sentence.append(t)\n","    t_position += 1\n","  \n","  print('Tokenized and expanded sentence before correction: ', token_sentence)\n","  print('Tokenized and expanded sentence after correction: ', corrected_sentence)\n","  \n","  # return the sentence with the proposed correction\n","  return corrected_sentence\n","      \n","      \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnKNa7GFYrXK","colab_type":"text"},"source":["# Contraction of the solution string\n","\n","The concept of the solution string contraction is to contract the words which were originally contracted in the initial string to keep its original form. But in our code this part is not just used to contract word; it also tries to keep the same formatting as the initial string. Typically, if the word has an upper case, we try to keep this uppercase.\n","\n","In the following code, a solution for **Contracting of the solution sentence** is proposed.\n","\n","The idea is to use the the tokenized initial string (with contraction) and the new soution string (without contraction) and then make a comparison between them to check which tokens ar different. Typically, if one token of original string starts with the character \"'\", and not in the solution string, then we have a case of contracted word.\n","\n","We do the same process for word with upper case and lower case, and we can have at the end a solution string which keeps the same formatting as the original sentence. \n","\n","The function for this contraction function has 2 parameters:\n","* *new_st*: it corresponds to the new solution string which need to be worked to get the same formatting as the original string.\n","* *init_st*: it corresponds tto the original string which is the model of formatting to be used."]},{"cell_type":"code","metadata":{"id":"7WjR3d38-FmW","colab_type":"code","colab":{}},"source":["import re\n","\n","# contract_to_init_s is a function which set again the contracted words to its original state and keep the capital letters when it can\n","def contract_to_init_s(new_st, init_st):\n","  \"This function takes as input the original sentence tokenized with the tokenized new solution and then checks what word have been contracted and which one as an upercase an try to keep it\"\n","  \n","  # result_token is the final sentence which keeps upper case and contraction\n","  result_token = []\n","  \n","  # In this loop we are going through each element of the tokenized solution in parallel with the tokenized original sentence\n","  for n,i in zip(new_st, init_st):\n","    if re.match(r\"^'\\w+\",i):             # We check for word starting with \"'\" following by at least a letter word which means that it is a contracted word\n","      result_token.append(i)             # if it is a contracted word we add it in the result\n","    elif(i.lower() == n and not i.islower()): # then we check if there is a capital letter in the original word which is identical word as the solution word\n","      result_token.append(i)                  # if it is the case we keep the original word\n","    elif(len(i) == len(n) and not i.islower()): # if the length of original and new word are same, we keep the upper case to the same position letter for the new word (here new word and old word are not same -- it has been corrected)\n","      n_tmp=[]\n","      for k in range(len(i)):\n","        if i[k].isupper():\n","          n_tmp.append(n[k].upper())\n","        else:\n","          n_tmp.append(n[k])\n","      result_token.append(\"\".join(n_tmp))\n","    elif(not i.islower()):                 # if the length of new word and original word is different (and they are not identical and does not have same lengh), we keep only the 1st upper case letter if it is in original word\n","      n_tmp=[]\n","      if i[0].isupper():\n","        n_tmp.append(n[0].upper())\n","      else:\n","        n_tmp.append(n[0])\n","      for k in range(1, len(n)):\n","        n_tmp.append(n[k])\n","      result_token.append(\"\".join(n_tmp))\n","    else:\n","      result_token.append(n)              # End result of sentence after keeping contraction and upper case.\n","\n","  print('Contracted and formatted sentence after correction: ', result_token)\n","  \n","  # return the result of the solution\n","  return result_token"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kq_lUxD3Y3_j","colab_type":"text"},"source":["# Detokenization of the solution string\n","\n","For the detokenization process, the idea is to merge all the tokens to get at the end a proper string sentence.\n","\n","In the following code, a solution for **Detokenizing of the solution sentence** is proposed.\n","\n","This time, we use the function \"*TreebankWordDetokenizer.detokenize*\" from library \"*nltk.tokenize.treebank*\". In our code, we build a function which load the string to be detokenized and then detokenize it by applying the nltk.tokenize.treebank function.\n"]},{"cell_type":"code","metadata":{"id":"NhhONNsM5kgG","colab_type":"code","colab":{}},"source":["from nltk.tokenize.treebank import TreebankWordDetokenizer\n","\n","# detokenizeTxt is a function which revert the tokenisation operation (it untokenizes)\n","def detokenizeTxt(t):\n","  \"this function uses the nltk.tokenize.treebank package to untokenize the sentence\"\n","  detoken = TreebankWordDetokenizer().detokenize(t)\n","  \n","  print('Untokenized solution sentence: ', detoken)\n","  \n","  # return the untokenized sentence\n","  return detoken"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wu_SRKLz3NqS","colab_type":"text"},"source":["# Error Correction Module\n","\n","**This section corresponds to the final solution of our assignement: \"*An Error Correction Module*\" is proposed.**\n","\n","**The function calls the different function above in order to return at the end the corrected sentence**\n"]},{"cell_type":"code","metadata":{"id":"mhVM_ckn1sS1","colab_type":"code","colab":{}},"source":["\n","def Error_Correction_Module(sentence, dict_ref, ngram, word_delta_len_modulation=0, word_similarity_acceptation=0.5, word_selection_type='contextual'):\n","  d_sentence = decontractSentence(sentence)          # the sentence is decontracted \n","  init_sentence = tokenizeTxt(sentence)              # we tokenize the original sentence to use it at the end for contracting the solution sentence\n","  tokenized_sentence = tokenizeTxt(d_sentence[0])    # then, it is tokenized\n","  new_sentence = sentence_correction(tokenized_sentence, dict_ref, ngram, word_delta_len_modulation, word_similarity_acceptation, word_selection_type) # we launch our sentence correction solution\n","  contracted_sentence = contract_to_init_s(new_sentence, init_sentence) # then we contract the obtained solution to keep as much as possible the sam contraction and upper case letter thant the original sentence\n","  final_sentence=detokenizeTxt(contracted_sentence)                     # final result sentence is ready\n","  return final_sentence\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UodNGcbfqTyH","colab_type":"text"},"source":["### Building Dictionnary : Corpus database\n","\n","After implementing the Error Correction module, it is time to do some test on our module.\n","\n","We should first build our dictionary using the corpus text in english. For that, we use the function \"*words_dict*\" implemented previously.\n"]},{"cell_type":"code","metadata":{"id":"lxQKgSlSAIx5","colab_type":"code","outputId":"6f64e140-4b45-4638-a7c6-272790053481","executionInfo":{"status":"ok","timestamp":1572180646888,"user_tz":0,"elapsed":51600,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["corpus_file = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/europarl/txt/en/all_en.txt'  # now we will consider the corpus file as input  \n","\n","dict_corpus = words_dict(corpus_file, 2)                              # we build our dictionnary\n","  "],"execution_count":16,"outputs":[{"output_type":"stream","text":["Read dictionnary\n","Counting words dictionnary\n","Number of words in dict      :  78600\n","Number of total words in text:  58440476\n","Creating gram collection\n","Number of grams:  675\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ioes46SQpZC3","colab_type":"text"},"source":["### Error Correction Module Test\n","\n","Below, we will try different sentence to see the behavior of the Error Correction Module.\n","\n","The idea is to test 3 different sentences (with some mispelled words) and see the proposed solution of our Error Correction Module. The 3 sentences are:\n","\n","1. \"*He ta**b**es his time to pay his ta**b**es*\" : For this sentence the expected result should be \"He ta**k**es his time to pay his ta**x**es\". \n","\n","1. \"*He **k**s with **k**s*\" : For this sentence the expected result should be \"He **i**s with **u**s\". \n","\n","1. \"*I wa**f**ted outside for hours and I wa**f**ted my time*\" : For this sentence the expected result should be \"I wa**i**ted outside for hours and I wa**s**ted my time\". \n","\n","In the 3 above cases, we have a very interesting sentence to test since each of them have identical mispelled words for 2 different existing words. Typically,\n","* For the 1st sentence, we have \"*tabes*\" for respectively \"*takes*\" and \"*taxes*\"\n","* For the 2nd sentence, we have \"*ks*\" for respectively \"*is*\" and \"*us*\"\n","* For the 3rd sentence, we have \"*wafted*\" for respectively \"*waited*\" and \"*wasted*\"\n","\n","In those cases, the Levenshtein distance between the mispelled word and the correct one is same for both corrected words. Therefore, we should get multiple solutions after the Levenshtein word distance calculation with the function \"*wlev_distance*\".  The function \"*best_word_correction*\" should therefore manage to determine what is the optimal solution between those different possible correct words using the 2 possible modes we have implemented the \"*occurence*\" mode and the \"*conceptual*\" mode. \n","\n","The 2nd sentence is also stressing the mispelling of a short words like \"*is*\" or \"*us*\". Indeed, we are using bigram matching pattern to make our proposition of corrected words which in theory should not work not very well for short words.\n","\n","Our work is to check now how the Error Module is solving those mistakes: \n"]},{"cell_type":"code","metadata":{"id":"UqwwHPYw7PE8","colab_type":"code","outputId":"878300e5-312c-487c-dfac-e49aff226ec3","executionInfo":{"status":"ok","timestamp":1572181115760,"user_tz":0,"elapsed":520431,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","# below we apply our methodology for word correction in various test sentences\n","\n","orig_sentence= 'He tabes his time to pay his tabes'\n","orig_sentence2= 'He ks with ks'\n","orig_sentence3= 'I wafted outside for hours and I have wafted my time'\n","  \n","ngram = 2                                                             # we are considering the bigram for our solution\n","word_delta_len_modulation = 0                                         # we are not accepting any delta letter more or less than the original word (only spelling mistake is considered)\n","word_similarity_acceptation = 0.5                                     # we only propose a similarity of 50% from the best proposition for our range of proposed word for correction\n","\n","print('')\n","print('###### TEST ON 1st SENTENCE: ' + '\"' + orig_sentence + '\"')\n","print('## On occurence mode' )\n","result_sentence1_occ = Error_Correction_Module(orig_sentence, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, 'occurence')\n","print('')\n","print('## On contextual mode' )\n","result_sentence1_context = Error_Correction_Module(orig_sentence, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, 'contextual')\n","print('')\n","print('## Final Result' )\n","print(\"Original sentence 1:                     \", orig_sentence)\n","print(\"Result of sentence 1 in occurence mode:  \", result_sentence1_occ)\n","print(\"Result of sentence 1 in contextual mode: \", result_sentence1_context)\n","\n","print('')\n","print('')\n","print('###### TEST ON 2nd SENTENCE: ' + '\"' + orig_sentence2 + '\"')\n","print('## On occurence mode' )\n","result_sentence2_occ = Error_Correction_Module(orig_sentence2, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, 'occurence')\n","print('')\n","print('## On contextual mode' )\n","result_sentence2_context = Error_Correction_Module(orig_sentence2, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, 'contextual')\n","print('')\n","print('## Final Result' )\n","print(\"Original sentence 2:                     \", orig_sentence2)\n","print(\"Result of sentence 2 in occurence mode:  \", result_sentence2_occ)\n","print(\"Result of sentence 2 in contextual mode: \", result_sentence2_context)\n","\n","print('')\n","print('')\n","print('###### TEST ON 3rd SENTENCE: ' + '\"' + orig_sentence3 + '\"')\n","print('## On occurence mode' )\n","result_sentence3_occ = Error_Correction_Module(orig_sentence3, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, 'occurence')\n","print('')\n","print('## On contextual mode' )\n","result_sentence3_context = Error_Correction_Module(orig_sentence3, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, 'contextual')\n","print('')\n","print('## Final Result' )\n","print(\"Original sentence 3:                     \", orig_sentence3)\n","print(\"Result of sentence 3 in occurence mode:  \", result_sentence3_occ)\n","print(\"Result of sentence 3 in contextual mode: \", result_sentence3_context)\n","\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\n","###### TEST ON 1st SENTENCE: \"He tabes his time to pay his tabes\"\n","## On occurence mode\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Sentence to expand: \"He tabes his time to pay his tabes\"\n","Sentence after expansion: \"He tabes his time to pay his tabes\"\n","Sentence before tokenization: \"He tabes his time to pay his tabes\"\n","Sentence after tokenization: \" ['He', 'tabes', 'his', 'time', 'to', 'pay', 'his', 'tabes'] \"\n","Sentence before tokenization: \"He tabes his time to pay his tabes\"\n","Sentence after tokenization: \" ['He', 'tabes', 'his', 'time', 'to', 'pay', 'his', 'tabes'] \"\n","Words candidate for correction of word  \"tabes\"  using ngram technique:  [('tabet', 3), ('babes', 3), ('stabi', 2), ('sabet', 2), ('haben', 2), ('abeed', 2), ('abera', 2), ('tabou', 2), ('habet', 2), ('abeto', 2), ('stabs', 2), ('abbey', 2), ('zabel', 2), ('haber', 2), ('labem', 2), ('taboo', 2), ('abbed', 2), ('label', 2), ('abeba', 2), ('babel', 2), ('table', 2), ('jaber', 2), ('obese', 2), ('beset', 2), ('berta', 2), ('besis', 2), ('pdbes', 2), ('beste', 2), ('beata', 2), ('besim', 2), ('benes', 2), ('vibes', 2), ('kubes', 2), ('cubes', 2), ('gibes', 2), ('bests', 2), ('beres', 2), ('bessa', 2), ('robes', 2), ('tubes', 2), ('jibes', 2), ('testa', 2), ('takes', 2), ('estai', 2), ('taxes', 2), ('tapes', 2), ('tares', 2), ('staes', 2), ('tales', 2), ('taves', 2), ('festa', 2)]\n","Words Top candidates for correction of word  \"tabes\"   after Levenshtein distance calculation:  [('tabet', 1.0), ('babes', 1.0), ('tubes', 1.0), ('takes', 1.0), ('taxes', 1.0), ('tapes', 1.0), ('tares', 1.0), ('tales', 1.0), ('taves', 1.0)]\n","Words Top candidates for correction of word  \"tabes\"   after using \"occurence\" mode:  [('takes', 7239), ('taxes', 2964), ('tales', 43), ('tubes', 17), ('tapes', 14), ('tabet', 4), ('babes', 4), ('tares', 1), ('taves', 1)]\n","Best word for replacement:  takes\n","Words candidate for correction of word  \"tabes\"  using ngram technique:  [('tabet', 3), ('babes', 3), ('stabi', 2), ('sabet', 2), ('haben', 2), ('abeed', 2), ('abera', 2), ('tabou', 2), ('habet', 2), ('abeto', 2), ('stabs', 2), ('abbey', 2), ('zabel', 2), ('haber', 2), ('labem', 2), ('taboo', 2), ('abbed', 2), ('label', 2), ('abeba', 2), ('babel', 2), ('table', 2), ('jaber', 2), ('obese', 2), ('beset', 2), ('berta', 2), ('besis', 2), ('pdbes', 2), ('beste', 2), ('beata', 2), ('besim', 2), ('benes', 2), ('vibes', 2), ('kubes', 2), ('cubes', 2), ('gibes', 2), ('bests', 2), ('beres', 2), ('bessa', 2), ('robes', 2), ('tubes', 2), ('jibes', 2), ('testa', 2), ('takes', 2), ('estai', 2), ('taxes', 2), ('tapes', 2), ('tares', 2), ('staes', 2), ('tales', 2), ('taves', 2), ('festa', 2)]\n","Words Top candidates for correction of word  \"tabes\"   after Levenshtein distance calculation:  [('tabet', 1.0), ('babes', 1.0), ('tubes', 1.0), ('takes', 1.0), ('taxes', 1.0), ('tapes', 1.0), ('tares', 1.0), ('tales', 1.0), ('taves', 1.0)]\n","Words Top candidates for correction of word  \"tabes\"   after using \"occurence\" mode:  [('takes', 7239), ('taxes', 2964), ('tales', 43), ('tubes', 17), ('tapes', 14), ('tabet', 4), ('babes', 4), ('tares', 1), ('taves', 1)]\n","Best word for replacement:  takes\n","Tokenized and expanded sentence before correction:  ['He', 'tabes', 'his', 'time', 'to', 'pay', 'his', 'tabes']\n","Tokenized and expanded sentence after correction:  ['He', 'takes', 'his', 'time', 'to', 'pay', 'his', 'takes']\n","Contracted and formatted sentence after correction:  ['He', 'takes', 'his', 'time', 'to', 'pay', 'his', 'takes']\n","Untokenized solution sentence:  He takes his time to pay his takes\n","\n","## On contextual mode\n","Sentence to expand: \"He tabes his time to pay his tabes\"\n","Sentence after expansion: \"He tabes his time to pay his tabes\"\n","Sentence before tokenization: \"He tabes his time to pay his tabes\"\n","Sentence after tokenization: \" ['He', 'tabes', 'his', 'time', 'to', 'pay', 'his', 'tabes'] \"\n","Sentence before tokenization: \"He tabes his time to pay his tabes\"\n","Sentence after tokenization: \" ['He', 'tabes', 'his', 'time', 'to', 'pay', 'his', 'tabes'] \"\n","Words candidate for correction of word  \"tabes\"  using ngram technique:  [('tabet', 3), ('babes', 3), ('stabi', 2), ('sabet', 2), ('haben', 2), ('abeed', 2), ('abera', 2), ('tabou', 2), ('habet', 2), ('abeto', 2), ('stabs', 2), ('abbey', 2), ('zabel', 2), ('haber', 2), ('labem', 2), ('taboo', 2), ('abbed', 2), ('label', 2), ('abeba', 2), ('babel', 2), ('table', 2), ('jaber', 2), ('obese', 2), ('beset', 2), ('berta', 2), ('besis', 2), ('pdbes', 2), ('beste', 2), ('beata', 2), ('besim', 2), ('benes', 2), ('vibes', 2), ('kubes', 2), ('cubes', 2), ('gibes', 2), ('bests', 2), ('beres', 2), ('bessa', 2), ('robes', 2), ('tubes', 2), ('jibes', 2), ('testa', 2), ('takes', 2), ('estai', 2), ('taxes', 2), ('tapes', 2), ('tares', 2), ('staes', 2), ('tales', 2), ('taves', 2), ('festa', 2)]\n","Words Top candidates for correction of word  \"tabes\"   after Levenshtein distance calculation:  [('tabet', 1.0), ('babes', 1.0), ('tubes', 1.0), ('takes', 1.0), ('taxes', 1.0), ('tapes', 1.0), ('tares', 1.0), ('tales', 1.0), ('taves', 1.0)]\n","Words Top candidates for correction of word  \"tabes\"   after using \"context\" mode:  [('takes', 61), ('tabet', 0), ('babes', 0), ('tubes', 0), ('taxes', 0), ('tapes', 0), ('tares', 0), ('tales', 0), ('taves', 0)]\n","Best word for replacement:  takes\n","Words candidate for correction of word  \"tabes\"  using ngram technique:  [('tabet', 3), ('babes', 3), ('stabi', 2), ('sabet', 2), ('haben', 2), ('abeed', 2), ('abera', 2), ('tabou', 2), ('habet', 2), ('abeto', 2), ('stabs', 2), ('abbey', 2), ('zabel', 2), ('haber', 2), ('labem', 2), ('taboo', 2), ('abbed', 2), ('label', 2), ('abeba', 2), ('babel', 2), ('table', 2), ('jaber', 2), ('obese', 2), ('beset', 2), ('berta', 2), ('besis', 2), ('pdbes', 2), ('beste', 2), ('beata', 2), ('besim', 2), ('benes', 2), ('vibes', 2), ('kubes', 2), ('cubes', 2), ('gibes', 2), ('bests', 2), ('beres', 2), ('bessa', 2), ('robes', 2), ('tubes', 2), ('jibes', 2), ('testa', 2), ('takes', 2), ('estai', 2), ('taxes', 2), ('tapes', 2), ('tares', 2), ('staes', 2), ('tales', 2), ('taves', 2), ('festa', 2)]\n","Words Top candidates for correction of word  \"tabes\"   after Levenshtein distance calculation:  [('tabet', 1.0), ('babes', 1.0), ('tubes', 1.0), ('takes', 1.0), ('taxes', 1.0), ('tapes', 1.0), ('tares', 1.0), ('tales', 1.0), ('taves', 1.0)]\n","Words Top candidates for correction of word  \"tabes\"   after using \"context\" mode:  [('taxes', 440), ('takes', 44), ('tales', 8), ('tubes', 3), ('tapes', 2), ('tares', 1), ('tabet', 0), ('babes', 0), ('taves', 0)]\n","Best word for replacement:  taxes\n","Tokenized and expanded sentence before correction:  ['He', 'tabes', 'his', 'time', 'to', 'pay', 'his', 'tabes']\n","Tokenized and expanded sentence after correction:  ['He', 'takes', 'his', 'time', 'to', 'pay', 'his', 'taxes']\n","Contracted and formatted sentence after correction:  ['He', 'takes', 'his', 'time', 'to', 'pay', 'his', 'taxes']\n","Untokenized solution sentence:  He takes his time to pay his taxes\n","\n","## Final Result\n","Original sentence 1:                      He tabes his time to pay his tabes\n","Result of sentence 1 in occurence mode:   He takes his time to pay his takes\n","Result of sentence 1 in contextual mode:  He takes his time to pay his taxes\n","\n","\n","###### TEST ON 2nd SENTENCE: \"He ks with ks\"\n","## On occurence mode\n","Sentence to expand: \"He ks with ks\"\n","Sentence after expansion: \"He ks with ks\"\n","Sentence before tokenization: \"He ks with ks\"\n","Sentence after tokenization: \" ['He', 'ks', 'with', 'ks'] \"\n","Sentence before tokenization: \"He ks with ks\"\n","Sentence after tokenization: \" ['He', 'ks', 'with', 'ks'] \"\n","Words candidate for correction of word  \"ks\"  using ngram technique:  {('it', 1), ('cl', 1), ('eq', 1), ('ip', 1), ('ng', 1), ('ot', 1), ('dk', 1), ('af', 1), ('mc', 1), ('mp', 1), ('je', 1), ('ms', 1), ('mt', 1), ('du', 1), ('we', 1), ('ma', 1), ('os', 1), ('nm', 1), ('ti', 1), ('tc', 1), ('fg', 1), ('sk', 1), ('mi', 1), ('xu', 1), ('aa', 1), ('er', 1), ('hr', 1), ('cw', 1), ('nf', 1), ('kw', 1), ('dl', 1), ('ju', 1), ('qi', 1), ('iv', 1), ('em', 1), ('vw', 1), ('xs', 1), ('ie', 1), ('gu', 1), ('oq', 1), ('z', 1), ('cf', 1), ('pl', 1), ('iq', 1), ('o', 1), ('ha', 1), ('nl', 1), ('ci', 1), ('vb', 1), ('y', 1), ('si', 1), ('ds', 1), ('cs', 1), ('ji', 1), ('vs', 1), ('na', 1), ('eg', 1), ('ok', 1), ('um', 1), ('i', 1), ('ho', 1), ('zi', 1), ('ga', 1), ('ww', 1), ('at', 1), ('sg', 1), ('pd', 1), ('tu', 1), ('pe', 1), ('zr', 1), ('hz', 1), ('ec', 1), ('pf', 1), ('vp', 1), ('kg', 1), ('cb', 1), ('va', 1), ('il', 1), ('sl', 1), ('ex', 1), ('bp', 1), ('tv', 1), ('nd', 1), ('lt', 1), ('qa', 1), ('ue', 1), ('ku', 1), ('gg', 1), ('n', 1), ('lp', 1), ('hm', 1), ('og', 1), ('so', 1), ('ax', 1), ('he', 1), ('bt', 1), ('ee', 1), ('cc', 1), ('vf', 1), ('bf', 1), ('uk', 1), ('gm', 1), ('np', 1), ('fr', 1), ('or', 1), ('fn', 1), ('en', 1), ('cm', 1), ('ta', 1), ('ln', 1), ('cv', 1), ('im', 1), ('ns', 1), ('kn', 1), ('ob', 1), ('bu', 1), ('a', 1), ('me', 1), ('r', 1), ('k', 1), ('ry', 1), ('ay', 1), ('vi', 1), ('wa', 1), ('lm', 1), ('am', 1), ('pa', 1), ('of', 1), ('gb', 1), ('sq', 1), ('de', 1), ('my', 1), ('ph', 1), ('eo', 1), ('ii', 1), ('su', 1), ('br', 1), ('dg', 1), ('xy', 1), ('ak', 1), ('oa', 1), ('ko', 1), ('kt', 1), ('gs', 1), ('sp', 1), ('la', 1), ('oh', 1), ('ni', 1), ('xx', 1), ('us', 1), ('co', 1), ('zu', 1), ('no', 1), ('se', 1), ('ac', 1), ('hf', 1), ('ss', 1), ('x', 1), ('lg', 1), ('g', 1), ('j', 1), ('kk', 1), ('c', 1), ('f', 1), ('el', 1), ('op', 1), ('ya', 1), ('wi', 1), ('bn', 1), ('jo', 1), ('fp', 1), ('ce', 1), ('hq', 1), ('om', 1), ('e', 1), ('xv', 1), ('cz', 1), ('ro', 1), ('bv', 1), ('bb', 1), ('lu', 1), ('hl', 1), ('ft', 1), ('pc', 1), ('m', 1), ('pq', 1), ('hy', 1), ('tt', 1), ('bc', 1), ('u', 1), ('ia', 1), ('yu', 1), ('rs', 1), ('by', 1), ('ig', 1), ('gr', 1), ('nx', 1), ('ny', 1), ('cu', 1), ('pg', 1), ('mv', 1), ('to', 1), ('ev', 1), ('nk', 1), ('rt', 1), ('ae', 1), ('po', 1), ('oz', 1), ('ai', 1), ('cg', 1), ('te', 1), ('kv', 1), ('b', 1), ('ik', 1), ('fc', 1), ('km', 1), ('th', 1), ('p', 1), ('cn', 1), ('ba', 1), ('fo', 1), ('ge', 1), ('st', 1), ('oo', 1), ('hu', 1), ('eu', 1), ('ye', 1), ('t', 1), ('gl', 1), ('ep', 1), ('cy', 1), ('ap', 1), ('ou', 1), ('za', 1), ('ab', 1), ('mo', 1), ('ul', 1), ('ir', 1), ('xi', 1), ('rc', 1), ('jp', 1), ('et', 1), ('tb', 1), ('ff', 1), ('xo', 1), ('dr', 1), ('ca', 1), ('bo', 1), ('es', 1), ('as', 1), ('ar', 1), ('ur', 1), ('da', 1), ('sf', 1), ('ja', 1), ('ah', 1), ('gd', 1), ('w', 1), ('vo', 1), ('mr', 1), ('hi', 1), ('yo', 1), ('ag', 1), ('oc', 1), ('oi', 1), ('fa', 1), ('xl', 1), ('pr', 1), ('if', 1), ('di', 1), ('vx', 1), ('lo', 1), ('s', 1), ('bg', 1), ('sv', 1), ('eb', 1), ('gi', 1), ('go', 1), ('od', 1), ('an', 1), ('mg', 1), ('on', 1), ('pv', 1), ('qu', 1), ('ka', 1), ('rn', 1), ('do', 1), ('sn', 1), ('ne', 1), ('ts', 1), ('ki', 1), ('ad', 1), ('pm', 1), ('hp', 1), ('h', 1), ('wu', 1), ('ct', 1), ('ru', 1), ('v', 1), ('dj', 1), ('lb', 1), ('ls', 1), ('un', 1), ('mm', 1), ('ll', 1), ('db', 1), ('ps', 1), ('aj', 1), ('lv', 1), ('fi', 1), ('ea', 1), ('hc', 1), ('jc', 1), ('wb', 1), ('iu', 1), ('nv', 1), ('qc', 1), ('ox', 1), ('ek', 1), ('re', 1), ('sy', 1), ('al', 1), ('mn', 1), ('cd', 1), ('q', 1), ('av', 1), ('le', 1), ('bq', 1), ('bi', 1), ('ui', 1), ('yi', 1), ('ut', 1), ('vu', 1), ('ve', 1), ('ml', 1), ('rd', 1), ('dm', 1), ('mw', 1), ('wo', 1), ('uh', 1), ('cp', 1), ('is', 1), ('sr', 1), ('in', 1), ('gp', 1), ('li', 1), ('au', 1), ('gt', 1), ('td', 1), ('ly', 1), ('fu', 1), ('uv', 1), ('kc', 1), ('mu', 1), ('pp', 1), ('ze', 1), ('ed', 1), ('up', 1), ('mb', 1), ('ix', 1), ('pt', 1), ('ys', 1), ('be', 1), ('dc', 1), ('fm', 1), ('id', 1), ('ib', 1), ('l', 1), ('d', 1), ('oj', 1), ('sa', 1)}\n","Words Top candidates for correction of word  \"ks\"   after Levenshtein distance calculation:  [('ms', 1.0), ('os', 1.0), ('kw', 1.0), ('xs', 1.0), ('ds', 1.0), ('cs', 1.0), ('vs', 1.0), ('kg', 1.0), ('ku', 1.0), ('ns', 1.0), ('kn', 1.0), ('k', 1.0), ('ko', 1.0), ('kt', 1.0), ('gs', 1.0), ('us', 1.0), ('ss', 1.0), ('kk', 1.0), ('rs', 1.0), ('kv', 1.0), ('km', 1.0), ('es', 1.0), ('as', 1.0), ('s', 1.0), ('ka', 1.0), ('ts', 1.0), ('ki', 1.0), ('ls', 1.0), ('ps', 1.0), ('is', 1.0), ('kc', 1.0), ('ys', 1.0)]\n","Words Top candidates for correction of word  \"ks\"   after using \"occurence\" mode:  [('is', 931421), ('as', 397296), ('s', 187761), ('us', 93922), ('es', 2148), ('ms', 1500), ('cs', 1122), ('km', 571), ('k', 235), ('kg', 175), ('ki', 70), ('ss', 27), ('ps', 18), ('ts', 8), ('vs', 7), ('kw', 5), ('kv', 5), ('os', 4), ('gs', 4), ('ka', 3), ('kc', 3), ('ds', 2), ('ns', 2), ('ko', 2), ('xs', 1), ('ku', 1), ('kn', 1), ('kt', 1), ('kk', 1), ('rs', 1), ('ls', 1), ('ys', 1)]\n","Best word for replacement:  is\n","Words candidate for correction of word  \"ks\"  using ngram technique:  {('it', 1), ('cl', 1), ('eq', 1), ('ip', 1), ('ng', 1), ('ot', 1), ('dk', 1), ('af', 1), ('mc', 1), ('mp', 1), ('je', 1), ('ms', 1), ('mt', 1), ('du', 1), ('we', 1), ('ma', 1), ('os', 1), ('nm', 1), ('ti', 1), ('tc', 1), ('fg', 1), ('sk', 1), ('mi', 1), ('xu', 1), ('aa', 1), ('er', 1), ('hr', 1), ('cw', 1), ('nf', 1), ('kw', 1), ('dl', 1), ('ju', 1), ('qi', 1), ('iv', 1), ('em', 1), ('vw', 1), ('xs', 1), ('ie', 1), ('gu', 1), ('oq', 1), ('z', 1), ('cf', 1), ('pl', 1), ('iq', 1), ('o', 1), ('ha', 1), ('nl', 1), ('ci', 1), ('vb', 1), ('y', 1), ('si', 1), ('ds', 1), ('cs', 1), ('ji', 1), ('vs', 1), ('na', 1), ('eg', 1), ('ok', 1), ('um', 1), ('i', 1), ('ho', 1), ('zi', 1), ('ga', 1), ('ww', 1), ('at', 1), ('sg', 1), ('pd', 1), ('tu', 1), ('pe', 1), ('zr', 1), ('hz', 1), ('ec', 1), ('pf', 1), ('vp', 1), ('kg', 1), ('cb', 1), ('va', 1), ('il', 1), ('sl', 1), ('ex', 1), ('bp', 1), ('tv', 1), ('nd', 1), ('lt', 1), ('qa', 1), ('ue', 1), ('ku', 1), ('gg', 1), ('n', 1), ('lp', 1), ('hm', 1), ('og', 1), ('so', 1), ('ax', 1), ('he', 1), ('bt', 1), ('ee', 1), ('cc', 1), ('vf', 1), ('bf', 1), ('uk', 1), ('gm', 1), ('np', 1), ('fr', 1), ('or', 1), ('fn', 1), ('en', 1), ('cm', 1), ('ta', 1), ('ln', 1), ('cv', 1), ('im', 1), ('ns', 1), ('kn', 1), ('ob', 1), ('bu', 1), ('a', 1), ('me', 1), ('r', 1), ('k', 1), ('ry', 1), ('ay', 1), ('vi', 1), ('wa', 1), ('lm', 1), ('am', 1), ('pa', 1), ('of', 1), ('gb', 1), ('sq', 1), ('de', 1), ('my', 1), ('ph', 1), ('eo', 1), ('ii', 1), ('su', 1), ('br', 1), ('dg', 1), ('xy', 1), ('ak', 1), ('oa', 1), ('ko', 1), ('kt', 1), ('gs', 1), ('sp', 1), ('la', 1), ('oh', 1), ('ni', 1), ('xx', 1), ('us', 1), ('co', 1), ('zu', 1), ('no', 1), ('se', 1), ('ac', 1), ('hf', 1), ('ss', 1), ('x', 1), ('lg', 1), ('g', 1), ('j', 1), ('kk', 1), ('c', 1), ('f', 1), ('el', 1), ('op', 1), ('ya', 1), ('wi', 1), ('bn', 1), ('jo', 1), ('fp', 1), ('ce', 1), ('hq', 1), ('om', 1), ('e', 1), ('xv', 1), ('cz', 1), ('ro', 1), ('bv', 1), ('bb', 1), ('lu', 1), ('hl', 1), ('ft', 1), ('pc', 1), ('m', 1), ('pq', 1), ('hy', 1), ('tt', 1), ('bc', 1), ('u', 1), ('ia', 1), ('yu', 1), ('rs', 1), ('by', 1), ('ig', 1), ('gr', 1), ('nx', 1), ('ny', 1), ('cu', 1), ('pg', 1), ('mv', 1), ('to', 1), ('ev', 1), ('nk', 1), ('rt', 1), ('ae', 1), ('po', 1), ('oz', 1), ('ai', 1), ('cg', 1), ('te', 1), ('kv', 1), ('b', 1), ('ik', 1), ('fc', 1), ('km', 1), ('th', 1), ('p', 1), ('cn', 1), ('ba', 1), ('fo', 1), ('ge', 1), ('st', 1), ('oo', 1), ('hu', 1), ('eu', 1), ('ye', 1), ('t', 1), ('gl', 1), ('ep', 1), ('cy', 1), ('ap', 1), ('ou', 1), ('za', 1), ('ab', 1), ('mo', 1), ('ul', 1), ('ir', 1), ('xi', 1), ('rc', 1), ('jp', 1), ('et', 1), ('tb', 1), ('ff', 1), ('xo', 1), ('dr', 1), ('ca', 1), ('bo', 1), ('es', 1), ('as', 1), ('ar', 1), ('ur', 1), ('da', 1), ('sf', 1), ('ja', 1), ('ah', 1), ('gd', 1), ('w', 1), ('vo', 1), ('mr', 1), ('hi', 1), ('yo', 1), ('ag', 1), ('oc', 1), ('oi', 1), ('fa', 1), ('xl', 1), ('pr', 1), ('if', 1), ('di', 1), ('vx', 1), ('lo', 1), ('s', 1), ('bg', 1), ('sv', 1), ('eb', 1), ('gi', 1), ('go', 1), ('od', 1), ('an', 1), ('mg', 1), ('on', 1), ('pv', 1), ('qu', 1), ('ka', 1), ('rn', 1), ('do', 1), ('sn', 1), ('ne', 1), ('ts', 1), ('ki', 1), ('ad', 1), ('pm', 1), ('hp', 1), ('h', 1), ('wu', 1), ('ct', 1), ('ru', 1), ('v', 1), ('dj', 1), ('lb', 1), ('ls', 1), ('un', 1), ('mm', 1), ('ll', 1), ('db', 1), ('ps', 1), ('aj', 1), ('lv', 1), ('fi', 1), ('ea', 1), ('hc', 1), ('jc', 1), ('wb', 1), ('iu', 1), ('nv', 1), ('qc', 1), ('ox', 1), ('ek', 1), ('re', 1), ('sy', 1), ('al', 1), ('mn', 1), ('cd', 1), ('q', 1), ('av', 1), ('le', 1), ('bq', 1), ('bi', 1), ('ui', 1), ('yi', 1), ('ut', 1), ('vu', 1), ('ve', 1), ('ml', 1), ('rd', 1), ('dm', 1), ('mw', 1), ('wo', 1), ('uh', 1), ('cp', 1), ('is', 1), ('sr', 1), ('in', 1), ('gp', 1), ('li', 1), ('au', 1), ('gt', 1), ('td', 1), ('ly', 1), ('fu', 1), ('uv', 1), ('kc', 1), ('mu', 1), ('pp', 1), ('ze', 1), ('ed', 1), ('up', 1), ('mb', 1), ('ix', 1), ('pt', 1), ('ys', 1), ('be', 1), ('dc', 1), ('fm', 1), ('id', 1), ('ib', 1), ('l', 1), ('d', 1), ('oj', 1), ('sa', 1)}\n","Words Top candidates for correction of word  \"ks\"   after Levenshtein distance calculation:  [('ms', 1.0), ('os', 1.0), ('kw', 1.0), ('xs', 1.0), ('ds', 1.0), ('cs', 1.0), ('vs', 1.0), ('kg', 1.0), ('ku', 1.0), ('ns', 1.0), ('kn', 1.0), ('k', 1.0), ('ko', 1.0), ('kt', 1.0), ('gs', 1.0), ('us', 1.0), ('ss', 1.0), ('kk', 1.0), ('rs', 1.0), ('kv', 1.0), ('km', 1.0), ('es', 1.0), ('as', 1.0), ('s', 1.0), ('ka', 1.0), ('ts', 1.0), ('ki', 1.0), ('ls', 1.0), ('ps', 1.0), ('is', 1.0), ('kc', 1.0), ('ys', 1.0)]\n","Words Top candidates for correction of word  \"ks\"   after using \"occurence\" mode:  [('is', 931421), ('as', 397296), ('s', 187761), ('us', 93922), ('es', 2148), ('ms', 1500), ('cs', 1122), ('km', 571), ('k', 235), ('kg', 175), ('ki', 70), ('ss', 27), ('ps', 18), ('ts', 8), ('vs', 7), ('kw', 5), ('kv', 5), ('os', 4), ('gs', 4), ('ka', 3), ('kc', 3), ('ds', 2), ('ns', 2), ('ko', 2), ('xs', 1), ('ku', 1), ('kn', 1), ('kt', 1), ('kk', 1), ('rs', 1), ('ls', 1), ('ys', 1)]\n","Best word for replacement:  is\n","Tokenized and expanded sentence before correction:  ['He', 'ks', 'with', 'ks']\n","Tokenized and expanded sentence after correction:  ['He', 'is', 'with', 'is']\n","Contracted and formatted sentence after correction:  ['He', 'is', 'with', 'is']\n","Untokenized solution sentence:  He is with is\n","\n","## On contextual mode\n","Sentence to expand: \"He ks with ks\"\n","Sentence after expansion: \"He ks with ks\"\n","Sentence before tokenization: \"He ks with ks\"\n","Sentence after tokenization: \" ['He', 'ks', 'with', 'ks'] \"\n","Sentence before tokenization: \"He ks with ks\"\n","Sentence after tokenization: \" ['He', 'ks', 'with', 'ks'] \"\n","Words candidate for correction of word  \"ks\"  using ngram technique:  {('it', 1), ('cl', 1), ('eq', 1), ('ip', 1), ('ng', 1), ('ot', 1), ('dk', 1), ('af', 1), ('mc', 1), ('mp', 1), ('je', 1), ('ms', 1), ('mt', 1), ('du', 1), ('we', 1), ('ma', 1), ('os', 1), ('nm', 1), ('ti', 1), ('tc', 1), ('fg', 1), ('sk', 1), ('mi', 1), ('xu', 1), ('aa', 1), ('er', 1), ('hr', 1), ('cw', 1), ('nf', 1), ('kw', 1), ('dl', 1), ('ju', 1), ('qi', 1), ('iv', 1), ('em', 1), ('vw', 1), ('xs', 1), ('ie', 1), ('gu', 1), ('oq', 1), ('z', 1), ('cf', 1), ('pl', 1), ('iq', 1), ('o', 1), ('ha', 1), ('nl', 1), ('ci', 1), ('vb', 1), ('y', 1), ('si', 1), ('ds', 1), ('cs', 1), ('ji', 1), ('vs', 1), ('na', 1), ('eg', 1), ('ok', 1), ('um', 1), ('i', 1), ('ho', 1), ('zi', 1), ('ga', 1), ('ww', 1), ('at', 1), ('sg', 1), ('pd', 1), ('tu', 1), ('pe', 1), ('zr', 1), ('hz', 1), ('ec', 1), ('pf', 1), ('vp', 1), ('kg', 1), ('cb', 1), ('va', 1), ('il', 1), ('sl', 1), ('ex', 1), ('bp', 1), ('tv', 1), ('nd', 1), ('lt', 1), ('qa', 1), ('ue', 1), ('ku', 1), ('gg', 1), ('n', 1), ('lp', 1), ('hm', 1), ('og', 1), ('so', 1), ('ax', 1), ('he', 1), ('bt', 1), ('ee', 1), ('cc', 1), ('vf', 1), ('bf', 1), ('uk', 1), ('gm', 1), ('np', 1), ('fr', 1), ('or', 1), ('fn', 1), ('en', 1), ('cm', 1), ('ta', 1), ('ln', 1), ('cv', 1), ('im', 1), ('ns', 1), ('kn', 1), ('ob', 1), ('bu', 1), ('a', 1), ('me', 1), ('r', 1), ('k', 1), ('ry', 1), ('ay', 1), ('vi', 1), ('wa', 1), ('lm', 1), ('am', 1), ('pa', 1), ('of', 1), ('gb', 1), ('sq', 1), ('de', 1), ('my', 1), ('ph', 1), ('eo', 1), ('ii', 1), ('su', 1), ('br', 1), ('dg', 1), ('xy', 1), ('ak', 1), ('oa', 1), ('ko', 1), ('kt', 1), ('gs', 1), ('sp', 1), ('la', 1), ('oh', 1), ('ni', 1), ('xx', 1), ('us', 1), ('co', 1), ('zu', 1), ('no', 1), ('se', 1), ('ac', 1), ('hf', 1), ('ss', 1), ('x', 1), ('lg', 1), ('g', 1), ('j', 1), ('kk', 1), ('c', 1), ('f', 1), ('el', 1), ('op', 1), ('ya', 1), ('wi', 1), ('bn', 1), ('jo', 1), ('fp', 1), ('ce', 1), ('hq', 1), ('om', 1), ('e', 1), ('xv', 1), ('cz', 1), ('ro', 1), ('bv', 1), ('bb', 1), ('lu', 1), ('hl', 1), ('ft', 1), ('pc', 1), ('m', 1), ('pq', 1), ('hy', 1), ('tt', 1), ('bc', 1), ('u', 1), ('ia', 1), ('yu', 1), ('rs', 1), ('by', 1), ('ig', 1), ('gr', 1), ('nx', 1), ('ny', 1), ('cu', 1), ('pg', 1), ('mv', 1), ('to', 1), ('ev', 1), ('nk', 1), ('rt', 1), ('ae', 1), ('po', 1), ('oz', 1), ('ai', 1), ('cg', 1), ('te', 1), ('kv', 1), ('b', 1), ('ik', 1), ('fc', 1), ('km', 1), ('th', 1), ('p', 1), ('cn', 1), ('ba', 1), ('fo', 1), ('ge', 1), ('st', 1), ('oo', 1), ('hu', 1), ('eu', 1), ('ye', 1), ('t', 1), ('gl', 1), ('ep', 1), ('cy', 1), ('ap', 1), ('ou', 1), ('za', 1), ('ab', 1), ('mo', 1), ('ul', 1), ('ir', 1), ('xi', 1), ('rc', 1), ('jp', 1), ('et', 1), ('tb', 1), ('ff', 1), ('xo', 1), ('dr', 1), ('ca', 1), ('bo', 1), ('es', 1), ('as', 1), ('ar', 1), ('ur', 1), ('da', 1), ('sf', 1), ('ja', 1), ('ah', 1), ('gd', 1), ('w', 1), ('vo', 1), ('mr', 1), ('hi', 1), ('yo', 1), ('ag', 1), ('oc', 1), ('oi', 1), ('fa', 1), ('xl', 1), ('pr', 1), ('if', 1), ('di', 1), ('vx', 1), ('lo', 1), ('s', 1), ('bg', 1), ('sv', 1), ('eb', 1), ('gi', 1), ('go', 1), ('od', 1), ('an', 1), ('mg', 1), ('on', 1), ('pv', 1), ('qu', 1), ('ka', 1), ('rn', 1), ('do', 1), ('sn', 1), ('ne', 1), ('ts', 1), ('ki', 1), ('ad', 1), ('pm', 1), ('hp', 1), ('h', 1), ('wu', 1), ('ct', 1), ('ru', 1), ('v', 1), ('dj', 1), ('lb', 1), ('ls', 1), ('un', 1), ('mm', 1), ('ll', 1), ('db', 1), ('ps', 1), ('aj', 1), ('lv', 1), ('fi', 1), ('ea', 1), ('hc', 1), ('jc', 1), ('wb', 1), ('iu', 1), ('nv', 1), ('qc', 1), ('ox', 1), ('ek', 1), ('re', 1), ('sy', 1), ('al', 1), ('mn', 1), ('cd', 1), ('q', 1), ('av', 1), ('le', 1), ('bq', 1), ('bi', 1), ('ui', 1), ('yi', 1), ('ut', 1), ('vu', 1), ('ve', 1), ('ml', 1), ('rd', 1), ('dm', 1), ('mw', 1), ('wo', 1), ('uh', 1), ('cp', 1), ('is', 1), ('sr', 1), ('in', 1), ('gp', 1), ('li', 1), ('au', 1), ('gt', 1), ('td', 1), ('ly', 1), ('fu', 1), ('uv', 1), ('kc', 1), ('mu', 1), ('pp', 1), ('ze', 1), ('ed', 1), ('up', 1), ('mb', 1), ('ix', 1), ('pt', 1), ('ys', 1), ('be', 1), ('dc', 1), ('fm', 1), ('id', 1), ('ib', 1), ('l', 1), ('d', 1), ('oj', 1), ('sa', 1)}\n","Words Top candidates for correction of word  \"ks\"   after Levenshtein distance calculation:  [('ms', 1.0), ('os', 1.0), ('kw', 1.0), ('xs', 1.0), ('ds', 1.0), ('cs', 1.0), ('vs', 1.0), ('kg', 1.0), ('ku', 1.0), ('ns', 1.0), ('kn', 1.0), ('k', 1.0), ('ko', 1.0), ('kt', 1.0), ('gs', 1.0), ('us', 1.0), ('ss', 1.0), ('kk', 1.0), ('rs', 1.0), ('kv', 1.0), ('km', 1.0), ('es', 1.0), ('as', 1.0), ('s', 1.0), ('ka', 1.0), ('ts', 1.0), ('ki', 1.0), ('ls', 1.0), ('ps', 1.0), ('is', 1.0), ('kc', 1.0), ('ys', 1.0)]\n","Words Top candidates for correction of word  \"ks\"   after using \"context\" mode:  [('is', 5133), ('us', 2004), ('as', 823), ('s', 21), ('km', 6), ('ms', 1), ('kg', 1), ('os', 0), ('kw', 0), ('xs', 0), ('ds', 0), ('cs', 0), ('vs', 0), ('ku', 0), ('ns', 0), ('kn', 0), ('k', 0), ('ko', 0), ('kt', 0), ('gs', 0), ('ss', 0), ('kk', 0), ('rs', 0), ('kv', 0), ('es', 0), ('ka', 0), ('ts', 0), ('ki', 0), ('ls', 0), ('ps', 0), ('kc', 0), ('ys', 0)]\n","Best word for replacement:  is\n","Words candidate for correction of word  \"ks\"  using ngram technique:  {('it', 1), ('cl', 1), ('eq', 1), ('ip', 1), ('ng', 1), ('ot', 1), ('dk', 1), ('af', 1), ('mc', 1), ('mp', 1), ('je', 1), ('ms', 1), ('mt', 1), ('du', 1), ('we', 1), ('ma', 1), ('os', 1), ('nm', 1), ('ti', 1), ('tc', 1), ('fg', 1), ('sk', 1), ('mi', 1), ('xu', 1), ('aa', 1), ('er', 1), ('hr', 1), ('cw', 1), ('nf', 1), ('kw', 1), ('dl', 1), ('ju', 1), ('qi', 1), ('iv', 1), ('em', 1), ('vw', 1), ('xs', 1), ('ie', 1), ('gu', 1), ('oq', 1), ('z', 1), ('cf', 1), ('pl', 1), ('iq', 1), ('o', 1), ('ha', 1), ('nl', 1), ('ci', 1), ('vb', 1), ('y', 1), ('si', 1), ('ds', 1), ('cs', 1), ('ji', 1), ('vs', 1), ('na', 1), ('eg', 1), ('ok', 1), ('um', 1), ('i', 1), ('ho', 1), ('zi', 1), ('ga', 1), ('ww', 1), ('at', 1), ('sg', 1), ('pd', 1), ('tu', 1), ('pe', 1), ('zr', 1), ('hz', 1), ('ec', 1), ('pf', 1), ('vp', 1), ('kg', 1), ('cb', 1), ('va', 1), ('il', 1), ('sl', 1), ('ex', 1), ('bp', 1), ('tv', 1), ('nd', 1), ('lt', 1), ('qa', 1), ('ue', 1), ('ku', 1), ('gg', 1), ('n', 1), ('lp', 1), ('hm', 1), ('og', 1), ('so', 1), ('ax', 1), ('he', 1), ('bt', 1), ('ee', 1), ('cc', 1), ('vf', 1), ('bf', 1), ('uk', 1), ('gm', 1), ('np', 1), ('fr', 1), ('or', 1), ('fn', 1), ('en', 1), ('cm', 1), ('ta', 1), ('ln', 1), ('cv', 1), ('im', 1), ('ns', 1), ('kn', 1), ('ob', 1), ('bu', 1), ('a', 1), ('me', 1), ('r', 1), ('k', 1), ('ry', 1), ('ay', 1), ('vi', 1), ('wa', 1), ('lm', 1), ('am', 1), ('pa', 1), ('of', 1), ('gb', 1), ('sq', 1), ('de', 1), ('my', 1), ('ph', 1), ('eo', 1), ('ii', 1), ('su', 1), ('br', 1), ('dg', 1), ('xy', 1), ('ak', 1), ('oa', 1), ('ko', 1), ('kt', 1), ('gs', 1), ('sp', 1), ('la', 1), ('oh', 1), ('ni', 1), ('xx', 1), ('us', 1), ('co', 1), ('zu', 1), ('no', 1), ('se', 1), ('ac', 1), ('hf', 1), ('ss', 1), ('x', 1), ('lg', 1), ('g', 1), ('j', 1), ('kk', 1), ('c', 1), ('f', 1), ('el', 1), ('op', 1), ('ya', 1), ('wi', 1), ('bn', 1), ('jo', 1), ('fp', 1), ('ce', 1), ('hq', 1), ('om', 1), ('e', 1), ('xv', 1), ('cz', 1), ('ro', 1), ('bv', 1), ('bb', 1), ('lu', 1), ('hl', 1), ('ft', 1), ('pc', 1), ('m', 1), ('pq', 1), ('hy', 1), ('tt', 1), ('bc', 1), ('u', 1), ('ia', 1), ('yu', 1), ('rs', 1), ('by', 1), ('ig', 1), ('gr', 1), ('nx', 1), ('ny', 1), ('cu', 1), ('pg', 1), ('mv', 1), ('to', 1), ('ev', 1), ('nk', 1), ('rt', 1), ('ae', 1), ('po', 1), ('oz', 1), ('ai', 1), ('cg', 1), ('te', 1), ('kv', 1), ('b', 1), ('ik', 1), ('fc', 1), ('km', 1), ('th', 1), ('p', 1), ('cn', 1), ('ba', 1), ('fo', 1), ('ge', 1), ('st', 1), ('oo', 1), ('hu', 1), ('eu', 1), ('ye', 1), ('t', 1), ('gl', 1), ('ep', 1), ('cy', 1), ('ap', 1), ('ou', 1), ('za', 1), ('ab', 1), ('mo', 1), ('ul', 1), ('ir', 1), ('xi', 1), ('rc', 1), ('jp', 1), ('et', 1), ('tb', 1), ('ff', 1), ('xo', 1), ('dr', 1), ('ca', 1), ('bo', 1), ('es', 1), ('as', 1), ('ar', 1), ('ur', 1), ('da', 1), ('sf', 1), ('ja', 1), ('ah', 1), ('gd', 1), ('w', 1), ('vo', 1), ('mr', 1), ('hi', 1), ('yo', 1), ('ag', 1), ('oc', 1), ('oi', 1), ('fa', 1), ('xl', 1), ('pr', 1), ('if', 1), ('di', 1), ('vx', 1), ('lo', 1), ('s', 1), ('bg', 1), ('sv', 1), ('eb', 1), ('gi', 1), ('go', 1), ('od', 1), ('an', 1), ('mg', 1), ('on', 1), ('pv', 1), ('qu', 1), ('ka', 1), ('rn', 1), ('do', 1), ('sn', 1), ('ne', 1), ('ts', 1), ('ki', 1), ('ad', 1), ('pm', 1), ('hp', 1), ('h', 1), ('wu', 1), ('ct', 1), ('ru', 1), ('v', 1), ('dj', 1), ('lb', 1), ('ls', 1), ('un', 1), ('mm', 1), ('ll', 1), ('db', 1), ('ps', 1), ('aj', 1), ('lv', 1), ('fi', 1), ('ea', 1), ('hc', 1), ('jc', 1), ('wb', 1), ('iu', 1), ('nv', 1), ('qc', 1), ('ox', 1), ('ek', 1), ('re', 1), ('sy', 1), ('al', 1), ('mn', 1), ('cd', 1), ('q', 1), ('av', 1), ('le', 1), ('bq', 1), ('bi', 1), ('ui', 1), ('yi', 1), ('ut', 1), ('vu', 1), ('ve', 1), ('ml', 1), ('rd', 1), ('dm', 1), ('mw', 1), ('wo', 1), ('uh', 1), ('cp', 1), ('is', 1), ('sr', 1), ('in', 1), ('gp', 1), ('li', 1), ('au', 1), ('gt', 1), ('td', 1), ('ly', 1), ('fu', 1), ('uv', 1), ('kc', 1), ('mu', 1), ('pp', 1), ('ze', 1), ('ed', 1), ('up', 1), ('mb', 1), ('ix', 1), ('pt', 1), ('ys', 1), ('be', 1), ('dc', 1), ('fm', 1), ('id', 1), ('ib', 1), ('l', 1), ('d', 1), ('oj', 1), ('sa', 1)}\n","Words Top candidates for correction of word  \"ks\"   after Levenshtein distance calculation:  [('ms', 1.0), ('os', 1.0), ('kw', 1.0), ('xs', 1.0), ('ds', 1.0), ('cs', 1.0), ('vs', 1.0), ('kg', 1.0), ('ku', 1.0), ('ns', 1.0), ('kn', 1.0), ('k', 1.0), ('ko', 1.0), ('kt', 1.0), ('gs', 1.0), ('us', 1.0), ('ss', 1.0), ('kk', 1.0), ('rs', 1.0), ('kv', 1.0), ('km', 1.0), ('es', 1.0), ('as', 1.0), ('s', 1.0), ('ka', 1.0), ('ts', 1.0), ('ki', 1.0), ('ls', 1.0), ('ps', 1.0), ('is', 1.0), ('kc', 1.0), ('ys', 1.0)]\n","Words Top candidates for correction of word  \"ks\"   after using \"context\" mode:  [('us', 10012), ('is', 2686), ('es', 2113), ('cs', 1107), ('as', 728), ('s', 545), ('k', 164), ('ms', 103), ('km', 74), ('kg', 30), ('ps', 6), ('vs', 5), ('ss', 4), ('ts', 3), ('kv', 2), ('kw', 1), ('gs', 1), ('os', 0), ('xs', 0), ('ds', 0), ('ku', 0), ('ns', 0), ('kn', 0), ('ko', 0), ('kt', 0), ('kk', 0), ('rs', 0), ('ka', 0), ('ki', 0), ('ls', 0), ('kc', 0), ('ys', 0)]\n","Best word for replacement:  us\n","Tokenized and expanded sentence before correction:  ['He', 'ks', 'with', 'ks']\n","Tokenized and expanded sentence after correction:  ['He', 'is', 'with', 'us']\n","Contracted and formatted sentence after correction:  ['He', 'is', 'with', 'us']\n","Untokenized solution sentence:  He is with us\n","\n","## Final Result\n","Original sentence 2:                      He ks with ks\n","Result of sentence 2 in occurence mode:   He is with is\n","Result of sentence 2 in contextual mode:  He is with us\n","\n","\n","###### TEST ON 3rd SENTENCE: \"I wafted outside for hours and I have wafted my time\"\n","## On occurence mode\n","Sentence to expand: \"I wafted outside for hours and I have wafted my time\"\n","Sentence after expansion: \"I wafted outside for hours and I have wafted my time\"\n","Sentence before tokenization: \"I wafted outside for hours and I have wafted my time\"\n","Sentence after tokenization: \" ['I', 'wafted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wafted', 'my', 'time'] \"\n","Sentence before tokenization: \"I wafted outside for hours and I have wafted my time\"\n","Sentence after tokenization: \" ['I', 'wafted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wafted', 'my', 'time'] \"\n","Words candidate for correction of word  \"wafted\"  using ngram technique:  [('dafter', 3), ('sifted', 3), ('wanted', 3), ('waited', 3), ('lifted', 3), ('gifted', 3), ('wasted', 3), ('drafts', 2), ('leafed', 2), ('waffly', 2), ('grafts', 2), ('crafts', 2), ('loafed', 2), ('shafts', 2), ('safwan', 2), ('crafty', 2), ('graaft', 2), ('waffle', 2), ('waffen', 2), ('wafers', 2), ('potted', 2), ('hosted', 2), ('empted', 2), ('warped', 2), ('grated', 2), ('warmed', 2), ('ratted', 2), ('crated', 2), ('coated', 2), ('skated', 2), ('listed', 2), ('tended', 2), ('dented', 2), ('witted', 2), ('fitted', 2), ('warded', 2), ('rooted', 2), ('suited', 2), ('warned', 2), ('gusted', 2), ('tedlar', 2), ('tinted', 2), ('ranted', 2), ('hooted', 2), ('hinted', 2), ('jolted', 2), ('wailed', 2), ('bedste', 2), ('lasted', 2), ('vested', 2), ('hunted', 2), ('vetted', 2), ('footed', 2), ('slated', 2), ('edward', 2), ('wonted', 2), ('silted', 2), ('tedium', 2), ('teamed', 2), ('mooted', 2), ('minted', 2), ('melted', 2), ('exited', 2), ('washed', 2), ('sorted', 2), ('bolted', 2), ('salted', 2), ('costed', 2), ('carted', 2), ('edited', 2), ('wagged', 2), ('whited', 2), ('hatted', 2), ('dusted', 2), ('pitted', 2), ('rented', 2), ('hotted', 2), ('fasted', 2), ('panted', 2), ('stated', 2), ('elated', 2), ('touted', 2), ('fatted', 2), ('plated', 2), ('heated', 2), ('batted', 2), ('ousted', 2), ('rioted', 2), ('termed', 2), ('fisted', 2), ('halted', 2), ('waived', 2), ('looted', 2), ('parted', 2), ('pasted', 2), ('posted', 2), ('gutted', 2), ('quoted', 2), ('kitted', 2), ('patted', 2), ('pelted', 2), ('rested', 2), ('dotted', 2), ('bested', 2), ('teased', 2), ('tilted', 2), ('tented', 2), ('walled', 2), ('united', 2), ('abated', 2), ('tasted', 2), ('seated', 2), ('rusted', 2), ('nested', 2), ('swayed', 2), ('netted', 2), ('tested', 2), ('walked', 2), ('sedate', 2), ('routed', 2), ('vented', 2), ('soften', 2), ('softer', 2), ('watery', 2), ('waiter', 2), ('waters', 2), ('wastes', 2), ('walter', 2)]\n","Words Top candidates for correction of word  \"wafted\"   after Levenshtein distance calculation:  [('wanted', 1.0), ('waited', 1.0), ('wasted', 1.0)]\n","Words Top candidates for correction of word  \"wafted\"   after using \"occurence\" mode:  [('wanted', 6025), ('wasted', 648), ('waited', 277)]\n","Best word for replacement:  wanted\n","Words candidate for correction of word  \"wafted\"  using ngram technique:  [('dafter', 3), ('sifted', 3), ('wanted', 3), ('waited', 3), ('lifted', 3), ('gifted', 3), ('wasted', 3), ('drafts', 2), ('leafed', 2), ('waffly', 2), ('grafts', 2), ('crafts', 2), ('loafed', 2), ('shafts', 2), ('safwan', 2), ('crafty', 2), ('graaft', 2), ('waffle', 2), ('waffen', 2), ('wafers', 2), ('potted', 2), ('hosted', 2), ('empted', 2), ('warped', 2), ('grated', 2), ('warmed', 2), ('ratted', 2), ('crated', 2), ('coated', 2), ('skated', 2), ('listed', 2), ('tended', 2), ('dented', 2), ('witted', 2), ('fitted', 2), ('warded', 2), ('rooted', 2), ('suited', 2), ('warned', 2), ('gusted', 2), ('tedlar', 2), ('tinted', 2), ('ranted', 2), ('hooted', 2), ('hinted', 2), ('jolted', 2), ('wailed', 2), ('bedste', 2), ('lasted', 2), ('vested', 2), ('hunted', 2), ('vetted', 2), ('footed', 2), ('slated', 2), ('edward', 2), ('wonted', 2), ('silted', 2), ('tedium', 2), ('teamed', 2), ('mooted', 2), ('minted', 2), ('melted', 2), ('exited', 2), ('washed', 2), ('sorted', 2), ('bolted', 2), ('salted', 2), ('costed', 2), ('carted', 2), ('edited', 2), ('wagged', 2), ('whited', 2), ('hatted', 2), ('dusted', 2), ('pitted', 2), ('rented', 2), ('hotted', 2), ('fasted', 2), ('panted', 2), ('stated', 2), ('elated', 2), ('touted', 2), ('fatted', 2), ('plated', 2), ('heated', 2), ('batted', 2), ('ousted', 2), ('rioted', 2), ('termed', 2), ('fisted', 2), ('halted', 2), ('waived', 2), ('looted', 2), ('parted', 2), ('pasted', 2), ('posted', 2), ('gutted', 2), ('quoted', 2), ('kitted', 2), ('patted', 2), ('pelted', 2), ('rested', 2), ('dotted', 2), ('bested', 2), ('teased', 2), ('tilted', 2), ('tented', 2), ('walled', 2), ('united', 2), ('abated', 2), ('tasted', 2), ('seated', 2), ('rusted', 2), ('nested', 2), ('swayed', 2), ('netted', 2), ('tested', 2), ('walked', 2), ('sedate', 2), ('routed', 2), ('vented', 2), ('soften', 2), ('softer', 2), ('watery', 2), ('waiter', 2), ('waters', 2), ('wastes', 2), ('walter', 2)]\n","Words Top candidates for correction of word  \"wafted\"   after Levenshtein distance calculation:  [('wanted', 1.0), ('waited', 1.0), ('wasted', 1.0)]\n","Words Top candidates for correction of word  \"wafted\"   after using \"occurence\" mode:  [('wanted', 6025), ('wasted', 648), ('waited', 277)]\n","Best word for replacement:  wanted\n","Tokenized and expanded sentence before correction:  ['I', 'wafted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wafted', 'my', 'time']\n","Tokenized and expanded sentence after correction:  ['I', 'wanted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wanted', 'my', 'time']\n","Contracted and formatted sentence after correction:  ['I', 'wanted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wanted', 'my', 'time']\n","Untokenized solution sentence:  I wanted outside for hours and I have wanted my time\n","\n","## On contextual mode\n","Sentence to expand: \"I wafted outside for hours and I have wafted my time\"\n","Sentence after expansion: \"I wafted outside for hours and I have wafted my time\"\n","Sentence before tokenization: \"I wafted outside for hours and I have wafted my time\"\n","Sentence after tokenization: \" ['I', 'wafted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wafted', 'my', 'time'] \"\n","Sentence before tokenization: \"I wafted outside for hours and I have wafted my time\"\n","Sentence after tokenization: \" ['I', 'wafted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wafted', 'my', 'time'] \"\n","Words candidate for correction of word  \"wafted\"  using ngram technique:  [('dafter', 3), ('sifted', 3), ('wanted', 3), ('waited', 3), ('lifted', 3), ('gifted', 3), ('wasted', 3), ('drafts', 2), ('leafed', 2), ('waffly', 2), ('grafts', 2), ('crafts', 2), ('loafed', 2), ('shafts', 2), ('safwan', 2), ('crafty', 2), ('graaft', 2), ('waffle', 2), ('waffen', 2), ('wafers', 2), ('potted', 2), ('hosted', 2), ('empted', 2), ('warped', 2), ('grated', 2), ('warmed', 2), ('ratted', 2), ('crated', 2), ('coated', 2), ('skated', 2), ('listed', 2), ('tended', 2), ('dented', 2), ('witted', 2), ('fitted', 2), ('warded', 2), ('rooted', 2), ('suited', 2), ('warned', 2), ('gusted', 2), ('tedlar', 2), ('tinted', 2), ('ranted', 2), ('hooted', 2), ('hinted', 2), ('jolted', 2), ('wailed', 2), ('bedste', 2), ('lasted', 2), ('vested', 2), ('hunted', 2), ('vetted', 2), ('footed', 2), ('slated', 2), ('edward', 2), ('wonted', 2), ('silted', 2), ('tedium', 2), ('teamed', 2), ('mooted', 2), ('minted', 2), ('melted', 2), ('exited', 2), ('washed', 2), ('sorted', 2), ('bolted', 2), ('salted', 2), ('costed', 2), ('carted', 2), ('edited', 2), ('wagged', 2), ('whited', 2), ('hatted', 2), ('dusted', 2), ('pitted', 2), ('rented', 2), ('hotted', 2), ('fasted', 2), ('panted', 2), ('stated', 2), ('elated', 2), ('touted', 2), ('fatted', 2), ('plated', 2), ('heated', 2), ('batted', 2), ('ousted', 2), ('rioted', 2), ('termed', 2), ('fisted', 2), ('halted', 2), ('waived', 2), ('looted', 2), ('parted', 2), ('pasted', 2), ('posted', 2), ('gutted', 2), ('quoted', 2), ('kitted', 2), ('patted', 2), ('pelted', 2), ('rested', 2), ('dotted', 2), ('bested', 2), ('teased', 2), ('tilted', 2), ('tented', 2), ('walled', 2), ('united', 2), ('abated', 2), ('tasted', 2), ('seated', 2), ('rusted', 2), ('nested', 2), ('swayed', 2), ('netted', 2), ('tested', 2), ('walked', 2), ('sedate', 2), ('routed', 2), ('vented', 2), ('soften', 2), ('softer', 2), ('watery', 2), ('waiter', 2), ('waters', 2), ('wastes', 2), ('walter', 2)]\n","Words Top candidates for correction of word  \"wafted\"   after Levenshtein distance calculation:  [('wanted', 1.0), ('waited', 1.0), ('wasted', 1.0)]\n","Words Top candidates for correction of word  \"wafted\"   after using \"context\" mode:  [('wanted', 1552), ('waited', 14), ('wasted', 0)]\n","Best word for replacement:  wanted\n","Words candidate for correction of word  \"wafted\"  using ngram technique:  [('dafter', 3), ('sifted', 3), ('wanted', 3), ('waited', 3), ('lifted', 3), ('gifted', 3), ('wasted', 3), ('drafts', 2), ('leafed', 2), ('waffly', 2), ('grafts', 2), ('crafts', 2), ('loafed', 2), ('shafts', 2), ('safwan', 2), ('crafty', 2), ('graaft', 2), ('waffle', 2), ('waffen', 2), ('wafers', 2), ('potted', 2), ('hosted', 2), ('empted', 2), ('warped', 2), ('grated', 2), ('warmed', 2), ('ratted', 2), ('crated', 2), ('coated', 2), ('skated', 2), ('listed', 2), ('tended', 2), ('dented', 2), ('witted', 2), ('fitted', 2), ('warded', 2), ('rooted', 2), ('suited', 2), ('warned', 2), ('gusted', 2), ('tedlar', 2), ('tinted', 2), ('ranted', 2), ('hooted', 2), ('hinted', 2), ('jolted', 2), ('wailed', 2), ('bedste', 2), ('lasted', 2), ('vested', 2), ('hunted', 2), ('vetted', 2), ('footed', 2), ('slated', 2), ('edward', 2), ('wonted', 2), ('silted', 2), ('tedium', 2), ('teamed', 2), ('mooted', 2), ('minted', 2), ('melted', 2), ('exited', 2), ('washed', 2), ('sorted', 2), ('bolted', 2), ('salted', 2), ('costed', 2), ('carted', 2), ('edited', 2), ('wagged', 2), ('whited', 2), ('hatted', 2), ('dusted', 2), ('pitted', 2), ('rented', 2), ('hotted', 2), ('fasted', 2), ('panted', 2), ('stated', 2), ('elated', 2), ('touted', 2), ('fatted', 2), ('plated', 2), ('heated', 2), ('batted', 2), ('ousted', 2), ('rioted', 2), ('termed', 2), ('fisted', 2), ('halted', 2), ('waived', 2), ('looted', 2), ('parted', 2), ('pasted', 2), ('posted', 2), ('gutted', 2), ('quoted', 2), ('kitted', 2), ('patted', 2), ('pelted', 2), ('rested', 2), ('dotted', 2), ('bested', 2), ('teased', 2), ('tilted', 2), ('tented', 2), ('walled', 2), ('united', 2), ('abated', 2), ('tasted', 2), ('seated', 2), ('rusted', 2), ('nested', 2), ('swayed', 2), ('netted', 2), ('tested', 2), ('walked', 2), ('sedate', 2), ('routed', 2), ('vented', 2), ('soften', 2), ('softer', 2), ('watery', 2), ('waiter', 2), ('waters', 2), ('wastes', 2), ('walter', 2)]\n","Words Top candidates for correction of word  \"wafted\"   after Levenshtein distance calculation:  [('wanted', 1.0), ('waited', 1.0), ('wasted', 1.0)]\n","Words Top candidates for correction of word  \"wafted\"   after using \"context\" mode:  [('wanted', 125), ('waited', 101), ('wasted', 53)]\n","Best word for replacement:  wanted\n","Tokenized and expanded sentence before correction:  ['I', 'wafted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wafted', 'my', 'time']\n","Tokenized and expanded sentence after correction:  ['I', 'wanted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wanted', 'my', 'time']\n","Contracted and formatted sentence after correction:  ['I', 'wanted', 'outside', 'for', 'hours', 'and', 'I', 'have', 'wanted', 'my', 'time']\n","Untokenized solution sentence:  I wanted outside for hours and I have wanted my time\n","\n","## Final Result\n","Original sentence 3:                      I wafted outside for hours and I have wafted my time\n","Result of sentence 3 in occurence mode:   I wanted outside for hours and I have wanted my time\n","Result of sentence 3 in contextual mode:  I wanted outside for hours and I have wanted my time\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_-1T-gzex3Lw","colab_type":"text"},"source":["### Explanation of the result\n","\n","The Test of our ECM (Error Correction Module) gives interesting result:\n","\n","1. For the 1st sentence, \"*He ta**b**es his time to pay his ta**b**es*\" , we get: \n","   * In \"*occurence*\" mode : \"*He ta**k**es his time to pay his ta**k**es*\"\n","   * In \"*contextual*\" mode: \"*He ta**k**es his time to pay his ta**x**es*\".\n","   \n","   As expected, the \"*contextual*\" mode gives a better result than the \"*occurence*\" mode. Indeed, in the \"*occurence*\" mode, we simply take the word which has the highest occurence in the corpus text to solve the Levenshtein distance tie between the words \"*takes*\" and \"*taxes*\" as replacement of \"*tabes*\". Despite of being faster than the \"*contextual*\" mode, this strategy has its limit since it does not see the context of the word in the sentence and apply the replacement without \"knowing\" with the requested words needs to be a \"noun\" or a \"verb\".\n","   \n","   The \"*contextual*\" mode removes this limitation; but as a drawback, the process is a little bit longer, since it has to go throuw all the words list in the dictionary dict_corpus['txt'] to determine the context of each candidate before and after.\n","   \n","1. For the 2nd sentence, \"*He **k**s with **k**s*\" , we get: \n","   * In \"*occurence*\" mode : \"*He **i**s with **i**s*\"\n","   * In \"*contextual*\" mode: \"*He **i**s with **u**s*\".\n","   \n","   Again, the \"*contextual*\" mode gives a better result than the \"*occurence*\" mode for the same reason as above.\n","   \n","   But actually, the interesting part here is whatever the context (\"*occurence*\" and \"*contextual*\"), the ECM is able to catch small words mispelling mistake (words with less than 3 characters). Indeed, as we are using bigram tokens to check the similarity between the candidate words and the mispelled words. But in our \"*words_proposition*\" function, we implement special case for short words mispelling and we are not using anymore the bigram tokens but the dict_corpus['shortw'] list. This is the reason why, our ECM is able to handle short words mispelling.\n","   \n","1. For the 3rd sentence, \"*I wa**f**ted outside for hours and I wa**f**ted my time*\", we get: \n","   * In \"*occurence*\" mode : \"*I wa**n**ted outside for hours and I wa**n**ted my time*\"\n","   * In \"*contextual*\" mode: \"*I wa**n**ted outside for hours and I wa**n**ted my time*\".\n","   \n","   Here, we can see the limitation of our ECM model. Indeed, the expected sentence should have been \"*I wa**i**ted outside for hours and I wa**s**ted my time*\"\n","   \n","\n","#### Limitation Explanation\n","   \n","   This limitation is visible even in our \"*contextual*\" mode. The reason is that in this sentence the 2 expected correct words (\"waited\" and \"wasted\") and the proposed word (\"wanted\") are very similar in their context: They are all 3 verbs with only one Levenshtein distance between them. Their usage is approximatively identical as the 3 verbs are used following a noun and they are transitive verbs which means they are also automatically followed by noun. Therefore determining the context is not that simple.\n","   \n","   In order, to remove thsi limitation, we should have taken a wider context. Instead of using bigram context (word before and after targeted word), we should use trigram or 4-gram context (respectively 2 words before and after; and 3 words before and after) to widen the context. A larger corpus could also be a little bit helpful. However, in that case, the ECM process would have been extremely heavy and very long to produce a result.\n","   \n"]},{"cell_type":"markdown","metadata":{"id":"tiy772UHWI6h","colab_type":"text"},"source":["# Error Correction Module applied after the OCR\n","\n","In this section, we are testing the Error Correction Module. For that we use the output of the OCR system provided by the lecturer and then try to correct the output word.\n","\n","Most of the part below has not been implemented by the student. The complete OCR module has been reused from the lecturer program. Only the Main section of the program has been modified a litle bit to support the **Error Correction Module** after collecting the output of OCR module."]},{"cell_type":"code","metadata":{"id":"4Jye2sf3smcL","colab_type":"code","outputId":"c2060070-b51c-4ce6-cb9f-f87488b48390","executionInfo":{"status":"ok","timestamp":1572181168636,"user_tz":0,"elapsed":573282,"user":{"displayName":"Nirmal Pregassame","photoUrl":"","userId":"11048350488911722984"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import random\n","import sys\n","import numpy as np\n","import cv2\n","import editdistance\n","import tensorflow as tf\n","\n","\n","\n","#ExpDir = '/Users/haithem.afli/Desktop/CIT/Research/IndustryProjects/Unitek/NN_OCR/run_model/'\n","#ExpDir = r'C:\\Users\\npregass\\OneDrive - Qualcomm\\Documents\\Masters AI\\NLP\\Assignement 1\\Project1NLP_Model\\Project1NLP_Model'\n","ExpDir = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/'\n","#print ('Experiment Dir is:' + ExpDir)\n","#modelDir = r'C:\\Users\\npregass\\OneDrive - Qualcomm\\Documents\\Masters AI\\NLP\\Assignement 1\\Project1NLP_Model\\Project1NLP_Model/model/'\n","modelDir = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/model/'\n","#modelDir = '/Users/haithem.afli/Desktop/CIT/Research/IndustryProjects/Unitek/NN_OCR/run_model/model/'\n","#print ('Model is in:' + modelDir)\n","#You need to change this path with your local path to the saved model I provides with the code\n","\n","class FilePaths:\n","  \"filenames and paths to data\"\n","  fnCharList = modelDir+'charList.txt'\n","  fnAccuracy = modelDir+'accuracy.txt'\n","  fnInfer = ExpDir+'unitek12.png'\n","\n","    \n","\n","class Batch:\n","  \"batch containing images and ground truth texts\"\n","  def __init__(self, gtTexts, imgs):\n","    self.imgs = np.stack(imgs, axis=0)\n","    self.gtTexts = gtTexts\n","\n","class DecoderType:\n","  BestPath = 0\n","  BeamSearch = 1\n","  WordBeamSearch = 2\n","        \n","def preprocess(img, imgSize, dataAugmentation=False):\n","  \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n","\n","  # there are damaged files in IAM dataset - just use black image instead\n","  if img is None:\n","    img = np.zeros([imgSize[1], imgSize[0]])\n","\n","  # increase dataset size by applying random stretches to the images\n","  if dataAugmentation:\n","    stretch = (random.random() - 0.5) # -0.5 .. +0.5\n","    wStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n","    img = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n","  \n","  # create target image and copy sample image into it\n","  (wt, ht) = imgSize\n","  (h, w) = img.shape\n","  fx = w / wt\n","  fy = h / ht\n","  f = max(fx, fy)\n","  newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n","  img = cv2.resize(img, newSize)\n","  target = np.ones([ht, wt]) * 255\n","  target[0:newSize[1], 0:newSize[0]] = img\n","\n","  # transpose for TF\n","  img = cv2.transpose(target)\n","\n","  # normalize\n","  (m, s) = cv2.meanStdDev(img)\n","  m = m[0][0]\n","  s = s[0][0]\n","  img = img - m\n","  img = img / s if s>0 else img\n","  return img\n","\n","def train(model, loader):\n","  \"train NN\"\n","  epoch = 0 # number of training epochs since start\n","  bestCharErrorRate = float('inf') # best valdiation character error rate\n","  noImprovementSince = 0 # number of epochs no improvement of character error rate occured\n","  earlyStopping = 5 # stop training after this number of epochs without improvement\n","  while True:\n","    epoch += 1\n","    print('Epoch:', epoch)\n","\n","    # train\n","    print('Train NN')\n","    loader.trainSet()\n","    while loader.hasNext():\n","      iterInfo = loader.getIteratorInfo()\n","      batch = loader.getNext()\n","      loss = model.trainBatch(batch)\n","      print('Batch:', iterInfo[0],'/', iterInfo[1], 'Loss:', loss)\n","\n","    # validate\n","    charErrorRate = validate(model, loader)\n","    \n","    # if best validation accuracy so far, save model parameters\n","    if charErrorRate < bestCharErrorRate:\n","      print('Character error rate improved, save model')\n","      bestCharErrorRate = charErrorRate\n","      noImprovementSince = 0\n","      model.save()\n","      open(FilePaths.fnAccuracy, 'w').write('Validation character error rate of saved model: %f%%' % (charErrorRate*100.0))\n","    else:\n","      print('Character error rate not improved')\n","      noImprovementSince += 1\n","\n","    # stop training if no more improvement in the last x epochs\n","    if noImprovementSince >= earlyStopping:\n","      print('No more improvement since %d epochs. Training stopped.' % earlyStopping)\n","      break\n","\n","\n","def validate(model, loader):\n","  \"validate NN\"\n","  print('Validate NN')\n","  loader.validationSet()\n","  numCharErr = 0\n","  numCharTotal = 0\n","  numWordOK = 0\n","  numWordTotal = 0\n","  while loader.hasNext():\n","    iterInfo = loader.getIteratorInfo()\n","    print('Batch:', iterInfo[0],'/', iterInfo[1])\n","    batch = loader.getNext()\n","    recognized = model.inferBatch(batch)\n","    \n","    print('Ground truth -> Recognized')  \n","    for i in range(len(recognized)):\n","      numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n","      numWordTotal += 1\n","      dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n","      numCharErr += dist\n","      numCharTotal += len(batch.gtTexts[i])\n","      print('[OK]' if dist==0 else '[ERR:%d]' % dist,'\"' + batch.gtTexts[i] + '\"', '->', '\"' + recognized[i] + '\"')\n","  \n","  # print validation result\n","  charErrorRate = numCharErr / numCharTotal\n","  wordAccuracy = numWordOK / numWordTotal\n","  print('Character error rate: %f%%. Word accuracy: %f%%.' % (charErrorRate*100.0, wordAccuracy*100.0))\n","  return charErrorRate\n","\n","modelDir = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/model/'\n","\n","class Model: \n","  \"minimalistic TF model for HTR\"\n","\n","  # model constants\n","  batchSize = 50\n","  imgSize = (128, 32)\n","  maxTextLen = 32\n","\n","  def __init__(self, charList, decoderType=DecoderType.BestPath, mustRestore=False):\n","    \"init model: add CNN, RNN and CTC and initialize TF\"\n","    self.charList = charList\n","    self.decoderType = decoderType\n","    self.mustRestore = mustRestore\n","    self.snapID = 0\n","\n","    # CNN\n","    self.inputImgs = tf.placeholder(tf.float32, shape=(Model.batchSize, Model.imgSize[0], Model.imgSize[1]))\n","    cnnOut4d = self.setupCNN(self.inputImgs)\n","\n","    # RNN\n","    rnnOut3d = self.setupRNN(cnnOut4d)\n","\n","    # CTC\n","    (self.loss, self.decoder) = self.setupCTC(rnnOut3d)\n","\n","    # optimizer for NN parameters\n","    self.batchesTrained = 0\n","    self.learningRate = tf.placeholder(tf.float32, shape=[])\n","    self.optimizer = tf.train.RMSPropOptimizer(self.learningRate).minimize(self.loss)\n","\n","    # initialize TF\n","    (self.sess, self.saver) = self.setupTF()\n","\n","      \n","  def setupCNN(self, cnnIn3d):\n","    \"create CNN layers and return output of these layers\"\n","    cnnIn4d = tf.expand_dims(input=cnnIn3d, axis=3)\n","\n","    # list of parameters for the layers\n","    kernelVals = [5, 5, 3, 3, 3]\n","    featureVals = [1, 32, 64, 128, 128, 256]\n","    strideVals = poolVals = [(2,2), (2,2), (1,2), (1,2), (1,2)]\n","    numLayers = len(strideVals)\n","\n","    # create layers\n","    pool = cnnIn4d # input to first CNN layer\n","    for i in range(numLayers):\n","      kernel = tf.Variable(tf.truncated_normal([kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]], stddev=0.1))\n","      conv = tf.nn.conv2d(pool, kernel, padding='SAME',  strides=(1,1,1,1))\n","      relu = tf.nn.relu(conv)\n","      pool = tf.nn.max_pool(relu, (1, poolVals[i][0], poolVals[i][1], 1), (1, strideVals[i][0], strideVals[i][1], 1), 'VALID')\n","\n","    return pool\n","\n","\n","  def setupRNN(self, rnnIn4d):\n","    \"create RNN layers and return output of these layers\"\n","    rnnIn3d = tf.squeeze(rnnIn4d, axis=[2])\n","\n","    # basic cells which is used to build RNN\n","    numHidden = 256\n","    cells = [tf.contrib.rnn.LSTMCell(num_units=numHidden, state_is_tuple=True) for _ in range(2)] # 2 layers\n","\n","    # stack basic cells\n","    stacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n","\n","    # bidirectional RNN\n","    # BxTxF -> BxTx2H\n","    ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d, dtype=rnnIn3d.dtype)\n","                  \n","    # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n","    concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n","                  \n","    # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n","    kernel = tf.Variable(tf.truncated_normal([1, 1, numHidden * 2, len(self.charList) + 1], stddev=0.1))\n","    return tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n","    \n","\n","  def setupCTC(self, ctcIn3d):\n","    \"create CTC loss and decoder and return them\"\n","    # BxTxC -> TxBxC\n","    ctcIn3dTBC = tf.transpose(ctcIn3d, [1, 0, 2])\n","    # ground truth text as sparse tensor\n","    self.gtTexts = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]) , tf.placeholder(tf.int32, [None]), tf.placeholder(tf.int64, [2]))\n","    # calc loss for batch\n","    self.seqLen = tf.placeholder(tf.int32, [None])\n","    loss = tf.nn.ctc_loss(labels=self.gtTexts, inputs=ctcIn3dTBC, sequence_length=self.seqLen, ctc_merge_repeated=True)\n","    # decoder: either best path decoding or beam search decoding\n","    if self.decoderType == DecoderType.BestPath:\n","      decoder = tf.nn.ctc_greedy_decoder(inputs=ctcIn3dTBC, sequence_length=self.seqLen)\n","    elif self.decoderType == DecoderType.BeamSearch:\n","      decoder = tf.nn.ctc_beam_search_decoder(inputs=ctcIn3dTBC, sequence_length=self.seqLen, beam_width=50, merge_repeated=False)\n","    elif self.decoderType == DecoderType.WordBeamSearch:\n","      # import compiled word beam search operation (see https://github.com/githubharald/CTCWordBeamSearch)\n","      word_beam_search_module = tf.load_op_library('TFWordBeamSearch.so')\n","\n","      # prepare information about language (dictionary, characters in dataset, characters forming words) \n","      chars = str().join(self.charList)\n","      wordChars = open(modelDir+'wordCharList.txt').read().splitlines()[0]\n","      corpus = open(modelDir+'corpus.txt').read()\n","\n","      # decode using the \"Words\" mode of word beam search\n","      decoder = word_beam_search_module.word_beam_search(tf.nn.softmax(ctcIn3dTBC, dim=2), 50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'), wordChars.encode('utf8'))\n","\n","    # return a CTC operation to compute the loss and a CTC operation to decode the RNN output\n","    return (tf.reduce_mean(loss), decoder)\n","\n","\n","  def setupTF(self):\n","    \"initialize TF\"\n","    \n","    print('Python: '+sys.version)\n","    print('Tensorflow: '+tf.__version__)\n","\n","    sess=tf.Session() # TF session\n","    \n","    modelDir = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/model/'\n","\n","    saver = tf.train.Saver(max_to_keep=1) # saver saves model to file\n","    #modelDir = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/model/'\n","    #modelDir = '/Users/haithem.afli/Desktop/CIT/Research/IndustryProjects/Unitek/NN_OCR/run_model/model/'\n","    latestSnapshot = tf.train.latest_checkpoint(modelDir) # is there a saved model?\n","\n","    # if model must be restored (for inference), there must be a snapshot\n","    if self.mustRestore and not latestSnapshot:\n","      raise Exception('No saved model found in: ' + modelDir)\n","\n","    # load saved model if available\n","    if latestSnapshot:\n","      print('Init with stored values from ' + latestSnapshot)\n","      saver.restore(sess, latestSnapshot)\n","    else:\n","      print('Init with new values')\n","      sess.run(tf.global_variables_initializer())\n","\n","    return (sess,saver)\n","\n","\n","  def toSparse(self, texts):\n","    \"put ground truth texts into sparse tensor for ctc_loss\"\n","    indices = []\n","    values = []\n","    shape = [len(texts), 0] # last entry must be max(labelList[i])\n","\n","    # go over all texts\n","    for (batchElement, text) in enumerate(texts):\n","      # convert to string of label (i.e. class-ids)\n","      labelStr = [self.charList.index(c) for c in text]\n","      # sparse tensor must have size of max. label-string\n","      if len(labelStr) > shape[1]:\n","        shape[1] = len(labelStr)\n","      # put each label into sparse tensor\n","      for (i, label) in enumerate(labelStr):\n","        indices.append([batchElement, i])\n","        values.append(label)\n","\n","    return (indices, values, shape)\n","\n","\n","  def decoderOutputToText(self, ctcOutput):\n","    \"extract texts from output of CTC decoder\"\n","    \n","    # contains string of labels for each batch element\n","    encodedLabelStrs = [[] for i in range(Model.batchSize)]\n","\n","    # word beam search: label strings terminated by blank\n","    if self.decoderType == DecoderType.WordBeamSearch:\n","      blank=len(self.charList)\n","      for b in range(Model.batchSize):\n","        for label in ctcOutput[b]:\n","          if label==blank:\n","            break\n","          encodedLabelStrs[b].append(label)\n","\n","    # TF decoders: label strings are contained in sparse tensor\n","    else:\n","      # ctc returns tuple, first element is SparseTensor \n","      decoded=ctcOutput[0][0] \n","\n","      # go over all indices and save mapping: batch -> values\n","      idxDict = { b : [] for b in range(Model.batchSize) }\n","      for (idx, idx2d) in enumerate(decoded.indices):\n","        label = decoded.values[idx]\n","        batchElement = idx2d[0] # index according to [b,t]\n","        encodedLabelStrs[batchElement].append(label)\n","\n","    # map labels to chars for all batch elements\n","    return [str().join([self.charList[c] for c in labelStr]) for labelStr in encodedLabelStrs]\n","\n","\n","  def trainBatch(self, batch):\n","    \"feed a batch into the NN to train it\"\n","    sparse = self.toSparse(batch.gtTexts)\n","    rate = 0.01 if self.batchesTrained < 10 else (0.001 if self.batchesTrained < 10000 else 0.0001) # decay learning rate\n","    (_, lossVal) = self.sess.run([self.optimizer, self.loss], { self.inputImgs : batch.imgs, self.gtTexts : sparse , self.seqLen : [Model.maxTextLen] * Model.batchSize, self.learningRate : rate} )\n","    self.batchesTrained += 1\n","    return lossVal\n","\n","\n","  def inferBatch(self, batch):\n","    \"feed a batch into the NN to recngnize the texts\"\n","    decoded = self.sess.run(self.decoder, { self.inputImgs : batch.imgs, self.seqLen : [Model.maxTextLen] * Model.batchSize } )\n","    return self.decoderOutputToText(decoded)\n","  \n","\n","  def save(self):\n","    \"save model to file\"\n","    self.snapID += 1\n","    self.saver.save(self.sess, 'model/snapshot', global_step=self.snapID)\n"," \n","\n","\n","def infer(model, fnImg):\n","  \"recognize text in image provided by file path\"\n","  img = preprocess(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), Model.imgSize)\n","  batch = Batch(None, [img] * Model.batchSize) # fill all batch elements with same input image\n","  recognized = model.inferBatch(batch) # recognize text\n","  print('Recognized:', '\"' + recognized[0] + '\"') # all batch elements hold same result\n","  return recognized[0]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\"\"\" The above Part has directly been taken from the lecturer code.\n","    Below, the main function has been modified to take the output of the lecturer code had process the solution proposed to correct the word\"\"\"\n","\n","\n","\n","\n","\n","# the main function has been modified to support the proposed solution to correct the word.\n","def main():\n","  \"main function\"\n","  \n","  #this part has been taken from the lecturer code\n","  decoderType = DecoderType.BestPath\n","\n","  print(open(FilePaths.fnAccuracy).read())\n","  model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True)\n","  #infer(model, FilePaths.fnInfer)\n","  \n","  \n","  #this part below has been changed to put in place our solution.\n","  \n","  print('\\n####### STARTING THE ERROR CORRECTION MODULE #######')\n","  \n","  sentence = infer(model, FilePaths.fnInfer)         # sentence take the output of the lecturer code\n","  print('Sentence to study:' , '\"' , sentence , '\"')\n","     \n","  # Below are the list of parameter we apply for our solution\n","  #corpus_file = '/content/drive/My Drive/Colab Notebooks/Project1NLP_Model/europarl/txt/en/all_en.txt'  # now we will consider the corpus file as input  \n","  ngram = 2                                                             # we are considering the bigram for our solution\n","  word_delta_len_modulation = 0                                         # we are not accepting any delta letter more or less than the original word (only spelling mistake is considered)\n","  word_similarity_acceptation = 0.5                                     # we only propose a similarity of 50% from the best proposition for our range of proposed word for correction\n","  word_selection_type = 'contextual'                                    # we select the best candidate word for replacement using its context in the sentence \n","  \n","  # below we apply our methodology for word correction\n","  \n","  # dictionary is already build before the ECM test. Therefore, there is no need to create a dictionary.\n","  #dict_corpus = words_dict(corpus_file, 2)                              # we build our dictionnary\n","  \n","  add_word_dict('Unitek',dict_corpus,ngram)        # we add the \"unitek\" word in our dictionnary\n","  final_sentence = Error_Correction_Module(sentence, dict_corpus, ngram, word_delta_len_modulation, word_similarity_acceptation, word_selection_type) # Launching the ECM process\n","\n","  print()\n","  print(\"Final Result: \", final_sentence)                               # the final result is printed\n","  \n","if __name__ == '__main__':\n","  main()\n","\n"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Validation character error rate of saved model: 13.956289%\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-18-84db2a627f23>:205: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-18-84db2a627f23>:208: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-18-84db2a627f23>:212: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","Python: 3.6.8 (default, Oct  7 2019, 12:59:55) \n","[GCC 8.3.0]\n","Tensorflow: 1.15.0\n","Init with stored values from /content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/model/snapshot-32\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Project1NLP_Model/Project1NLP_Model/model/snapshot-32\n","\n","####### STARTING THE ERROR CORRECTION MODULE #######\n","Recognized: \"Unetek\"\n","Sentence to study: \" Unetek \"\n","Adding words in dictionnary: \"Unitek\"\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Sentence to expand: \"Unetek\"\n","Sentence after expansion: \"Unetek\"\n","Sentence before tokenization: \"Unetek\"\n","Sentence after tokenization: \" ['Unetek'] \"\n","Sentence before tokenization: \"Unetek\"\n","Sentence after tokenization: \" ['Unetek'] \"\n","Words candidate for correction of word  \"unetek\"  using ngram technique:  [('unitek', 3), ('nextek', 3), ('canete', 3), ('brunet', 3), ('retune', 3), ('tenets', 3), ('netted', 3), ('tennet', 3), ('teleki', 2), ('franek', 2), ('sponek', 2), ('petter', 2), ('rutnet', 2), ('leonet', 2), ('magnet', 2), ('steter', 2), ('yvette', 2), ('meteor', 2), ('eionet', 2), ('unamet', 2), ('tetchy', 2), ('nether', 2), ('zetter', 2), ('monnet', 2), ('chanet', 2), ('peters', 2), ('silete', 2), ('iconet', 2), ('nettle', 2), ('petten', 2), ('effete', 2), ('wetter', 2), ('cetane', 2), ('setter', 2), ('letter', 2), ('voynet', 2), ('fernet', 2), ('tonnet', 2), ('detect', 2), ('exeter', 2), ('metteg', 2), ('pieter', 2), ('meters', 2), ('etelka', 2), ('vetted', 2), ('gamete', 2), ('getter', 2), ('ethane', 2), ('sisnet', 2), ('veneta', 2), ('veneto', 2), ('retest', 2), ('egonet', 2), ('mettez', 2), ('metten', 2), ('better', 2), ('ninety', 2), ('teeter', 2), ('hornet', 2), ('bonnet', 2), ('delete', 2), ('dieter', 2), ('deters', 2), ('tetovo', 2), ('thanet', 2), ('voinet', 2), ('detest', 2), ('aetate', 2), ('eranet', 2), ('cetera', 2), ('tether', 2), ('untaet', 2), ('sernet', 2), ('barnet', 2), ('planet', 2), ('sunset', 2), ('muette', 2), ('kunewa', 2), ('munere', 2), ('negate', 2), ('attune', 2), ('immune', 2), ('funnel', 2), ('unmine', 2), ('itunes', 2), ('neuter', 2), ('pruned', 2), ('kuneva', 2), ('beaune', 2), ('unione', 2), ('brunei', 2), ('gunned', 2), ('prunes', 2), ('uneven', 2), ('nested', 2), ('unease', 2), ('neater', 2), ('runner', 2), ('undine', 2), ('unesco', 2), ('undone', 2), ('tunnel', 2), ('brunel', 2), ('eluned', 2), ('uneasy', 2), ('gunter', 2), ('punter', 2), ('suntex', 2), ('hunted', 2), ('unites', 2), ('hunter', 2), ('united', 2), ('uniate', 2)]\n","Words Top candidates for correction of word  \"unetek\"   after Levenshtein distance calculation:  [('unitek', 1.0)]\n","Best word for replacement:  unitek\n","Tokenized and expanded sentence before correction:  ['Unetek']\n","Tokenized and expanded sentence after correction:  ['unitek']\n","Contracted and formatted sentence after correction:  ['Unitek']\n","Untokenized solution sentence:  Unitek\n","\n","Final Result:  Unitek\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8kfJw2Y0DWow","colab_type":"text"},"source":["# Conclusion\n","\n","The ECM implemented is relatively strong as:\n","* It corrects also short word mispelling\n","* It is able to detect the context of the mispelled word and correct it in many cases.\n","* And it also keep the formatting of the original sentence (with its contraction and upper case letter).\n","\n","However, we have seen some limitation of our ECM model. Typically, if the Levenshtein distance of the proposed words are identical but their context are different (transitive verbs, intransitive verbs, noun, adjective, ...), the ECM is able to propose an accurate word proposal for correction. But as soon as their context get very similar (both are transitive verbs, intransitive verbs, noun, or adjective, ...), our ECM has difficulty to propose an accurate solution.\n","\n","2 ways can be done to avoid such limitaion:\n","1. The ECM model should extend the context to a wider one (it is currently a bigram context, we should think to use for example a trigram context)\n","1. The ECM could also integrate a Machine Learning module to identify use cases of the proposed word for correction.\n","\n"]}]}