# -*- coding: utf-8 -*-
"""DL_Assignment2_Part_A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18m0fac_XTCF7MKUDEFC4OoPTeYRYYWQt

\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
#     PART A Section I       # 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip "/content/gdrive/My Drive/Colab Notebooks/DL Assignment 2/data1-2.h5.zip"

import numpy as np
import h5py

# Function to load Images from data1.h5 
def loadDataH5():
  with h5py.File('data1.h5','r') as hf:
    trainX = np.array(hf.get('trainX'))
    trainY = np.array(hf.get('trainY'))
    valX = np.array(hf.get('valX'))
    valY = np.array(hf.get('valY'))
    print (trainX.shape,trainY.shape)
    print (valX.shape,valY.shape)
  return trainX, trainY, valX, valY

trainX, trainY, testX, testY = loadDataH5()

"""##1) CNN 1 conv + 1 MaxPool

The below CNN implementation will study a basic CNN baseline with 1 convolutional layer and 1 Max Pooling layer
"""

import tensorflow as tf

# Create class ShallowNet to build CNN model containing layers of convolutional and pooling
class ShallowNet:
  # function for creating the CNN model
  def build(width, height, depth, classes):
    # Sequential model with 1 conv, 1 Maxpooling and after flattening, the last soft max activation layer
    model = tf.keras.Sequential()
    inputShape = (height, width, depth)
    # convolutional Layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # Max pooling Layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding="same"))
    # the output is flattenned ...
    model.add(tf.keras.layers.Flatten())
    # ... before feeding the softmax activation layer 
    model.add(tf.keras.layers.Dense(classes, activation='softmax'))
    
    return model

# Nb of epochs
NUM_EPOCHS = 15

# Batch size
batchSize = 51


print("Compiling model...")
# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model = ShallowNet.build(width=128, height=128, depth=3, classes=17)

print (model.summary())
# CNN Model is compiled with the above optimizer
model.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# CNN model is trained with data1.h5 data with the above specified batch size and nb of epochs
print("Training network...")
H = model.fit(trainX, trainY, validation_data=(testX, testY),batch_size=batchSize, epochs=NUM_EPOCHS)

import matplotlib.pyplot as plt

# The above trained sample results is plot.
plt.style.use("ggplot")
plt.figure()
# Train loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["loss"], label="train_loss")
# Validation loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["val_loss"], label="val_loss")
# Train accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["acc"], label="train_acc")
# Validation accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""##2) CNN 2 conv + 2  MaxPool
The below CNN implementation will study a basic CNN baseline with 2 convolutional layers and 2 Max Pooling layers
"""

import tensorflow as tf

# Create class ShallowNet to build CNN model containing 2 layers of convolutional and 2 max pooling
class ShallowNet2:
  # function for creating the CNN model
  def build(width, height, depth, classes):
    # Sequential model with 2 conv, 2 Maxpooling and after flattening, the last soft max activation layer
    model = tf.keras.Sequential()
    inputShape = (height, width, depth)
    # 1st convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 1st Max Pooling layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding="same"))
    # 2nd convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 2nd Max Pooling layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding="same"))
    # the output is flattenned ...
    model.add(tf.keras.layers.Flatten())
    # ... before feeding the softmax activation layer
    model.add(tf.keras.layers.Dense(classes, activation='softmax'))
    
    return model

# Nb of epochs
NUM_EPOCHS = 15

# Batch size
batchSize = 51


print("Compiling model...")
# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model2 = ShallowNet2.build(width=128, height=128, depth=3, classes=17)

print (model2.summary())
# CNN Model is compiled with the above optimizer
model2.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# CNN model is trained with data1.h5 data with the above specified batch size and nb of epochs
print("Training network...")
H2 = model2.fit(trainX, trainY, validation_data=(testX, testY),batch_size=batchSize, epochs=NUM_EPOCHS)

import matplotlib.pyplot as plt

# The above trained sample results is plot.
plt.style.use("ggplot")
plt.figure()
# Train loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["loss"], label="train_loss")
# Validation loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_loss"], label="val_loss")
# Train accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["acc"], label="train_acc")
# Validation accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""##3) CNN 6 conv + 3  MaxPool
The below CNN implementation will study a basic CNN baseline with 6 convolutional layers and 3 Max Pooling layers
"""

import tensorflow as tf

# Create class ShallowNet to build CNN model containing layers of convolutional and pooling
class ShallowNet3:
  # function for creating the CNN model
  def build(width, height, depth, classes):
    # Sequential model with 2 conv, 2 Maxpooling and after flattening, the last soft max activation layer
    model = tf.keras.Sequential()
    inputShape = (height, width, depth)
    # 1st convolutional layer
    model.add(tf.keras.layers.Conv2D(32, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 2nd convolutional layer
    model.add(tf.keras.layers.Conv2D(32, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 1st Max Pooling layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding="same"))
    # 3rd convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 4th convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 2nd Max Pooling layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding="same"))
    # 5th convolutional layer
    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 6th convolutional layer
    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding="same",input_shape=inputShape, activation='relu'))
    # 3rd Max Pooling layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding="same"))
    # the output is flattenned ...
    model.add(tf.keras.layers.Flatten())
    # ... before feeding the softmax activation layer
    model.add(tf.keras.layers.Dense(classes, activation='softmax'))
    
    return model

# Nb of epochs
NUM_EPOCHS = 25

# Batch size
batchSize = 51


print("Compiling model...")
# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model3 = ShallowNet3.build(width=128, height=128, depth=3, classes=17)

print (model3.summary())
# CNN Model is compiled with the above optimizer
model3.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# CNN model is trained with data1.h5 data with the above specified batch size and nb of epochs
print("Training network...")
H3 = model3.fit(trainX, trainY, validation_data=(testX, testY),batch_size=batchSize, epochs=NUM_EPOCHS)

import matplotlib.pyplot as plt

# The above trained sample results is plot.
plt.style.use("ggplot")
plt.figure()
# Train loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["loss"], label="train_loss")
# Validation loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["val_loss"], label="val_loss")
# Train accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["acc"], label="train_acc")
# Validation accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
#     PART A Section II       # 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

##1) Data Augmentation on CNN 1 conv + 1 MaxPool
"""

numTrainingSamples= 2000
numValidationSamples= 800
# Nb of epochs
NUM_EPOCHS = 100
# Batch size
batchSize = 51

# construct the image generator for data augmentation with the following parameters:
#       Allowed rotation range : 40 degrees
#       Allowed shift : 1/10 from original image
#       Allowed crop : 0.2
#       Allowed zoom range : 40
#       Allowed horizontal flip : true
trainDataGenerator= tf.keras.preprocessing.image.ImageDataGenerator( 
    rotation_range=40, width_shift_range=0.1, shear_range=0.2, zoom_range=0.4, horizontal_flip=True)

# trainX data will be used for data augmentaation
trainDataGenerator.fit(trainX, augment=True)

# Data is generated on the flow
train_generator= trainDataGenerator.flow(trainX, trainY, batch_size=batchSize)

# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model = ShallowNet.build(width=128, height=128, depth=3, classes=17)

# CNN Model is compiled with the above optimizer
model.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# Our 1st CNN model is trained with augmented data
H = model.fit_generator(
    train_generator,
    steps_per_epoch= numTrainingSamples,
    epochs=NUM_EPOCHS,  
    validation_data=(testX,testY)
)

import matplotlib.pyplot as plt
# plot the training loss and accuracy of 1st CNN model
plt.style.use("ggplot")
plt.figure()
# Train loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["loss"], label="train_loss")
# Validation loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["val_loss"], label="val_loss")
# Train accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["acc"], label="train_acc")
# Validation accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""##2) Data Augmentation on CNN 2 conv + 2 MaxPool"""

# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model2 = ShallowNet2.build(width=128, height=128, depth=3, classes=17)

# CNN Model is compiled with the above optimizer
model2.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# Our 2nd CNN model is trained with augmented data
H2 = model2.fit_generator(
    train_generator,
    steps_per_epoch= numTrainingSamples,
    epochs=NUM_EPOCHS,  
    validation_data=(testX,testY)
)

import matplotlib.pyplot as plt
# plot the training loss and accuracy of 2nd CNN model
plt.style.use("ggplot")
plt.figure()
# Train loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["loss"], label="train_loss")
# Validation loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_loss"], label="val_loss")
# Train accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["acc"], label="train_acc")
# Validation accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""##3) Data Augmentation on CNN 6 conv + 3 MaxPool"""

# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model3 = ShallowNet3.build(width=128, height=128, depth=3, classes=17)

# CNN Model is compiled with the above optimizer
model3.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# Our 3rd CNN model is trained with augmented data
H3 = model3.fit_generator(
    train_generator,
    steps_per_epoch= numTrainingSamples,
    epochs=NUM_EPOCHS,  
    validation_data=(testX,testY)
)

import matplotlib.pyplot as plt
# plot the training loss and accuracy of 3rd CNN model
plt.style.use("ggplot")
plt.figure()
# Train loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["loss"], label="train_loss")
# Validation loss is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["val_loss"], label="val_loss")
# Train accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["acc"], label="train_acc")
# Validation accuracy is plot
plt.plot(np.arange(0, NUM_EPOCHS), H3.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""##4) Data Augmentation without horizontal flip on CNN 2 conv + 2 **MaxPool**"""

numTrainingSamples= 2000
numValidationSamples= 800
# Nb of epochs
NUM_EPOCHS = 100
# Batch size
batchSize = 51

# construct the image generator for data augmentation with the following parameters:
#       Allowed rotation range : 40 degrees
#       Allowed shift : 1/10 from original image
#       Allowed crop : 0.2
#       Allowed zoom range : 40
# No horizontal flip parameter is used here
trainDataGenerator= tf.keras.preprocessing.image.ImageDataGenerator( 
    rotation_range=40, width_shift_range=0.1, shear_range=0.2, zoom_range=0.4)

# trainX data will be used for data augmentaation
trainDataGenerator.fit(trainX, augment=True)

# Data is generated on the flow
train_generator= trainDataGenerator.flow(trainX, trainY, batch_size=batchSize)

# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model2 = ShallowNet2.build(width=128, height=128, depth=3, classes=17)

# CNN Model is compiled with the above optimizer
model2.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# Our 2nd CNN model is trained with augmented data
H2 = model2.fit_generator(
    train_generator,
    steps_per_epoch= numTrainingSamples,
    epochs=NUM_EPOCHS,  
    validation_data=(testX,testY)
)

import matplotlib.pyplot as plt
# plot the training loss and accuracy of 2nd model
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["loss"], label="train_loss")
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["acc"], label="train_acc")
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""##5) Data Augmentation without crop on CNN 2 conv + 2 **MaxPool**"""

numTrainingSamples= 2000
numValidationSamples= 800
# Nb of epochs
NUM_EPOCHS = 100
# Batch size
batchSize = 51

# construct the image generator for data augmentation with the following parameters:
#       Allowed rotation range : 40 degrees
#       Allowed shift : 1/10 from original image
#       Allowed zoom range : 40
#       Allowed horizontal flip : true
# No crop/shear parameter is used here
trainDataGenerator= tf.keras.preprocessing.image.ImageDataGenerator( 
    rotation_range=40, width_shift_range=0.1, zoom_range=0.4, horizontal_flip=True)

# trainX data will be used for data augmentaation
trainDataGenerator.fit(trainX, augment=True)

# Data is generated on the flow
train_generator= trainDataGenerator.flow(trainX, trainY, batch_size=batchSize)

# Choice of an optimizer: SGD Optimizer with a learning rate of 0.01 
opt = tf.keras.optimizers.SGD(lr=0.01)

# CNN model is built for iputs image of 128x128x3
model2 = ShallowNet2.build(width=128, height=128, depth=3, classes=17)

# CNN Model is compiled with the above optimizer
model2.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

# Our 2nd CNN model is trained with augmented data
H2 = model2.fit_generator(
    train_generator,
    steps_per_epoch= numTrainingSamples,
    epochs=NUM_EPOCHS,  
    validation_data=(testX,testY)
)

import matplotlib.pyplot as plt
# plot the training loss and accuracy of 2nd model
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["loss"], label="train_loss")
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["acc"], label="train_acc")
plt.plot(np.arange(0, NUM_EPOCHS), H2.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()

"""\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
#     PART A Section III       # 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
"""

# Create class nin_cnn_model to build CNN model containing layers of convolutional and pooling
class nin_cnn_model:
  #function for creating the CNN model
  def build(width, height, depth, classes):
    # Sequential model with 2 conv, 2 dropout, 1 Maxpooling and after flattening, the last soft max activation layer
    model = tf.keras.Sequential()
    inputShape = (height, width, depth)
    # 1st convolutional Layer
    model.add(tf.keras.layers.Conv2D(32, (8, 8), padding="valid",input_shape=inputShape, activation='relu'))
    # Max pooling Layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    # 1st Dropout layer
    model.add(tf.keras.layers.Dropout(0.5))
    # 2nd convolutional Layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="valid", activation='relu'))
    # 2nd Dropout layer
    model.add(tf.keras.layers.Dropout(0.5))
    # the output is flattenned ...
    model.add(tf.keras.layers.Flatten())
    # ... before feeding the softmax activation layer 
    model.add(tf.keras.layers.Dense(classes, activation='softmax'))

    return model

# Nb of epochs
NUM_EPOCHS = 10
# Batch size
batchSize = 51

# Create class Train_model to train the CNN model
class Train_model:
  def train(modeltrain, X_train, Y_train, X_test, Y_test, nb_epochs, bs):
    print("Compiling model...")
    # Choice of an optimizer: SGD Optimizer with a learning rate of 0.01
    opt = tf.keras.optimizers.SGD(lr=0.01)
    # CNN model is built for iputs image of 128x128x3
    model = modeltrain.build(width=128, height=128, depth=3, classes=17)

    print (model.summary())
    # CNN Model is compiled with the above optimizer
    model.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

    # CNN model is trained with data1.h5 data with the above specified batch size and nb of epochs
    print("Training network...")
    H = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),batch_size=batchSize, epochs=NUM_EPOCHS)
    
    # we return the model, its history and nb of epoch
    return model, H, nb_epochs

### In this section we are trying to train 10 identical models to build later on an ensemble

import random

# List of models which will be trained
trained_models = []

# List of validation accuracy for each trained model
trained_acc = []

# A loop is created to train 10 models
for model in list(range(10)):
  # the model is trained through the Train_model class, train function
  # the functions output 3 objects: the model, its history and the nb of epoch
  conv_pool_model, conv_pool_history, conv_pool_epochs = Train_model.train(nin_cnn_model, trainX, trainY, testX, testY, NUM_EPOCHS, batchSize)
  # the model is appended to the trained list
  trained_models.append(conv_pool_model)
  # the validation accuracy of the trained model is also appended to the trained_acc list
  trained_acc.append(conv_pool_history.history["val_acc"][-1])

### In this section we are building an ensemble of the trained models

# nb of trained models
nb_models = len(trained_models)

#initilization of the prediction sum
sum_pred = [0]

# Loop iterating through each model
for mod in trained_models:
  # Make a prediction on testX data through each trained model
  prediction = mod.predict(testX)
  # sum those arrays of predictions
  sum_pred = np.add(sum_pred,prediction)

# Compute the average predictions
average_pred = np.divide(sum_pred,float(nb_models))

# select the class with the highest value
predictions = np.argmax(average_pred, axis=1)
# Check if the model prediction is correct (True if prediction correct, False otherwise)
correct = np.equal(predictions, testY)
# Conversion of the boolean array into a numerical array (1 if True, 0 if False)
pred_correct = correct.astype(np.float32)
# mean value of predictions_correct
accuracy = np.mean(pred_correct)

# Display each accuracy of the trained model
for acc in list(range(len(trained_acc))):
  print("Accuracy of trained model ",acc, ":", trained_acc[acc])
print()

# print the overall accuracy of the ensemble of trained models
print("Accuracy of the ensemble:",accuracy)

# Create class nin_cnn_model2 to build CNN model containing layers of convolutional and pooling
class nin_cnn_model2:
  #function for creating the CNN model
  def build(width, height, depth, classes, initialization = "RandomNormal" ):
    # Choose an initialization
    if initialization == "RandomNormal":
      init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)
    elif initialization == "RandomUniform":
      init=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)
    elif initialization == "glorot_uniform":
      init=tf.keras.initializers.glorot_uniform(seed=None)
    elif initialization == "lecun_normal":
      init=tf.keras.initializers.lecun_normal(seed=None)
      
    # Sequential model with 2 conv, 2 dropout, 1 Maxpooling and after flattening, the last soft max activation layer
    model = tf.keras.Sequential()
    inputShape = (height, width, depth)
    # 1st convolutional Layer
    model.add(tf.keras.layers.Conv2D(32, (8, 8), padding="valid",input_shape=inputShape, activation='relu', kernel_initializer=init, bias_initializer=init))
    # Max pooling Layer
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    # 1st Dropout layer
    model.add(tf.keras.layers.Dropout(0.5))
    # 2nd convolutional Layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding="valid", activation='relu', kernel_initializer=init, bias_initializer=init))
    # 2nd Dropout layer
    model.add(tf.keras.layers.Dropout(0.5))
    # the output is flattenned ...
    model.add(tf.keras.layers.Flatten())
    # ... before feeding the softmax activation layer
    model.add(tf.keras.layers.Dense(classes, activation='softmax'))

    return model

import random
# Nb of epochs
NUM_EPOCHS = 10
# Batch size
batchSize = 51

# Create class Train_model to train the CNN model
class Train_model:
  def train(modeltrain, X_train, Y_train, X_test, Y_test, nb_epochs, bs, algo_opti="SGD"):
    print("Compiling model...")
    # Selection of an optimizer
    if algo_opti == "RMSprop":
      opt = tf.keras.optimizers.RMSprop(lr=0.001)
    elif algo_opti == "Nadam":
      opt = tf.keras.optimizers.Nadam(lr=0.001)
    elif algo_opti == "Adam":
      opt = tf.keras.optimizers.Adam(lr=0.001)
    elif algo_opti == "Adagrad":
      opt = tf.keras.optimizers.Adagrad(lr=0.001)
    else:
      opt = tf.keras.optimizers.SGD(lr=0.01)
    
    # list of available initializations
    algo_init = ["RandomNormal", "RandomUniform", "glorot_uniform", "lecun_normal"]
    # the selection of an initialization is random
    initializer = random.choice(algo_init)
    # CNN model is built for iputs image of 128x128x3
    model = modeltrain.build(width=128, height=128, depth=3, classes=17, initialization = initializer)

    print (model.summary())
    # CNN Model is compiled with a selected optimizer
    model.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

    # CNN model is trained with data1.h5 data with the above specified batch size and nb of epochs
    print("Training network...")
    H = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),batch_size=batchSize, epochs=NUM_EPOCHS)
    
    # we return the model, its history and nb of epoch
    return model, H, nb_epochs

### In this section we are trying to train 10 identical models to build later on an ensemble

import random

# List of models which will be trained
trained_models = []

# List of validation accuracy for each trained model
trained_acc = []

# list of available optimizers
algo_optimizers = ["SGD", "Adam", "Nadam", "RMSprop", "Adagrad"]

# A loop is created to train 10 models
for model in list(range(10)):
  # the choice of the optimizer is random
  opti = random.choice(algo_optimizers)
  print("Selection of optimizer :", opti)
  
  # the model is trained through the Train_model class, train function
  # the functions output 3 objects: the model, its history and the nb of epoch
  conv_pool_model, conv_pool_history, conv_pool_epochs = Train_model.train(nin_cnn_model2, trainX, trainY, testX, testY, NUM_EPOCHS, batchSize, opti)
  # the model is appended to the trained list
  trained_models.append(conv_pool_model)
  # the validation accuracy of the trained model is also appended to the trained_acc list
  trained_acc.append(conv_pool_history.history["val_acc"][-1])

### In this section we are building an ensemble of the trained models

# nb of trained models
nb_models = len(trained_models)

#initilization of the prediction sum
sum_pred = [0]

# Loop iterating through each model
for mod in trained_models:
  # Make a prediction on testX data through each trained model
  prediction = mod.predict(testX)
  # sum those arrays of predictions
  sum_pred = np.add(sum_pred,prediction)

# Compute the average predictions
average_pred = np.divide(sum_pred,float(nb_models))

# select the class with the highest value
predictions = np.argmax(average_pred, axis=1)
# Check if the model prediction is correct (True if prediction correct, False otherwise)
correct = np.equal(predictions, testY)
# Conversion of the boolean array into a numerical array (1 if True, 0 if False)
pred_correct = correct.astype(np.float32)
# mean value of predictions_correct
accuracy = np.mean(pred_correct)

# Display each accuracy of the trained model
for acc in list(range(len(trained_acc))):
  print("Accuracy of trained model ",acc, ":", trained_acc[acc])
print()

# print the overall accuracy of the ensemble of trained models
print("Accuracy of the ensemble:",accuracy)