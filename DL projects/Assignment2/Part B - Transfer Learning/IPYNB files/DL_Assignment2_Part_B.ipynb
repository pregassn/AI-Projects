{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment2_Part_B.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SZp0iRsBsw4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\n",
        "#     PART B Section I       # \n",
        "\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPHcpaPIxagd",
        "colab_type": "code",
        "outputId": "36b9641d-02fd-41ed-cfcf-7ddd87b8cab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip \"/content/gdrive/My Drive/Colab Notebooks/DL Assignment 2/data1-2.h5.zip\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  /content/gdrive/My Drive/Colab Notebooks/DL Assignment 2/data1-2.h5.zip\n",
            "replace data1.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXCUVzI-x2PI",
        "colab_type": "code",
        "outputId": "1f537a63-868f-47ac-e1f8-0edc496c51a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to load Images from data1.h5 \n",
        "def loadDataH5():\n",
        "  with h5py.File('data1.h5','r') as hf:\n",
        "    trainX = np.array(hf.get('trainX'))\n",
        "    trainY = np.array(hf.get('trainY'))\n",
        "    valX = np.array(hf.get('valX'))\n",
        "    valY = np.array(hf.get('valY'))\n",
        "    print (trainX.shape,trainY.shape)\n",
        "    print (valX.shape,valY.shape)\n",
        "  return trainX, trainY, valX, valY\n",
        "\n",
        "trainX, trainY, testX, testY = loadDataH5()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1020, 128, 128, 3) (1020,)\n",
            "(340, 128, 128, 3) (340,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHlNO0G279h",
        "colab_type": "text"
      },
      "source": [
        "##1) CNN Model VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd3pt-ETzZVz",
        "colab_type": "code",
        "outputId": "223f17d2-2068-457c-c427-79e571bc5889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Importation of pre trained CNN VGG16\n",
        "vggModel= tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "print (vggModel.summary())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxO7TBn-11km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A prediction is made on training data\n",
        "featuresTrain= vggModel.predict(trainX)\n",
        "\n",
        "#reshape to flatten feature data\n",
        "featuresTrain= featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "#  A prediction is made on test data\n",
        "featuresVal = vggModel.predict(testX)\n",
        "\n",
        "#reshape to flatten feature data\n",
        "featuresVal= featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1npdFN2SQpaP",
        "colab_type": "code",
        "outputId": "d112573c-9b5c-4b80-d19b-28d420447b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "### Section evaluating the different ML classifer\n",
        "\n",
        "print(\"### CNN Model: VGG16 \")\n",
        "print(\"  Accuracy Evaluation of different ML classifier\")\n",
        "\n",
        "\n",
        "## Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "model = RandomForestClassifier(700)\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Random Forest:\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "\n",
        "## Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Decision Tree:\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "\n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=9, p=3)\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     KNN          :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Naive Bayes  :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "model = SVC(gamma=\"auto\")\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     SVM          :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CNN Model: VGG16 \n",
            "  Accuracy Evaluation of different ML classifier\n",
            "     Random Forest: 0.8470588235294118\n",
            "     Decision Tree: 0.4647058823529412\n",
            "     KNN          : 0.7088235294117647\n",
            "     Naive Bayes  : 0.4676470588235294\n",
            "     SVM          : 0.7617647058823529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMcxxfB13cy-",
        "colab_type": "text"
      },
      "source": [
        "##2) CNN Model InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewbTUdBfYqwJ",
        "colab_type": "code",
        "outputId": "2a83b33f-0363-49b5-c2d6-6b2656b770be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11242
        }
      },
      "source": [
        "### CNN model: InceptionV3\n",
        "\n",
        "# Importation of pre trained CNN InceptionV3\n",
        "InceptionV3Model= tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "print (InceptionV3Model.summary())\n",
        "\n",
        "# A prediction is made on training data\n",
        "featuresTrain= InceptionV3Model.predict(trainX)\n",
        "\n",
        "#reshape to flatten feature data\n",
        "featuresTrain= featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "# A prediction is made on test data\n",
        "featuresVal = InceptionV3Model.predict(testX)\n",
        "\n",
        "#reshape to flatten feature data\n",
        "featuresVal= featuresVal.reshape(featuresVal.shape[0], -1)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 63, 63, 32)   864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 63, 63, 32)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 63, 63, 32)   0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 61, 61, 32)   9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 61, 61, 32)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 61, 61, 32)   0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 61, 61, 64)   18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 61, 61, 64)   192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 61, 61, 64)   0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 30, 30, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 30, 30, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 30, 30, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 30, 30, 80)   0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 28, 28, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 28, 28, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 28, 28, 192)  0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 13, 13, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 13, 13, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 13, 13, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 13, 13, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 13, 13, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 13, 13, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 13, 13, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 13, 13, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 13, 13, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 13, 13, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 13, 13, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 13, 13, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 13, 13, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 13, 13, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 13, 13, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 13, 13, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 13, 13, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 13, 13, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 13, 13, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 13, 13, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 13, 13, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 13, 13, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 13, 13, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 13, 13, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 13, 13, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 13, 13, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 13, 13, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 13, 13, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 13, 13, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 13, 13, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 13, 13, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 13, 13, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 13, 13, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 13, 13, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 13, 13, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 13, 13, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 13, 13, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 13, 13, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 13, 13, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 13, 13, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 13, 13, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 13, 13, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 13, 13, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 13, 13, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 13, 13, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 13, 13, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 13, 13, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 13, 13, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 13, 13, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 13, 13, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 13, 13, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 13, 13, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 13, 13, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 13, 13, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 13, 13, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 13, 13, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 13, 13, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 13, 13, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 13, 13, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 13, 13, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 13, 13, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 13, 13, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 13, 13, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 13, 13, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 13, 13, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 13, 13, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 13, 13, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 6, 6, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 6, 6, 96)     82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 6, 6, 384)    1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 6, 6, 96)     288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 6, 6, 384)    0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 6, 6, 96)     0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 6, 6, 768)    0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 6, 6, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 6, 6, 128)    384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 6, 6, 128)    0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 6, 6, 128)    114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 6, 6, 128)    384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 6, 6, 128)    0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 6, 6, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 6, 6, 128)    114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 6, 6, 128)    384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 6, 6, 128)    384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 6, 6, 128)    0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 6, 6, 128)    0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 6, 6, 128)    114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 6, 6, 128)    114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 6, 6, 128)    384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 6, 6, 128)    384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 6, 6, 128)    0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 6, 6, 128)    0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 6, 6, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 6, 6, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 192)    172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 6, 6, 192)    172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 6, 6, 192)    147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 6, 6, 192)    576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 6, 6, 192)    576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 6, 6, 192)    576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 6, 6, 192)    576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 6, 6, 768)    0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 6, 6, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 6, 6, 160)    480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 6, 6, 160)    179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 6, 6, 160)    480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 6, 6, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 6, 6, 160)    179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 6, 6, 160)    480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 6, 6, 160)    480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 6, 6, 160)    179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 6, 6, 160)    179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 6, 6, 160)    480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 6, 6, 160)    480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 6, 6, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 6, 6, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 6, 6, 192)    215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 6, 6, 192)    215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 6, 6, 192)    147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 6, 6, 192)    576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 6, 6, 192)    576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 6, 6, 192)    576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 6, 6, 192)    576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 6, 6, 768)    0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 6, 6, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 6, 6, 160)    480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 6, 6, 160)    179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 6, 6, 160)    480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 6, 6, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 6, 6, 160)    179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 6, 6, 160)    480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 6, 6, 160)    480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 6, 6, 160)    179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 6, 6, 160)    179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 6, 6, 160)    480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 6, 6, 160)    480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 6, 6, 160)    0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 6, 6, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 6, 6, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 6, 6, 192)    215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 6, 6, 192)    215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 6, 6, 192)    147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 6, 6, 192)    576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 6, 6, 192)    576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 6, 6, 192)    576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 6, 6, 192)    576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 6, 6, 768)    0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 6, 6, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 6, 6, 192)    576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 6, 6, 192)    258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 6, 6, 192)    576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 6, 6, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 6, 6, 192)    258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 6, 6, 192)    576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 6, 6, 192)    576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 6, 6, 192)    258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 6, 6, 192)    258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 6, 6, 192)    576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 6, 6, 192)    576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 6, 6, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 6, 6, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 6, 6, 192)    258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 6, 6, 192)    258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 6, 6, 192)    147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 6, 6, 192)    576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 6, 6, 192)    576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 6, 6, 192)    576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 6, 6, 192)    576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 6, 6, 768)    0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 6, 6, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 6, 6, 192)    576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 6, 6, 192)    258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 6, 6, 192)    576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 6, 6, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 6, 6, 192)    258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 6, 6, 192)    576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 6, 6, 192)    576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 6, 6, 192)    0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 2, 2, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 2, 2, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 2, 2, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 2, 2, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 2, 2, 320)    0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 2, 2, 192)    0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 2, 2, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 2, 2, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 2, 2, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 2, 2, 448)    0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 2, 2, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 2, 2, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 2, 2, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 2, 2, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 2, 2, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 2, 2, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 2, 2, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 2, 2, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 2, 2, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 2, 2, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 2, 2, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 2, 2, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 2, 2, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 2, 2, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 2, 2, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 2, 2, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 2, 2, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 2, 2, 320)    0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 2, 2, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 2, 2, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 2, 2, 192)    0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 2, 2, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 2, 2, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 2, 2, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 2, 2, 448)    0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 2, 2, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 2, 2, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 2, 2, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 2, 2, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 2, 2, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 2, 2, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 2, 2, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 2, 2, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 2, 2, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 2, 2, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 2, 2, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 2, 2, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 2, 2, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 2, 2, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 2, 2, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 2, 2, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 2, 2, 384)    0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 2, 2, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 2, 2, 320)    0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 2, 2, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 2, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 2, 2, 192)    0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 2, 2, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug5cctAVZTEo",
        "colab_type": "code",
        "outputId": "4c519cd9-27d2-445a-d833-524c5126c0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "### Section evaluating the different ML classifer\n",
        "\n",
        "print(\"### CNN Model: InceptionV3 \")\n",
        "print(\"  Accuracy Evaluation of different ML classifier\")\n",
        "\n",
        "\n",
        "## Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "model = RandomForestClassifier(700)\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Random Forest:\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "\n",
        "## Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Decision Tree:\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "\n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=9, p=3)\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     KNN          :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Naive Bayes  :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "model = SVC(gamma=\"auto\")\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     SVM          :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CNN Model: InceptionV3 \n",
            "  Accuracy Evaluation of different ML classifier\n",
            "     Random Forest: 0.8088235294117647\n",
            "     Decision Tree: 0.45\n",
            "     KNN          : 0.638235294117647\n",
            "     Naive Bayes  : 0.6794117647058824\n",
            "     SVM          : 0.8088235294117647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwKjGat83h02",
        "colab_type": "text"
      },
      "source": [
        "##3) CNN Model ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRK3_GptIwOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6387
        },
        "outputId": "5904b6bc-b1bb-4bde-9c72-d04a4ed943a5"
      },
      "source": [
        "### CNN model: ResNet50\n",
        "\n",
        "# Importation of pre trained CNN ResNet50\n",
        "ResNet50Model= tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "print (ResNet50Model.summary())\n",
        "\n",
        "# A prediction is made on training data\n",
        "featuresTrain= ResNet50Model.predict(trainX)\n",
        "\n",
        "#reshape to flatten feature data\n",
        "featuresTrain= featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "# A prediction is made on test data\n",
        "featuresVal = ResNet50Model.predict(testX)\n",
        "\n",
        "#reshape to flatten feature data\n",
        "featuresVal= featuresVal.reshape(featuresVal.shape[0], -1)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 2s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalizationV1) (None, 64, 64, 64)   256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 64, 64, 64)   0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 32, 32, 64)   4160        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 32, 32, 64)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 32, 32, 64)   0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 32, 32, 256)  16640       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 32, 32, 256)  1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 256)  0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 32, 32, 256)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 32, 32, 64)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 32, 32, 64)   0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 256)  0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 32, 32, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 32, 32, 64)   0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 32, 32, 64)   0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 256)  0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 32, 32, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 16, 16, 128)  32896       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 128)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 128)  0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 16, 16, 512)  131584      activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 16, 16, 512)  2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 512)  0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 512)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 128)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 128)  0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 512)  0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 16, 16, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 16, 16, 128)  0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 16, 16, 128)  0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 512)  0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 16, 16, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 16, 16, 128)  0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 16, 16, 128)  0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 512)  0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 16, 16, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 8, 8, 256)    131328      activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 8, 8, 256)    0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 8, 8, 256)    0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 8, 8, 1024)   525312      activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 8, 8, 1024)   4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 1024)   0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 8, 8, 1024)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 8, 8, 256)    0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 8, 8, 256)    0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 1024)   0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 8, 8, 1024)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 8, 8, 256)    0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 8, 8, 256)    0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 1024)   0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 8, 8, 1024)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 8, 8, 256)    0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 8, 8, 256)    0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 8, 8, 1024)   0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 8, 8, 1024)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 8, 8, 256)    0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 8, 8, 256)    0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 1024)   0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 8, 8, 1024)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 256)    0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 256)    0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 8, 8, 1024)   0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 1024)   0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 4, 4, 512)    524800      activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 4, 4, 512)    0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 4, 4, 512)    0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 4, 4, 2048)   2099200     activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 4, 4, 2048)   8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 4, 4, 2048)   0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 4, 4, 2048)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 4, 4, 512)    0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 4, 4, 512)    0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 4, 4, 2048)   0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 4, 4, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 4, 4, 512)    0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 4, 4, 512)    0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 4, 4, 2048)   0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 4, 4, 2048)   0           add_15[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2mqdkmNI_QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "db5e4881-94a9-486c-dfde-e29ca86c5a51"
      },
      "source": [
        "### Section evaluating the different ML classifer\n",
        "\n",
        "print(\"### CNN Model: ResNet50 \")\n",
        "print(\"  Accuracy Evaluation of different ML classifier\")\n",
        "\n",
        "\n",
        "## Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "model = RandomForestClassifier(700)\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Random Forest:\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "\n",
        "## Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Decision Tree:\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "\n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=9, p=3)\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     KNN          :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     Naive Bayes  :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n",
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "model = SVC(gamma=\"auto\")\n",
        "model.fit(featuresTrain, trainY)\n",
        "\n",
        "# evaluate the model\n",
        "results = model.predict(featuresVal)\n",
        "print (\"     SVM          :\", metrics.accuracy_score(results, testY))\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### CNN Model: ResNet50 \n",
            "  Accuracy Evaluation of different ML classifier\n",
            "     Random Forest: 0.47058823529411764\n",
            "     Decision Tree: 0.2088235294117647\n",
            "     KNN          : 0.3029411764705882\n",
            "     Naive Bayes  : 0.20294117647058824\n",
            "     SVM          : 0.04411764705882353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQkxmZU_qR6q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\n",
        "#     PART B Section II       # \n",
        "\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEmK7iy_3ria",
        "colab_type": "text"
      },
      "source": [
        "##1) Experiment1: CNN model VGG16 with SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNEDJWktqU6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "2666071b-1d89-4ea8-81aa-6b3934566c1c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the ImageNet VGG model. \n",
        "# extra conv layers, dropout and softmax activation is added \n",
        "\n",
        "vggModel= tf.keras.applications.VGG16( weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "vggModel.trainable= False\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(vggModel)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(17, activation='softmax'))\n",
        "\n",
        "print (model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 17)                4369      \n",
            "=================================================================\n",
            "Total params: 16,816,465\n",
            "Trainable params: 2,101,777\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOTLUWmSqst1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "31dcb541-4656-4f96-b74a-4d3dd9302a27"
      },
      "source": [
        "# The built model is compiled with SGD optimizer\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# The model is trained\n",
        "H =model.fit(trainX, trainY, epochs=12, batch_size=51, validation_data=(testX, testY))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1020 samples, validate on 340 samples\n",
            "Epoch 1/12\n",
            "1020/1020 [==============================] - 8s 7ms/sample - loss: 2.8599 - acc: 0.1304 - val_loss: 2.4874 - val_acc: 0.3176\n",
            "Epoch 2/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 2.4665 - acc: 0.2196 - val_loss: 2.1890 - val_acc: 0.3676\n",
            "Epoch 3/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 2.2002 - acc: 0.3284 - val_loss: 1.8838 - val_acc: 0.5706\n",
            "Epoch 4/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.9739 - acc: 0.3990 - val_loss: 1.7334 - val_acc: 0.5353\n",
            "Epoch 5/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.7838 - acc: 0.4539 - val_loss: 1.4648 - val_acc: 0.6676\n",
            "Epoch 6/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.6250 - acc: 0.4990 - val_loss: 1.3762 - val_acc: 0.6971\n",
            "Epoch 7/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.5200 - acc: 0.5373 - val_loss: 1.2268 - val_acc: 0.7118\n",
            "Epoch 8/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.3440 - acc: 0.6118 - val_loss: 1.1064 - val_acc: 0.7382\n",
            "Epoch 9/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.2499 - acc: 0.6294 - val_loss: 1.0883 - val_acc: 0.7412\n",
            "Epoch 10/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.2199 - acc: 0.6382 - val_loss: 1.0231 - val_acc: 0.7618\n",
            "Epoch 11/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.1170 - acc: 0.6725 - val_loss: 0.9294 - val_acc: 0.7882\n",
            "Epoch 12/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.0900 - acc: 0.6882 - val_loss: 0.9171 - val_acc: 0.7971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV9VGKCnvTTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5481f93-c112-494f-fd33-a1dbed9344d6"
      },
      "source": [
        "# Prediction on test data\n",
        "result_predictions = model.predict(testX)\n",
        "# select the class with the highest value\n",
        "predictions = np.argmax(result_predictions, axis=1)\n",
        "# Check if the model prediction is correct (True if prediction correct, False otherwise)\n",
        "correct = np.equal(predictions, testY)\n",
        "# Conversion of the boolean array into a numerical array (1 if True, 0 if False)\n",
        "pred_correct = correct.astype(np.float32)\n",
        "# mean value of predictions_correct\n",
        "accuracy = np.mean(pred_correct)\n",
        "print(\"CNN model VGG16, SGD optimizer accuracy:\",accuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN model VGG16, SGD optimizer accuracy: 0.7970588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m81JIBw353g",
        "colab_type": "text"
      },
      "source": [
        "##2) Experiment2: CNN model VGG16 with Nadam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_u8C5Prhht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "c88fde5b-5908-4fb2-8faf-088dc459bae7"
      },
      "source": [
        "# The model is now compiled with Nadam optimizer\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer=tf.keras.optimizers.Nadam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# The model is trained\n",
        "H =model.fit(trainX, trainY, epochs=12, batch_size=51, validation_data=(testX, testY))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1020 samples, validate on 340 samples\n",
            "Epoch 1/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 1.2421 - acc: 0.6245 - val_loss: 0.6602 - val_acc: 0.8324\n",
            "Epoch 2/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.6391 - acc: 0.8049 - val_loss: 0.5196 - val_acc: 0.8765\n",
            "Epoch 3/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.3733 - acc: 0.8902 - val_loss: 0.4505 - val_acc: 0.8765\n",
            "Epoch 4/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.2295 - acc: 0.9402 - val_loss: 0.4263 - val_acc: 0.8588\n",
            "Epoch 5/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.1827 - acc: 0.9520 - val_loss: 0.3815 - val_acc: 0.8706\n",
            "Epoch 6/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.1322 - acc: 0.9735 - val_loss: 0.4494 - val_acc: 0.8559\n",
            "Epoch 7/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.0913 - acc: 0.9873 - val_loss: 0.3651 - val_acc: 0.8853\n",
            "Epoch 8/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.0708 - acc: 0.9912 - val_loss: 0.3355 - val_acc: 0.8971\n",
            "Epoch 9/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.0615 - acc: 0.9922 - val_loss: 0.3893 - val_acc: 0.8706\n",
            "Epoch 10/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.0642 - acc: 0.9824 - val_loss: 0.4223 - val_acc: 0.8647\n",
            "Epoch 11/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.0433 - acc: 0.9980 - val_loss: 0.3511 - val_acc: 0.8912\n",
            "Epoch 12/12\n",
            "1020/1020 [==============================] - 3s 3ms/sample - loss: 0.0368 - acc: 0.9971 - val_loss: 0.3673 - val_acc: 0.8912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX09X0NYvz76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b290caca-5985-4575-8867-fd622a4da284"
      },
      "source": [
        "# Prediction on test data\n",
        "result_predictions = model.predict(testX)\n",
        "# select the class with the highest value\n",
        "predictions = np.argmax(result_predictions, axis=1)\n",
        "# Check if the model prediction is correct (True if prediction correct, False otherwise)\n",
        "correct = np.equal(predictions, testY)\n",
        "# Conversion of the boolean array into a numerical array (1 if True, 0 if False)\n",
        "pred_correct = correct.astype(np.float32)\n",
        "# mean value of predictions_correct\n",
        "accuracy = np.mean(pred_correct)\n",
        "print(\"CNN model VGG16, Nadam optimizer:\",accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN model VGG16, Nadam optimizer: 0.89117646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DUONLHl3_Hy",
        "colab_type": "text"
      },
      "source": [
        "##3) Experiment3: VGG16 Fine-tuning from block4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU3NmQNxr4LJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "3759df21-4bae-4562-df25-b2c44bbcb880"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 12), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 12), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 12), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 12), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIIQQQgQlASGEECIoCQghhBBB/X/HO09IXnwF7gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYgv1Pa4sLMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2050
        },
        "outputId": "4f4fc537-89e2-48fd-cd37-020eda511144"
      },
      "source": [
        "### In this section we start with the fine-tuning\n",
        "\n",
        "# the VGG16 is now unfrozen and is ready to be trained \n",
        "vggModel.trainable= True\n",
        "\n",
        "# Initilize the trainable flag to false\n",
        "trainableFlag= False\n",
        "\n",
        "# a loop goes through each layer of VGG16\n",
        "for layer in vggModel.layers:\n",
        "  # It checks if the layer name is block4_conv1\n",
        "  if layer.name== 'block4_conv1':\n",
        "    # when the specified layer name is found, the trainable flag is set to True\n",
        "    # This means that all layers from the specified layer could be trained\n",
        "    trainableFlag= True\n",
        "  layer.trainable= trainableFlag\n",
        "\n",
        "# The model is compiled with the Nadam optimizer with a very low learning rate  \n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.keras.optimizers.Nadam(lr=1e-5),metrics=['accuracy'])\n",
        "\n",
        "# The model is trained\n",
        "H =model.fit(trainX, trainY, epochs=60, batch_size=51, validation_data=(testX, testY))\n",
        "    \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1020 samples, validate on 340 samples\n",
            "Epoch 1/60\n",
            "1020/1020 [==============================] - 5s 5ms/sample - loss: 0.0178 - acc: 0.9980 - val_loss: 0.3691 - val_acc: 0.9029\n",
            "Epoch 2/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0163 - acc: 0.9971 - val_loss: 0.3937 - val_acc: 0.8941\n",
            "Epoch 3/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0095 - acc: 0.9990 - val_loss: 0.3935 - val_acc: 0.9000\n",
            "Epoch 4/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0074 - acc: 0.9980 - val_loss: 0.4165 - val_acc: 0.8765\n",
            "Epoch 5/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0118 - acc: 0.9971 - val_loss: 0.4935 - val_acc: 0.8735\n",
            "Epoch 6/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0170 - acc: 0.9951 - val_loss: 0.4153 - val_acc: 0.8765\n",
            "Epoch 7/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0110 - acc: 0.9971 - val_loss: 0.3822 - val_acc: 0.9000\n",
            "Epoch 8/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3624 - val_acc: 0.9029\n",
            "Epoch 9/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0042 - acc: 0.9980 - val_loss: 0.3855 - val_acc: 0.8941\n",
            "Epoch 10/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3811 - val_acc: 0.9029\n",
            "Epoch 11/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.3615 - val_acc: 0.9000\n",
            "Epoch 12/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.3948 - val_acc: 0.9088\n",
            "Epoch 13/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3631 - val_acc: 0.9176\n",
            "Epoch 14/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.6429e-04 - acc: 1.0000 - val_loss: 0.3735 - val_acc: 0.9059\n",
            "Epoch 15/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3630 - val_acc: 0.9029\n",
            "Epoch 16/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 0.4336 - val_acc: 0.9029\n",
            "Epoch 17/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3906 - val_acc: 0.9029\n",
            "Epoch 18/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4205 - val_acc: 0.9059\n",
            "Epoch 19/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0027 - acc: 0.9980 - val_loss: 0.3956 - val_acc: 0.9029\n",
            "Epoch 20/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0064 - acc: 0.9980 - val_loss: 0.5019 - val_acc: 0.8971\n",
            "Epoch 21/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4476 - val_acc: 0.8971\n",
            "Epoch 22/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4270 - val_acc: 0.8971\n",
            "Epoch 23/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0036 - acc: 0.9990 - val_loss: 0.4948 - val_acc: 0.8882\n",
            "Epoch 24/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0113 - acc: 0.9951 - val_loss: 0.4660 - val_acc: 0.9059\n",
            "Epoch 25/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0080 - acc: 0.9961 - val_loss: 0.5752 - val_acc: 0.8618\n",
            "Epoch 26/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 0.4331 - val_acc: 0.8882\n",
            "Epoch 27/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0055 - acc: 0.9980 - val_loss: 0.3602 - val_acc: 0.8941\n",
            "Epoch 28/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3935 - val_acc: 0.8912\n",
            "Epoch 29/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3749 - val_acc: 0.9118\n",
            "Epoch 30/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3890 - val_acc: 0.9088\n",
            "Epoch 31/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0064 - acc: 0.9971 - val_loss: 0.4638 - val_acc: 0.9088\n",
            "Epoch 32/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4433 - val_acc: 0.9088\n",
            "Epoch 33/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.5227 - val_acc: 0.8912\n",
            "Epoch 34/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4706 - val_acc: 0.8971\n",
            "Epoch 35/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4108 - val_acc: 0.9088\n",
            "Epoch 36/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 8.1550e-04 - acc: 1.0000 - val_loss: 0.4417 - val_acc: 0.9088\n",
            "Epoch 37/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0017 - acc: 0.9990 - val_loss: 0.5000 - val_acc: 0.9000\n",
            "Epoch 38/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0093 - acc: 0.9961 - val_loss: 0.4089 - val_acc: 0.9059\n",
            "Epoch 39/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4089 - val_acc: 0.9000\n",
            "Epoch 40/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.3047e-04 - acc: 1.0000 - val_loss: 0.4260 - val_acc: 0.8941\n",
            "Epoch 41/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 0.9176\n",
            "Epoch 42/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.0044e-04 - acc: 1.0000 - val_loss: 0.3867 - val_acc: 0.9147\n",
            "Epoch 43/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3914 - val_acc: 0.9029\n",
            "Epoch 44/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3770 - val_acc: 0.9088\n",
            "Epoch 45/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 7.1243e-04 - acc: 1.0000 - val_loss: 0.3765 - val_acc: 0.9088\n",
            "Epoch 46/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.2091e-04 - acc: 1.0000 - val_loss: 0.3836 - val_acc: 0.9088\n",
            "Epoch 47/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3915 - val_acc: 0.9029\n",
            "Epoch 48/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.0090e-04 - acc: 1.0000 - val_loss: 0.4132 - val_acc: 0.9088\n",
            "Epoch 49/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.1207e-04 - acc: 1.0000 - val_loss: 0.4054 - val_acc: 0.9059\n",
            "Epoch 50/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4016 - val_acc: 0.9088\n",
            "Epoch 51/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 7.7917e-04 - acc: 1.0000 - val_loss: 0.4332 - val_acc: 0.9118\n",
            "Epoch 52/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4637 - val_acc: 0.9088\n",
            "Epoch 53/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.4693e-04 - acc: 1.0000 - val_loss: 0.4631 - val_acc: 0.9088\n",
            "Epoch 54/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.1865e-04 - acc: 1.0000 - val_loss: 0.4627 - val_acc: 0.9059\n",
            "Epoch 55/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0017 - acc: 0.9990 - val_loss: 0.4421 - val_acc: 0.9029\n",
            "Epoch 56/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.1387e-04 - acc: 1.0000 - val_loss: 0.4206 - val_acc: 0.9088\n",
            "Epoch 57/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.7137e-04 - acc: 1.0000 - val_loss: 0.4157 - val_acc: 0.9147\n",
            "Epoch 58/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.3266e-04 - acc: 1.0000 - val_loss: 0.4210 - val_acc: 0.9118\n",
            "Epoch 59/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.0978e-04 - acc: 1.0000 - val_loss: 0.4209 - val_acc: 0.9088\n",
            "Epoch 60/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.8860e-04 - acc: 1.0000 - val_loss: 0.4213 - val_acc: 0.9059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEjVfZh3sQ4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "2ecf644a-3231-4c94-e434-8a8b9b25930f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 60), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjbfMkx4Ipy",
        "colab_type": "text"
      },
      "source": [
        "##4) Experiment4: VGG16 Fine-tuning from block3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYrInXB8y_DN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2050
        },
        "outputId": "26ff0837-855c-4f7e-e098-6a554a774faa"
      },
      "source": [
        "### In this section we start with the fine-tuning\n",
        "\n",
        "# the VGG16 is now unfrozen and is ready to be trained \n",
        "vggModel.trainable= True\n",
        "\n",
        "# Initilize the trainable flag to false\n",
        "trainableFlag= False\n",
        "\n",
        "# a loop goes through each layer of VGG16\n",
        "for layer in vggModel.layers:\n",
        "  # It checks if the layer name is block3_conv1\n",
        "  if layer.name== 'block3_conv1':\n",
        "    # when the specified layer name is found, the trainable flag is set to True\n",
        "    # This means that all layers from the specified layer could be trained\n",
        "    trainableFlag= True\n",
        "  layer.trainable= trainableFlag\n",
        "\n",
        "# The model is compiled with the Nadam optimizer with a very low learning rate  \n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.keras.optimizers.Nadam(lr=1e-5),metrics=['accuracy'])\n",
        "\n",
        "# The model is trained\n",
        "H =model.fit(trainX, trainY, epochs=60, batch_size=51, validation_data=(testX, testY))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1020 samples, validate on 340 samples\n",
            "Epoch 1/60\n",
            "1020/1020 [==============================] - 6s 6ms/sample - loss: 0.0019 - acc: 0.9990 - val_loss: 0.5420 - val_acc: 0.8971\n",
            "Epoch 2/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4476 - val_acc: 0.9147\n",
            "Epoch 3/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0142 - acc: 0.9961 - val_loss: 0.5482 - val_acc: 0.8853\n",
            "Epoch 4/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.4417 - val_acc: 0.9000\n",
            "Epoch 5/60\n",
            "1020/1020 [==============================] - 5s 4ms/sample - loss: 0.0045 - acc: 0.9980 - val_loss: 0.3933 - val_acc: 0.9206\n",
            "Epoch 6/60\n",
            "1020/1020 [==============================] - 5s 4ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4531 - val_acc: 0.9176\n",
            "Epoch 7/60\n",
            "1020/1020 [==============================] - 5s 4ms/sample - loss: 5.7153e-04 - acc: 1.0000 - val_loss: 0.4804 - val_acc: 0.9118\n",
            "Epoch 8/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.5585 - val_acc: 0.9000\n",
            "Epoch 9/60\n",
            "1020/1020 [==============================] - 5s 4ms/sample - loss: 8.4551e-04 - acc: 1.0000 - val_loss: 0.5768 - val_acc: 0.8971\n",
            "Epoch 10/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 7.0184e-04 - acc: 1.0000 - val_loss: 0.5617 - val_acc: 0.8971\n",
            "Epoch 11/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.9626e-04 - acc: 1.0000 - val_loss: 0.5186 - val_acc: 0.9029\n",
            "Epoch 12/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.9296e-04 - acc: 1.0000 - val_loss: 0.4773 - val_acc: 0.9000\n",
            "Epoch 13/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 7.1439e-04 - acc: 1.0000 - val_loss: 0.4897 - val_acc: 0.9059\n",
            "Epoch 14/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 7.9234e-04 - acc: 1.0000 - val_loss: 0.6002 - val_acc: 0.8853\n",
            "Epoch 15/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 8.5434e-04 - acc: 1.0000 - val_loss: 0.5271 - val_acc: 0.9059\n",
            "Epoch 16/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.5328 - val_acc: 0.9059\n",
            "Epoch 17/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 0.4641 - val_acc: 0.9059\n",
            "Epoch 18/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.9588e-04 - acc: 1.0000 - val_loss: 0.4529 - val_acc: 0.9118\n",
            "Epoch 19/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.9024e-04 - acc: 1.0000 - val_loss: 0.4338 - val_acc: 0.9147\n",
            "Epoch 20/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.9685e-04 - acc: 1.0000 - val_loss: 0.4462 - val_acc: 0.9147\n",
            "Epoch 21/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0023 - acc: 0.9990 - val_loss: 0.4045 - val_acc: 0.9147\n",
            "Epoch 22/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.5695e-04 - acc: 1.0000 - val_loss: 0.4222 - val_acc: 0.9206\n",
            "Epoch 23/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.1429e-04 - acc: 1.0000 - val_loss: 0.4285 - val_acc: 0.9265\n",
            "Epoch 24/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.8094e-04 - acc: 1.0000 - val_loss: 0.4250 - val_acc: 0.9176\n",
            "Epoch 25/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.9145e-04 - acc: 1.0000 - val_loss: 0.4430 - val_acc: 0.9206\n",
            "Epoch 26/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.3997e-04 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 0.9265\n",
            "Epoch 27/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.6860e-04 - acc: 1.0000 - val_loss: 0.4743 - val_acc: 0.8971\n",
            "Epoch 28/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.4479 - val_acc: 0.9118\n",
            "Epoch 29/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0020 - acc: 0.9990 - val_loss: 0.5795 - val_acc: 0.8971\n",
            "Epoch 30/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0064 - acc: 0.9971 - val_loss: 0.5070 - val_acc: 0.9029\n",
            "Epoch 31/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0130 - acc: 0.9971 - val_loss: 0.4272 - val_acc: 0.9000\n",
            "Epoch 32/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4541 - val_acc: 0.8941\n",
            "Epoch 33/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3968 - val_acc: 0.9118\n",
            "Epoch 34/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 6.8010e-04 - acc: 1.0000 - val_loss: 0.4047 - val_acc: 0.9118\n",
            "Epoch 35/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.8774e-04 - acc: 1.0000 - val_loss: 0.3985 - val_acc: 0.9235\n",
            "Epoch 36/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.6594e-04 - acc: 1.0000 - val_loss: 0.4252 - val_acc: 0.9235\n",
            "Epoch 37/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.5134e-04 - acc: 1.0000 - val_loss: 0.4341 - val_acc: 0.9176\n",
            "Epoch 38/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.6873e-04 - acc: 1.0000 - val_loss: 0.4442 - val_acc: 0.9206\n",
            "Epoch 39/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.0478e-04 - acc: 1.0000 - val_loss: 0.4451 - val_acc: 0.9176\n",
            "Epoch 40/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.5088e-04 - acc: 1.0000 - val_loss: 0.4516 - val_acc: 0.9147\n",
            "Epoch 41/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 8.6668e-04 - acc: 1.0000 - val_loss: 0.4523 - val_acc: 0.9206\n",
            "Epoch 42/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.4308e-04 - acc: 1.0000 - val_loss: 0.4657 - val_acc: 0.9118\n",
            "Epoch 43/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.9773e-04 - acc: 1.0000 - val_loss: 0.4575 - val_acc: 0.9118\n",
            "Epoch 44/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 4.8488e-04 - acc: 1.0000 - val_loss: 0.4328 - val_acc: 0.9265\n",
            "Epoch 45/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.4929e-05 - acc: 1.0000 - val_loss: 0.4345 - val_acc: 0.9235\n",
            "Epoch 46/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.9106e-04 - acc: 1.0000 - val_loss: 0.4560 - val_acc: 0.9147\n",
            "Epoch 47/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.4702e-04 - acc: 1.0000 - val_loss: 0.4681 - val_acc: 0.9118\n",
            "Epoch 48/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 3.5758e-04 - acc: 1.0000 - val_loss: 0.4651 - val_acc: 0.9118\n",
            "Epoch 49/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.9651e-05 - acc: 1.0000 - val_loss: 0.4647 - val_acc: 0.9147\n",
            "Epoch 50/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.6831e-04 - acc: 1.0000 - val_loss: 0.4726 - val_acc: 0.9088\n",
            "Epoch 51/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 5.3893e-04 - acc: 1.0000 - val_loss: 0.5471 - val_acc: 0.9029\n",
            "Epoch 52/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.7191e-04 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.9059\n",
            "Epoch 53/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.9488e-04 - acc: 1.0000 - val_loss: 0.4606 - val_acc: 0.9147\n",
            "Epoch 54/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.9142e-05 - acc: 1.0000 - val_loss: 0.4595 - val_acc: 0.9176\n",
            "Epoch 55/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 2.5225e-04 - acc: 1.0000 - val_loss: 0.4616 - val_acc: 0.9176\n",
            "Epoch 56/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.8717e-04 - acc: 1.0000 - val_loss: 0.4624 - val_acc: 0.9147\n",
            "Epoch 57/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.0841e-05 - acc: 1.0000 - val_loss: 0.4788 - val_acc: 0.9118\n",
            "Epoch 58/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 9.8271e-05 - acc: 1.0000 - val_loss: 0.4874 - val_acc: 0.9118\n",
            "Epoch 59/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 7.4729e-05 - acc: 1.0000 - val_loss: 0.4669 - val_acc: 0.9206\n",
            "Epoch 60/60\n",
            "1020/1020 [==============================] - 4s 4ms/sample - loss: 1.1294e-04 - acc: 1.0000 - val_loss: 0.4595 - val_acc: 0.9265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qesQzy9-wsGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "27ecf00a-fd1d-4a4d-ea2a-091b4d612e5b"
      },
      "source": [
        "# Prediction on test data\n",
        "result_predictions = model.predict(testX)\n",
        "# select the class with the highest value\n",
        "predictions = np.argmax(result_predictions, axis=1)\n",
        "# Check if the model prediction is correct (True if prediction correct, False otherwise)\n",
        "correct = np.equal(predictions, testY)\n",
        "# Conversion of the boolean array into a numerical array (1 if True, 0 if False)\n",
        "pred_correct = correct.astype(np.float32)\n",
        "# mean value of predictions_correct\n",
        "accuracy = np.mean(pred_correct)\n",
        "print(\"CNN model VGG16, Nadam optimizer:\")\n",
        "print(\"  Fine-tuning using Nadam optimiser from block3_conv1\")\n",
        "print(\"  Accuracy after fine-tuning:\", accuracy)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN model VGG16, Nadam optimizer:\n",
            "  Fine-tuning using Nadam optimiser from block3_conv1\n",
            "  Accuracy after fine-tuning: 0.9264706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWnhZ9Bn4Pej",
        "colab_type": "text"
      },
      "source": [
        "##5) Experiment5: CNN model InceptionV3 with Nadam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QT7IBPFsbcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "1d3842a2-e864-4523-cbbd-fffb663463af"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the ImageNet VGG model. \n",
        "# extra conv layers, dropout and softmax activation is added \n",
        "\n",
        "InceptionV3Model= tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "InceptionV3Model.trainable= False\n",
        "modelinc = tf.keras.models.Sequential()\n",
        "modelinc.add(InceptionV3Model)\n",
        "modelinc.add(tf.keras.layers.Flatten())\n",
        "modelinc.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "modelinc.add(tf.keras.layers.Dropout(0.5))\n",
        "modelinc.add(tf.keras.layers.Dense(17, activation='softmax'))\n",
        "\n",
        "print (modelinc.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 2, 2, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 17)                4369      \n",
            "=================================================================\n",
            "Total params: 23,904,561\n",
            "Trainable params: 2,101,777\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhWrqYYbsoVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1714
        },
        "outputId": "e6dd9b55-25d1-4b99-b2d9-db700908cb6d"
      },
      "source": [
        "# The built model is compiled with Nadam optimizer\n",
        "modelinc.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer=tf.keras.optimizers.Nadam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# The model is trained\n",
        "H =modelinc.fit(trainX, trainY, epochs=50, batch_size=51, validation_data=(testX, testY))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1020 samples, validate on 340 samples\n",
            "Epoch 1/50\n",
            "1020/1020 [==============================] - 6s 6ms/sample - loss: 3.1961 - acc: 0.1157 - val_loss: 2.8713 - val_acc: 0.1353\n",
            "Epoch 2/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 2.4405 - acc: 0.2578 - val_loss: 2.4652 - val_acc: 0.3147\n",
            "Epoch 3/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 2.1533 - acc: 0.3441 - val_loss: 1.8106 - val_acc: 0.4588\n",
            "Epoch 4/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 1.7895 - acc: 0.4637 - val_loss: 1.6322 - val_acc: 0.5088\n",
            "Epoch 5/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 1.4976 - acc: 0.5490 - val_loss: 1.4198 - val_acc: 0.5412\n",
            "Epoch 6/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 1.3424 - acc: 0.5902 - val_loss: 1.1416 - val_acc: 0.6529\n",
            "Epoch 7/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 1.2011 - acc: 0.6284 - val_loss: 1.3930 - val_acc: 0.6029\n",
            "Epoch 8/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 1.0983 - acc: 0.6588 - val_loss: 1.3052 - val_acc: 0.6118\n",
            "Epoch 9/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 1.0982 - acc: 0.6696 - val_loss: 1.1432 - val_acc: 0.6912\n",
            "Epoch 10/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.9486 - acc: 0.7059 - val_loss: 1.2676 - val_acc: 0.6794\n",
            "Epoch 11/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.8864 - acc: 0.7294 - val_loss: 1.2869 - val_acc: 0.6441\n",
            "Epoch 12/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.8307 - acc: 0.7402 - val_loss: 1.3148 - val_acc: 0.6412\n",
            "Epoch 13/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.8310 - acc: 0.7549 - val_loss: 1.2318 - val_acc: 0.6676\n",
            "Epoch 14/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.6891 - acc: 0.7804 - val_loss: 1.1954 - val_acc: 0.6824\n",
            "Epoch 15/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.6721 - acc: 0.7892 - val_loss: 1.2445 - val_acc: 0.6882\n",
            "Epoch 16/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.7275 - acc: 0.7882 - val_loss: 1.3964 - val_acc: 0.6735\n",
            "Epoch 17/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.6470 - acc: 0.8020 - val_loss: 1.3144 - val_acc: 0.6765\n",
            "Epoch 18/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.6448 - acc: 0.8039 - val_loss: 1.3046 - val_acc: 0.6853\n",
            "Epoch 19/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.6281 - acc: 0.8069 - val_loss: 1.4200 - val_acc: 0.6794\n",
            "Epoch 20/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.5421 - acc: 0.8196 - val_loss: 1.3660 - val_acc: 0.6529\n",
            "Epoch 21/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.5112 - acc: 0.8480 - val_loss: 1.4269 - val_acc: 0.6706\n",
            "Epoch 22/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.5742 - acc: 0.8196 - val_loss: 1.4818 - val_acc: 0.6529\n",
            "Epoch 23/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.5277 - acc: 0.8294 - val_loss: 1.4451 - val_acc: 0.6559\n",
            "Epoch 24/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4945 - acc: 0.8314 - val_loss: 1.3301 - val_acc: 0.6735\n",
            "Epoch 25/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.5464 - acc: 0.8353 - val_loss: 1.4622 - val_acc: 0.6471\n",
            "Epoch 26/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4598 - acc: 0.8588 - val_loss: 1.4141 - val_acc: 0.6676\n",
            "Epoch 27/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3969 - acc: 0.8716 - val_loss: 1.3580 - val_acc: 0.6794\n",
            "Epoch 28/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4130 - acc: 0.8647 - val_loss: 1.4515 - val_acc: 0.6706\n",
            "Epoch 29/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4318 - acc: 0.8529 - val_loss: 1.4543 - val_acc: 0.6647\n",
            "Epoch 30/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4603 - acc: 0.8412 - val_loss: 1.4233 - val_acc: 0.6765\n",
            "Epoch 31/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4603 - acc: 0.8539 - val_loss: 1.3431 - val_acc: 0.6794\n",
            "Epoch 32/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4148 - acc: 0.8578 - val_loss: 1.4442 - val_acc: 0.6647\n",
            "Epoch 33/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3760 - acc: 0.8735 - val_loss: 1.4458 - val_acc: 0.6618\n",
            "Epoch 34/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3853 - acc: 0.8775 - val_loss: 1.6101 - val_acc: 0.6529\n",
            "Epoch 35/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3881 - acc: 0.8784 - val_loss: 1.4684 - val_acc: 0.6794\n",
            "Epoch 36/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4001 - acc: 0.8804 - val_loss: 1.5343 - val_acc: 0.6853\n",
            "Epoch 37/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.4058 - acc: 0.8755 - val_loss: 1.5312 - val_acc: 0.6941\n",
            "Epoch 38/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3963 - acc: 0.8784 - val_loss: 1.4696 - val_acc: 0.6882\n",
            "Epoch 39/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3405 - acc: 0.8863 - val_loss: 1.7117 - val_acc: 0.6500\n",
            "Epoch 40/50\n",
            "1020/1020 [==============================] - 2s 1ms/sample - loss: 0.4138 - acc: 0.8696 - val_loss: 1.4135 - val_acc: 0.6588\n",
            "Epoch 41/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3288 - acc: 0.8951 - val_loss: 1.6363 - val_acc: 0.6618\n",
            "Epoch 42/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3236 - acc: 0.8971 - val_loss: 1.5250 - val_acc: 0.6618\n",
            "Epoch 43/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3478 - acc: 0.8853 - val_loss: 1.4969 - val_acc: 0.6912\n",
            "Epoch 44/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3232 - acc: 0.8971 - val_loss: 1.4595 - val_acc: 0.6912\n",
            "Epoch 45/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.2802 - acc: 0.9147 - val_loss: 1.4824 - val_acc: 0.7000\n",
            "Epoch 46/50\n",
            "1020/1020 [==============================] - 2s 1ms/sample - loss: 0.3403 - acc: 0.8892 - val_loss: 1.5352 - val_acc: 0.6853\n",
            "Epoch 47/50\n",
            "1020/1020 [==============================] - 2s 1ms/sample - loss: 0.3001 - acc: 0.8990 - val_loss: 1.4225 - val_acc: 0.6912\n",
            "Epoch 48/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3114 - acc: 0.8971 - val_loss: 1.5301 - val_acc: 0.6853\n",
            "Epoch 49/50\n",
            "1020/1020 [==============================] - 1s 1ms/sample - loss: 0.3554 - acc: 0.8902 - val_loss: 1.4974 - val_acc: 0.6971\n",
            "Epoch 50/50\n",
            "1020/1020 [==============================] - 2s 1ms/sample - loss: 0.2890 - acc: 0.9118 - val_loss: 1.4239 - val_acc: 0.7176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmglwMvBssde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "b931041b-80cf-4ae2-c288-e5727a02f63c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 50), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 50), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 50), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 50), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_44ic0OxSZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2b623aa-0306-4256-f2a6-fb827738283e"
      },
      "source": [
        "# Prediction on test data\n",
        "result_predictions = modelinc.predict(testX)\n",
        "# select the class with the highest value\n",
        "predictions = np.argmax(result_predictions, axis=1)\n",
        "# Check if the model prediction is correct (True if prediction correct, False otherwise)\n",
        "correct = np.equal(predictions, testY)\n",
        "# Conversion of the boolean array into a numerical array (1 if True, 0 if False)\n",
        "pred_correct = correct.astype(np.float32)\n",
        "# mean value of predictions_correct\n",
        "accuracy = np.mean(pred_correct)\n",
        "print(\"CNN model InceptionV3, Nadam optimizer:\",accuracy)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN model InceptionV3, Nadam optimizer: 0.7176471\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}