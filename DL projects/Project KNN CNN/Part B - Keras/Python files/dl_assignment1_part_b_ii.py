# -*- coding: utf-8 -*-
"""DL_Assignment1_Part_B_II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JWL4rQPM-26nvx1q0p-2GoBvWXTTxtzA

\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
#     PART B Section II       # 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
"""

# Importation of libraries (incuding tensorflow and keras)
import tensorflow as tf
from tensorflow import keras
from sklearn import preprocessing
import h5py

import numpy as np
import matplotlib.pyplot as plt

# read data
from google.colab import drive
drive.mount('/content/gdrive')
!unzip "/content/gdrive/My Drive/Colab Notebooks/data.zip"

# loadData is a function which load the dataset and split into training/test images and labels
def loadData():
  with h5py.File('data.h5','r') as hf:
    print('List of arrays in this file: \n', hf.keys())
    allTrain = hf.get('trainData')
    allTest = hf.get('testData')
    npTrain = np.array(allTrain)
    npTest = np.array(allTest)
    print('Shape of the array dataset_1: \n', npTrain.shape)
    print('Shape of the array dataset_2: \n', npTest.shape)
    return npTrain[:,:-1], npTrain[:, -1], npTest[:,:-1], npTest[:, -1]

# Load the data
x_train, y_train, x_test, y_test = loadData()

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 200 fully-connected neurons with a relu activation 
#   2nd layer of 10 fully-connected neurons with a softmax activation 
model_1 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_1.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 3 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_1 = model_1.fit(x_train, y_train, epochs=3, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_1.evaluate(x_test, y_test)

print("Final Test Loss (model 1): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 1): {:.2f}".format(results[1]))

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 400 fully-connected neurons with a relu activation 
#   2nd layer of 200 fully-connected neurons with a relu activation 
#   3rd layer of 10 fully-connected neurons with a softmax activation 
model_2 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(400, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_2.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 3 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_2 = model_2.fit(x_train, y_train, epochs=3, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_2.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 2): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 2): {:.2f}".format(results[1]))

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 600 fully-connected neurons with a relu activation 
#   2nd layer of 400 fully-connected neurons with a relu activation 
#   3rd layer of 200 fully-connected neurons with a relu activation 
#   4th layer of 10 fully-connected neurons with a softmax activation 
model_3 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(600, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(400, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_3.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 3 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_3 = model_3.fit(x_train, y_train, epochs=3, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_3.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 3): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 3): {:.2f}".format(results[1]))

import matplotlib.pyplot as plt

plt.plot(history_1.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_1.history['val_acc'])
plt.title('Model accuracy M1 = (L1:200n,relu ; L2:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_2.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_2.history['val_acc'])
plt.title('Model accuracy M2 = (L1:400n,relu ; L2:200n,relu ; L3:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_3.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_3.history['val_acc'])
plt.title('Model accuracy M3 = (L1:600n,relu ; L2:400n,relu ; L3:200n,relu ; L4:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

import matplotlib.pyplot as plt
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 3), history_1.history["loss"], label="train_loss")
plt.plot(np.arange(0, 3), history_1.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 3), history_1.history["acc"], label="train_acc")
plt.plot(np.arange(0, 3), history_1.history["val_acc"], label="val_acc")
plt.title("Training/Validation Loss and Accuracy for M1 = (L1:200n,relu ; L2:10n,softmax)")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 3), history_2.history["loss"], label="train_loss")
plt.plot(np.arange(0, 3), history_2.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 3), history_2.history["acc"], label="train_acc")
plt.plot(np.arange(0, 3), history_2.history["val_acc"], label="val_acc")
plt.title("Training/Validation Loss and Accuracy for M2 = (L1:400n,relu ; L2:200n,relu ; L3:10n,softmax)")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 3), history_3.history["loss"], label="train_loss")
plt.plot(np.arange(0, 3), history_3.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 3), history_3.history["acc"], label="train_acc")
plt.plot(np.arange(0, 3), history_3.history["val_acc"], label="val_acc")
plt.title("Training/Validation Loss and Accuracy for M3 = (L1:600n,relu ; L2:400n,relu ; L3:200n,relu ; L4:10n,softmax)")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 3), history_1.history["acc"], label="M1 train_acc")
plt.plot(np.arange(0, 3), history_1.history["val_acc"], label="M1 val_acc")
plt.plot(np.arange(0, 3), history_2.history["acc"], label="M2 train_acc")
plt.plot(np.arange(0, 3), history_2.history["val_acc"], label="M2 val_acc")
plt.plot(np.arange(0, 3), history_3.history["acc"], label="M3 train_acc")
plt.plot(np.arange(0, 3), history_3.history["val_acc"], label="M3 val_acc")
plt.title("Training/Validation Accuracy for M1, M2, M3")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 3), history_1.history["loss"], label="M1 train_loss")
plt.plot(np.arange(0, 3), history_1.history["val_loss"], label="M1 val_loss")
plt.plot(np.arange(0, 3), history_2.history["loss"], label="M2 train_loss")
plt.plot(np.arange(0, 3), history_2.history["val_loss"], label="M2 val_loss")
plt.plot(np.arange(0, 3), history_3.history["loss"], label="M3 train_loss")
plt.plot(np.arange(0, 3), history_3.history["val_loss"], label="M3 val_loss")
plt.title("Training/Validation Loss for M1, M2, M3")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend()