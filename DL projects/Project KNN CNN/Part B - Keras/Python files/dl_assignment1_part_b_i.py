# -*- coding: utf-8 -*-
"""DL_Assignment1_Part_B_I.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QRtxmyBABeaHKvhLh7lurG8ca3wUSLRp

\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
#     PART B Section I       # 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
"""

# Importation of libraries (incuding tensorflow and keras)
import tensorflow as tf
from tensorflow import keras
from sklearn import preprocessing
import h5py

import numpy as np
import matplotlib.pyplot as plt

# read data
from google.colab import drive
drive.mount('/content/gdrive')
!unzip "/content/gdrive/My Drive/Colab Notebooks/data.zip"

# loadData is a function which load the dataset and split into training/test images and labels
def loadData():
  with h5py.File('data.h5','r') as hf:
    print('List of arrays in this file: \n', hf.keys())
    allTrain = hf.get('trainData')
    allTest = hf.get('testData')
    npTrain = np.array(allTrain)
    npTest = np.array(allTest)
    print('Shape of the array dataset_1: \n', npTrain.shape)
    print('Shape of the array dataset_2: \n', npTest.shape)
    return npTrain[:,:-1], npTrain[:, -1], npTest[:,:-1], npTest[:, -1]

# Load the data
x_train, y_train, x_test, y_test = loadData()

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 512 fully-connected neurons with a relu activation 
#   2nd layer of 10 fully-connected neurons with a softmax activation 
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 3 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history = model.fit(x_train, y_train, epochs=3, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss: {:.2f}".format(results[0]))
print("Final Test Accuracy: {:.2f}".format(results[1]))

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 3), history.history["loss"], label="train_loss")
plt.plot(np.arange(0, 3), history.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 3), history.history["acc"], label="train_acc")
plt.plot(np.arange(0, 3), history.history["val_acc"], label="val_acc")
plt.title("Training/Validation Loss and Accuracy ")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()