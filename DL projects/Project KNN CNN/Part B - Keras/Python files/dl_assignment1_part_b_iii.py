# -*- coding: utf-8 -*-
"""DL_Assignment1_Part_B_III.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TV69ZwB3oHXBXKedwgi77Qaxw9ISCwhR

\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
#     PART B Section III       # 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
"""

# Importation of libraries (incuding tensorflow and keras)
import tensorflow as tf
from tensorflow import keras
from sklearn import preprocessing
import h5py

import numpy as np
import matplotlib.pyplot as plt

# read data
from google.colab import drive
drive.mount('/content/gdrive')
!unzip "/content/gdrive/My Drive/Colab Notebooks/data.zip"

# loadData is a function which load the dataset and split into training/test images and labels
def loadData():
  with h5py.File('data.h5','r') as hf:
    print('List of arrays in this file: \n', hf.keys())
    allTrain = hf.get('trainData')
    allTest = hf.get('testData')
    npTrain = np.array(allTrain)
    npTest = np.array(allTest)
    print('Shape of the array dataset_1: \n', npTrain.shape)
    print('Shape of the array dataset_2: \n', npTest.shape)
    return npTrain[:,:-1], npTrain[:, -1], npTest[:,:-1], npTest[:, -1]

# Load the data
x_train, y_train, x_test, y_test = loadData()

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 400 fully-connected neurons with a relu activation 
#   2nd layer of 200 fully-connected neurons with a relu activation 
#   3rd layer of 10 fully-connected neurons with a softmax activation 
model_2 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(400, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_2.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_2 = model_2.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_2.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 2): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 2): {:.2f}".format(results[1]))

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 600 fully-connected neurons with a relu activation 
#   2nd layer of 400 fully-connected neurons with a relu activation 
#   3rd layer of 200 fully-connected neurons with a relu activation 
#   4th layer of 10 fully-connected neurons with a softmax activation 
model_3 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(600, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(400, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_3.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_3 = model_3.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_3.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 3): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 3): {:.2f}".format(results[1]))

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 400 fully-connected neurons with a relu activation 
#   A Drop Out function is implemented with a percentage of 20% 
#   2nd layer of 200 fully-connected neurons with a relu activation 
#   3rd layer of 10 fully-connected neurons with a softmax activation 
model_2_do = tf.keras.models.Sequential([
    tf.keras.layers.Dense(400, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dropout(0.2, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_2_do.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_2_do = model_2_do.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_2_do.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 2 DO): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 2 Do): {:.2f}".format(results[1]))

from keras.regularizers import l1
from keras.layers import Activation

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 400 fully-connected neurons with a relu activation and l1 regularization of 0.01
#   2nd layer of 200 fully-connected neurons with a relu activation and l1 regularization of 0.01
#   3rd layer of 10 fully-connected neurons with a softmax activation 
model_2_l1r = tf.keras.models.Sequential([
    tf.keras.layers.Dense(400, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l1(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(200, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l1(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_2_l1r.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_2_l1r = model_2_l1r.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_2_l1r.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 2 l1r): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 2 l1r): {:.2f}".format(results[1]))

from keras.regularizers import l2

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 400 fully-connected neurons with a relu activation and l2 regularization of 0.01
#   2nd layer of 200 fully-connected neurons with a relu activation and l2 regularization of 0.01
#   3rd layer of 10 fully-connected neurons with a softmax activation 
model_2_l2r = tf.keras.models.Sequential([
    tf.keras.layers.Dense(400, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l2(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(200, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l2(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])


# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_2_l2r.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_2_l2r = model_2_l2r.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_2_l2r.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 2 l2r): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 2 l2r): {:.2f}".format(results[1]))

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 600 fully-connected neurons with a relu activation 
#   A Drop Out function is implemented with a percentage of 20% 
#   2nd layer of 400 fully-connected neurons with a relu activation 
#   A Drop Out function is implemented with a percentage of 20% 
#   3rd layer of 200 fully-connected neurons with a relu activation 
#   4th layer of 10 fully-connected neurons with a softmax activation 
model_3_do = tf.keras.models.Sequential([
    tf.keras.layers.Dense(600, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dropout(0.2, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(400, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dropout(0.2, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])


# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_3_do.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_3_do = model_3_do.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_3_do.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 3 do): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 3 do): {:.2f}".format(results[1]))

from keras.regularizers import l1

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 600 fully-connected neurons with a relu activation and l1 regularization of 0.01
#   2nd layer of 400 fully-connected neurons with a relu activation and l1 regularization of 0.01
#   3rd layer of 200 fully-connected neurons with a relu activation and l1 regularization of 0.01
#   4th layer of 10 fully-connected neurons with a softmax activation 
model_3_l1r = tf.keras.models.Sequential([
    tf.keras.layers.Dense(600, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l1(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(400, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l1(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(200, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l1(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_3_l1r.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_3_l1r = model_3_l1r.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_3_l1r.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 3 l1r): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 3 l1r): {:.2f}".format(results[1]))

from keras.regularizers import l2

# Build the keras neronal network model:
# Here, there are 2 layers:
#   1st layer of 600 fully-connected neurons with a relu activation and l2 regularization of 0.01
#   2nd layer of 400 fully-connected neurons with a relu activation and l2 regularization of 0.01
#   3rd layer of 200 fully-connected neurons with a relu activation and l2 regularization of 0.01
#   4th layer of 10 fully-connected neurons with a softmax activation 
model_3_l2r = tf.keras.models.Sequential([
    tf.keras.layers.Dense(600, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l2(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(400, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l2(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(200, activation='linear', input_shape=(x_train.shape[1],), activity_regularizer=l2(0.01)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# the model is configured for training with the those parameters:
#   For the optimizer: Adam Optimization algorithm
#   For the loss: the sparse categorical cross entropy
#   The metric to be evaluated by the model: accuracy 
model_3_l2r.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# the model is trained with the those parameters:
#   epoch: 6 iterations
#   batch_size: the size of the mini batch which contains 256 elements
#   validation_split: 10% of data used as a validation data
history_3_l2r = model_3_l2r.fit(x_train, y_train, epochs=6, batch_size=256, validation_split=0.1)

# the model is evaluated and returns test loss and accuracy
results = model_3_l2r.evaluate(x_test, y_test)

# Print the model result on the test data
print("Final Test Loss (model 2 l2r): {:.2f}".format(results[0]))
print("Final Test Accuracy (model 2 l2r): {:.2f}".format(results[1]))

plt.plot(history_2.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_2.history['val_acc'])
plt.title('Model accuracy M2 = (L1:400n,relu ; L2:200n,relu ; L3:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_2_do.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_2_do.history['val_acc'])
plt.title('Model accuracy M2 DO = (L1:400n,relu ; DO=0.2 ; L2:200n,relu ; L3:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_2_l1r.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_2_l1r.history['val_acc'])
plt.title('Model accuracy M2 L1R = (L1:400n,relu,l1-0.01 ; L2:200n,relu,l1-0.01 ; L3:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_2_l2r.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_2_l2r.history['val_acc'])
plt.title('Model accuracy M2 L2R = (L1:400n,relu ; L2:200n,relu,l2-0.01 ; L3:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_3.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_3.history['val_acc'])
plt.title('Model accuracy M3 = (L1:600n,relu ; L2:400n,relu ; L3:200n,relu ; L4:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_3_do.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_3_do.history['val_acc'])
plt.title('Model accuracy M3 DO = (L1:600n,relu ; DO=0.2 ; L2:400n,relu ; DO=0.2 ; L3:200n,relu ; L4:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_3_l1r.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_3_l1r.history['val_acc'])
plt.title('Model accuracy M3 L1R = (L1:600n,relu,l1-0.01 ; L2:400n,relu,l1-0.01 ; L3:200n,relu,l1-0.01 ; L4:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_3_l2r.history['acc']) 
# Plot training & validation accuracy values
plt.plot(history_3_l2r.history['val_acc'])
plt.title('Model accuracy M3 L2R = (L1:600n,relu,l2-0.01 ; L2:400n,relu,l2-0.01 ; L3:200n,relu,l2-0.01 ; L4:10n,softmax)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.figure()
plt.plot(np.arange(0, 6), history_2.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_2.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_2.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_2.history["val_acc"], label="val_acc")
plt.title("Training/Validation Loss and Accuracy for M2 = (L1:400n,relu ; L2:200n,relu ; L3:10n,softmax)")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_2_do.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_2_do.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_2_do.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_2_do.history["val_acc"], label="val_acc")
plt.title('Training/Validation Loss and Accuracy for M2 DO = (L1:400n,relu ; DO=0.2 ; L2:200n,relu ; L3:10n,softmax)')
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_2_l1r.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_2_l1r.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_2_l1r.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_2_l1r.history["val_acc"], label="val_acc")
plt.title('Training/Validation Loss and Accuracy for M2 L1R = (L1:400n,relu ; L2:200n,relu,l1-0.01 ; L3:10n,softmax)')
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_2_l2r.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_2_l2r.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_2_l2r.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_2_l2r.history["val_acc"], label="val_acc")
plt.title('Training/Validation Loss and Accuracy for M2 L2R = (L1:400n,relu ; L2:200n,relu,l2-0.01 ; L3:10n,softmax)')
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_3.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_3.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_3.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_3.history["val_acc"], label="val_acc")
plt.title("Training/Validation Loss and Accuracy for M3 = (L1:600n,relu ; L2:400n,relu ; L3:200n,relu ; L4:10n,softmax)")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_3_do.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_3_do.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_3_do.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_3_do.history["val_acc"], label="val_acc")
plt.title('Training/Validation Loss and Accuracy for M3 DO = (L1:600n,relu ; DO=0.2 ; L2:400n,relu ; DO=0.2 ; L3:200n,relu ; L4:10n,softmax)')
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_3_l1r.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_3_l1r.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_3_l1r.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_3_l1r.history["val_acc"], label="val_acc")
plt.title('Training/Validation Loss and Accuracy for M3 L1R = (L1:600n,relu,l1-0.01 ; L2:400n,relu,l1-0.01 ; L3:200n,relu,l1-0.01 ; L4:10n,softmax)')
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_3_l2r.history["loss"], label="train_loss")
plt.plot(np.arange(0, 6), history_3_l2r.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 6), history_3_l2r.history["acc"], label="train_acc")
plt.plot(np.arange(0, 6), history_3_l2r.history["val_acc"], label="val_acc")
plt.title('Training/Validation Loss and Accuracy for M3 L2R = (L1:600n,relu,l2-0.01 ; L2:400n,relu,l2-0.01 ; L3:200n,relu,l2-0.01 ; L4:10n,softmax)')
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_2.history["acc"], label="M2 train_acc")
plt.plot(np.arange(0, 6), history_2.history["val_acc"], label="M2 val_acc")
plt.plot(np.arange(0, 6), history_2_do.history["acc"], label="M2DO train_acc")
plt.plot(np.arange(0, 6), history_2_do.history["val_acc"], label="M2DO val_acc")
plt.plot(np.arange(0, 6), history_2_l1r.history["acc"], label="M2L1R train_acc")
plt.plot(np.arange(0, 6), history_2_l1r.history["val_acc"], label="M2L1R val_acc")
plt.plot(np.arange(0, 6), history_2_l2r.history["acc"], label="M2L2R train_acc")
plt.plot(np.arange(0, 6), history_2_l2r.history["val_acc"], label="M2L2R val_acc")
plt.title("Training/Validation Accuracy for M2, M2 DO, M2 L1R, M2 L2R")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_2.history["loss"], label="M3 train_loss")
plt.plot(np.arange(0, 6), history_2.history["val_loss"], label="M2 val_loss")
plt.plot(np.arange(0, 6), history_2_do.history["loss"], label="M2DO train_loss")
plt.plot(np.arange(0, 6), history_2_do.history["val_loss"], label="M2DO val_loss")
plt.plot(np.arange(0, 6), history_2_l1r.history["loss"], label="M2L1R train_loss")
plt.plot(np.arange(0, 6), history_2_l1r.history["val_loss"], label="M2L1R val_loss")
plt.plot(np.arange(0, 6), history_2_l2r.history["loss"], label="M2L2R train_loss")
plt.plot(np.arange(0, 6), history_2_l2r.history["val_loss"], label="M2L2R val_loss")
plt.title("Training/Validation Loss for M2, M2 DO, M2 L1R, M2 L2R")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_3.history["acc"], label="M3 train_acc")
plt.plot(np.arange(0, 6), history_3.history["val_acc"], label="M3 val_acc")
plt.plot(np.arange(0, 6), history_3_do.history["acc"], label="M3DO train_acc")
plt.plot(np.arange(0, 6), history_3_do.history["val_acc"], label="M3DO val_acc")
plt.plot(np.arange(0, 6), history_3_l1r.history["acc"], label="M3L1R train_acc")
plt.plot(np.arange(0, 6), history_3_l1r.history["val_acc"], label="M3L1R val_acc")
plt.plot(np.arange(0, 6), history_3_l2r.history["acc"], label="M3L2R train_acc")
plt.plot(np.arange(0, 6), history_3_l2r.history["val_acc"], label="M3L2R val_acc")
plt.title("Training/Validation Accuracy for M3, M3 DO, M3 L1R, M3 L2R")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend()

plt.figure()
plt.plot(np.arange(0, 6), history_3.history["loss"], label="M3 train_loss")
plt.plot(np.arange(0, 6), history_3.history["val_loss"], label="M3 val_loss")
plt.plot(np.arange(0, 6), history_3_do.history["loss"], label="M3DO train_loss")
plt.plot(np.arange(0, 6), history_3_do.history["val_loss"], label="M3DO val_loss")
plt.plot(np.arange(0, 6), history_3_l1r.history["loss"], label="M3L1R train_loss")
plt.plot(np.arange(0, 6), history_3_l1r.history["val_loss"], label="M3L1R val_loss")
plt.plot(np.arange(0, 6), history_3_l2r.history["loss"], label="M3L2R train_loss")
plt.plot(np.arange(0, 6), history_3_l2r.history["val_loss"], label="M3L2R val_loss")
plt.title("Training/Validation Loss for M3, M3 DO, M3 L1R, M3 L2R")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend()